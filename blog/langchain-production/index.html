<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LangChain in Production: What the Tutorials Don't Tell You | Muhammad Hassan Raza</title><meta name=keywords content="LangChain,Python,AI,Production,RAG,LLM,Backend Development,LCEL"><meta name=description content="
Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, &ldquo;Cool, I&rsquo;ll ship this by Friday.&rdquo; Three weeks later, you&rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.
I&rsquo;ve shipped LangChain-based features to production at multiple companies. Here&rsquo;s what I wish someone had told me before I started."><meta name=author content="Muhammad Hassan Raza"><link rel=canonical href=https://mhassan.dev/blog/langchain-production/><link crossorigin=anonymous href=/assets/css/stylesheet.4dee8d1e6e72ca23617a170af339289b36a0ffe1043ae9bd913bc8fa838c4eae.css integrity="sha256-Te6NHm5yyiNhehcK8zkomzag/+EEOum9kTvI+oOMTq4=" rel="preload stylesheet" as=style><link rel=icon href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=32x32 href=https://mhassan.dev/assets/favicon.svg><link rel=apple-touch-icon href=https://mhassan.dev/apple-touch-icon.png><link rel=mask-icon href=https://mhassan.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mhassan.dev/blog/langchain-production/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=preload href=/css/extended/extended.css as=style><link rel=stylesheet href=/css/extended/extended.css><link rel=preload href=/fonts/font-family.css as=style><link rel=stylesheet href=/fonts/font-family.css><link rel=preload href=/fonts/Manrope-Regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Medium.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Bold.woff2 as=font type=font/woff2 crossorigin><script async src=https://cloud.umami.is/script.js data-website-id=30c7d9d6-abac-4c52-b85a-c0234f863d22></script><script async src="https://www.googletagmanager.com/gtag/js?id=%7b%7d"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","{}")</script><meta property="og:url" content="https://mhassan.dev/blog/langchain-production/"><meta property="og:site_name" content="Muhammad Hassan Raza"><meta property="og:title" content="LangChain in Production: What the Tutorials Don't Tell You"><meta property="og:description" content=" Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, “Cool, I’ll ship this by Friday.” Three weeks later, you’re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.
I’ve shipped LangChain-based features to production at multiple companies. Here’s what I wish someone had told me before I started."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-20T10:00:00+05:00"><meta property="article:modified_time" content="2025-06-20T10:00:00+05:00"><meta property="article:tag" content="LangChain"><meta property="article:tag" content="Python"><meta property="article:tag" content="AI"><meta property="article:tag" content="Production"><meta property="article:tag" content="RAG"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://mhassan.dev/assets/langchain-prod.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mhassan.dev/assets/langchain-prod.jpg"><meta name=twitter:title content="LangChain in Production: What the Tutorials Don't Tell You"><meta name=twitter:description content="
Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, &ldquo;Cool, I&rsquo;ll ship this by Friday.&rdquo; Three weeks later, you&rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.
I&rsquo;ve shipped LangChain-based features to production at multiple companies. Here&rsquo;s what I wish someone had told me before I started."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mhassan.dev/blog/"},{"@type":"ListItem","position":2,"name":"LangChain in Production: What the Tutorials Don't Tell You","item":"https://mhassan.dev/blog/langchain-production/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LangChain in Production: What the Tutorials Don't Tell You","name":"LangChain in Production: What the Tutorials Don\u0027t Tell You","description":" Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, \u0026ldquo;Cool, I\u0026rsquo;ll ship this by Friday.\u0026rdquo; Three weeks later, you\u0026rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.\nI\u0026rsquo;ve shipped LangChain-based features to production at multiple companies. Here\u0026rsquo;s what I wish someone had told me before I started.\n","keywords":["LangChain","Python","AI","Production","RAG","LLM","Backend Development","LCEL"],"articleBody":" Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, “Cool, I’ll ship this by Friday.” Three weeks later, you’re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.\nI’ve shipped LangChain-based features to production at multiple companies. Here’s what I wish someone had told me before I started.\nWhen to Use LangChain (And When Not To) Let’s start with the uncomfortable truth: you probably don’t need LangChain.\nLangChain is an abstraction layer. Abstractions are great when they simplify common patterns and terrible when they obscure what’s actually happening. For LangChain, it depends entirely on your use case.\nUse LangChain when: You’re building complex chains with multiple LLM calls, tools, and conditional logic You need observability and tracing (LangSmith integration is genuinely good) You’re prototyping rapidly and might switch LLM providers Your team is already familiar with the framework Skip LangChain when: You’re making simple API calls to one model You need fine-grained control over request/response handling Your use case doesn’t fit LangChain’s mental model Bundle size or cold start time matters (serverless) At Entropy Labs, we use a hybrid approach: LangChain for complex agentic workflows, raw SDK calls for simple completions. The overhead isn’t worth it for a straightforward “summarize this text” endpoint.\nLCEL: The Good Parts LangChain Expression Language (LCEL) was a massive improvement over the legacy chain syntax. Here’s a pattern that actually works well in production:\nfrom langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_anthropic import ChatAnthropic from langchain_core.runnables import RunnablePassthrough # Clean, composable chain prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are a technical writer. Be concise.\"), (\"human\", \"{input}\") ]) model = ChatAnthropic( model=\"claude-sonnet-4-20250514\", max_tokens=1024, timeout=30.0, # Always set timeouts ) chain = ( {\"input\": RunnablePassthrough()} | prompt | model | StrOutputParser() ) # With retry logic from langchain_core.runnables import RunnableRetry robust_chain = chain.with_retry( stop_after_attempt=3, wait_exponential_jitter=True ) The pipe syntax makes composition clear. You can see data flow. That’s the good part.\nStreaming that actually works async def stream_response(query: str): async for chunk in chain.astream(query): yield chunk Simple, clean, no surprises. Until you add memory.\nThe Problems Nobody Warns You About 1. Memory management is a minefield LangChain’s conversation memory abstractions look elegant in docs. In production, they’re a footgun.\n# This looks innocent from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() chain = ConversationChain(llm=llm, memory=memory) Problems:\nMemory is stored in-process by default. Restart your server? Gone. No TTL. Chat histories grow unbounded. The memory object isn’t thread-safe. Concurrent requests? Corruption. What we actually use:\nfrom langchain_community.chat_message_histories import RedisChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory def get_session_history(session_id: str): return RedisChatMessageHistory( session_id, url=settings.REDIS_URL, ttl=3600 # 1 hour TTL ) chain_with_history = RunnableWithMessageHistory( chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\", ) Redis handles persistence, TTL, and concurrency. LangChain’s memory abstractions are just wrappers.\n2. Silent failures everywhere This one cost me 8 hours of debugging:\n# Looks fine, right? result = await chain.ainvoke({\"query\": user_input}) The chain returned an empty string. No error. No exception. Nothing in logs.\nThe cause? A malformed prompt template that resulted in an empty message list. The LLM received nothing, returned nothing. LangChain happily passed it through.\nAlways validate chain outputs:\nresult = await chain.ainvoke({\"query\": user_input}) if not result or not result.strip(): logger.error(f\"Empty response for query: {user_input[:100]}\") raise ValueError(\"LLM returned empty response\") 3. Version churn is exhausting LangChain’s API changes frequently. Code that worked in 0.1.x might not compile in 0.2.x. Import paths move. Classes get renamed.\n# v0.1.x from langchain.chat_models import ChatAnthropic # v0.2.x from langchain_anthropic import ChatAnthropic # v0.3.x # Who knows? Check the migration guide. Pin your versions aggressively:\n# pyproject.toml langchain = \"==0.2.14\" langchain-core = \"==0.2.33\" langchain-anthropic = \"==0.1.23\" And read the changelogs before upgrading.\nCost Tracking and Observability If you’re not tracking costs, you’re flying blind. LangSmith is the easiest path:\nimport os os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = \"your-key\" os.environ[\"LANGCHAIN_PROJECT\"] = \"production\" Every chain execution gets traced. You see latency, token counts, and costs. The callback system also lets you build custom tracking:\nfrom langchain_core.callbacks import BaseCallbackHandler from typing import Any class CostTracker(BaseCallbackHandler): def __init__(self): self.total_tokens = 0 self.total_cost = 0.0 def on_llm_end(self, response: Any, **kwargs): usage = response.llm_output.get(\"token_usage\", {}) input_tokens = usage.get(\"prompt_tokens\", 0) output_tokens = usage.get(\"completion_tokens\", 0) # Claude Sonnet pricing (example) cost = (input_tokens * 0.003 + output_tokens * 0.015) / 1000 self.total_cost += cost logger.info(f\"LLM call cost: ${cost:.4f}\") At Entropy Labs, we alert when daily spend exceeds thresholds. One runaway loop can burn through hundreds of dollars.\nAlternatives and When to Use Them LlamaIndex for pure RAG If your use case is “query documents and return answers,” LlamaIndex is more focused. Less abstraction, more batteries included for retrieval.\nDirect SDK calls For simple use cases, the Anthropic/OpenAI SDKs are cleaner:\nfrom anthropic import Anthropic client = Anthropic() response = client.messages.create( model=\"claude-sonnet-4-20250514\", max_tokens=1024, messages=[{\"role\": \"user\", \"content\": query}] ) No framework, no magic, full control.\nHaystack If you need more structure than raw SDKs but less opinion than LangChain, Haystack hits a middle ground. Worth evaluating for production RAG pipelines.\nMy Production Stack Here’s what I actually deploy:\nSimple completions: Anthropic SDK directly Complex chains: LangChain + LCEL Retrieval: LlamaIndex or custom (depending on scale) Memory: Redis with manual management Observability: LangSmith + custom Prometheus metrics Rate limiting: Redis-based token bucket Caching: Response caching for deterministic queries The theme: use LangChain where it adds value, bypass it where it adds complexity.\nThe Bottom Line LangChain is a powerful framework with rough edges. The tutorials show the happy path; production is everything else.\nBefore adopting it:\nUnderstand what abstraction you’re buying and what control you’re giving up Set up observability from day one Plan for version upgrades (they’re frequent and breaking) Build escape hatches for when the framework fights you The best LangChain code I’ve written is the code that uses it sparingly—for the problems it solves well, not for everything.\n","wordCount":"979","inLanguage":"en","image":"https://mhassan.dev/assets/langchain-prod.jpg","datePublished":"2025-06-20T10:00:00+05:00","dateModified":"2025-06-20T10:00:00+05:00","author":{"@type":"Person","name":"Muhammad Hassan Raza"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mhassan.dev/blog/langchain-production/"},"publisher":{"@type":"Organization","name":"Muhammad Hassan Raza","logo":{"@type":"ImageObject","url":"https://mhassan.dev/assets/favicon.svg"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://mhassan.dev/ accesskey=h title="Muhammad Hassan Raza (Alt + H)">Muhammad Hassan Raza</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mhassan.dev/book-a-call/ title="Book a Call"><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M8.26 1.289l-1.564.772c-5.793 3.02 2.798 20.944 9.31 20.944.46.0.904-.094 1.317-.284l1.542-.755-2.898-5.594-1.54.754c-.181.087-.384.134-.597.134-2.561.0-6.841-8.204-4.241-9.596l1.546-.763L8.26 1.289zM16.006 24C10.326 24 3.785 12.886 3.785 6.168c0-2.419.833-4.146 2.457-4.992l2.382-1.176 3.857 7.347-2.437 1.201c-1.439.772 2.409 8.424 3.956 7.68l2.399-1.179 3.816 7.36s-2.36 1.162-2.476 1.215c-.547.251-1.129.376-1.733.376"/></svg>Book a Call</span></a></li><li><a href=https://mhassan.dev/projects/ title=Projects><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M11 5h13v17H0V2h8l3 3zM1 3v18h22V6H10.586l-3-3H1z"/></svg>Projects</span></a></li><li><a href=https://mhassan.dev/about/ title=About><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="-1 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M12 0c6.623.0 12 5.377 12 12s-5.377 12-12 12S0 18.623.0 12 5.377.0 12 0zm8.127 19.41c-.282-.401-.772-.654-1.624-.85-3.848-.906-4.097-1.501-4.352-2.059-.259-.565-.19-1.23.205-1.977 1.726-3.257 2.09-6.024 1.027-7.79C14.709 5.615 13.508 5 12 5c-1.521.0-2.732.626-3.409 1.763-1.066 1.789-.693 4.544 1.049 7.757.402.742.476 1.406.22 1.974-.265.586-.611 1.19-4.365 2.066-.852.196-1.342.449-1.623.848C5.884 21.615 8.782 23 12 23s6.115-1.385 8.127-3.59zm.65-.782C22.172 16.784 23 14.488 23 12c0-6.071-4.929-11-11-11S1 5.929 1 12c0 2.487.827 4.783 2.222 6.626.409-.452 1.049-.81 2.049-1.041 2.025-.462 3.376-.836 3.678-1.502.122-.272.061-.628-.188-1.087-1.917-3.535-2.282-6.641-1.03-8.745C8.584 4.82 10.139 4 12 4c1.845.0 3.391.808 4.24 2.218 1.251 2.079.896 5.195-1 8.774-.245.463-.304.821-.179 1.094.305.668 1.644 1.038 3.667 1.499 1 .23 1.64.59 2.049 1.043z"/></svg>About</span></a></li><li><a href=https://mhassan.dev/search/ title="Search (Alt + /)" accesskey=/><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M15.853 16.56C14.17 18.077 11.942 19 9.5 19 4.257 19 0 14.743.0 9.5S4.257.0 9.5.0 19 4.257 19 9.5c0 2.442-.923 4.67-2.44 6.353l7.44 7.44-.707.707-7.44-7.44zm-6.353-15.56c4.691.0 8.5 3.809 8.5 8.5S14.191 18 9.5 18 1 14.191 1 9.5 4.809 1 9.5 1z"/></svg>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mhassan.dev/>Home</a>&nbsp;»&nbsp;<a href=https://mhassan.dev/blog/>Blog</a></div><h1 class=post-title>LangChain in Production: What the Tutorials Don't Tell You</h1><div class=post-meta><span title='2025-06-20 10:00:00 +0500 +0500'>June 20, 2025</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>Muhammad Hassan Raza</span></div></header><figure class=entry-cover><img loading=lazy src=https://mhassan.dev/assets/langchain-prod.jpg alt="LangChain Production Architecture"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#when-to-use-langchain-and-when-not-to aria-label="When to Use LangChain (And When Not To)">When to Use LangChain (And When Not To)</a><ul><li><a href=#use-langchain-when aria-label="Use LangChain when:">Use LangChain when:</a></li><li><a href=#skip-langchain-when aria-label="Skip LangChain when:">Skip LangChain when:</a></li></ul></li><li><a href=#lcel-the-good-parts aria-label="LCEL: The Good Parts">LCEL: The Good Parts</a><ul><li><a href=#streaming-that-actually-works aria-label="Streaming that actually works">Streaming that actually works</a></li></ul></li><li><a href=#the-problems-nobody-warns-you-about aria-label="The Problems Nobody Warns You About">The Problems Nobody Warns You About</a><ul><li><a href=#1-memory-management-is-a-minefield aria-label="1. Memory management is a minefield">1. Memory management is a minefield</a></li><li><a href=#2-silent-failures-everywhere aria-label="2. Silent failures everywhere">2. Silent failures everywhere</a></li><li><a href=#3-version-churn-is-exhausting aria-label="3. Version churn is exhausting">3. Version churn is exhausting</a></li></ul></li><li><a href=#cost-tracking-and-observability aria-label="Cost Tracking and Observability">Cost Tracking and Observability</a></li><li><a href=#alternatives-and-when-to-use-them aria-label="Alternatives and When to Use Them">Alternatives and When to Use Them</a><ul><li><a href=#llamaindex-for-pure-rag aria-label="LlamaIndex for pure RAG">LlamaIndex for pure RAG</a></li><li><a href=#direct-sdk-calls aria-label="Direct SDK calls">Direct SDK calls</a></li><li><a href=#haystack aria-label=Haystack>Haystack</a></li></ul></li><li><a href=#my-production-stack aria-label="My Production Stack">My Production Stack</a></li><li><a href=#the-bottom-line aria-label="The Bottom Line">The Bottom Line</a></li></ul></div></details></div><div class=post-content><div style=text-align:justify><p>Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, &ldquo;Cool, I&rsquo;ll ship this by Friday.&rdquo; Three weeks later, you&rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.</p><p>I&rsquo;ve shipped LangChain-based features to production at multiple companies. Here&rsquo;s what I wish someone had told me before I started.</p><hr><h2 id=when-to-use-langchain-and-when-not-to><span style=color:#8ac7db>When to Use LangChain (And When Not To)</span><a hidden class=anchor aria-hidden=true href=#when-to-use-langchain-and-when-not-to>#</a></h2><p>Let&rsquo;s start with the uncomfortable truth: <strong>you probably don&rsquo;t need LangChain</strong>.</p><p>LangChain is an abstraction layer. Abstractions are great when they simplify common patterns and terrible when they obscure what&rsquo;s actually happening. For LangChain, it depends entirely on your use case.</p><h3 id=use-langchain-when>Use LangChain when:<a hidden class=anchor aria-hidden=true href=#use-langchain-when>#</a></h3><ul><li>You&rsquo;re building complex chains with multiple LLM calls, tools, and conditional logic</li><li>You need observability and tracing (LangSmith integration is genuinely good)</li><li>You&rsquo;re prototyping rapidly and might switch LLM providers</li><li>Your team is already familiar with the framework</li></ul><h3 id=skip-langchain-when>Skip LangChain when:<a hidden class=anchor aria-hidden=true href=#skip-langchain-when>#</a></h3><ul><li>You&rsquo;re making simple API calls to one model</li><li>You need fine-grained control over request/response handling</li><li>Your use case doesn&rsquo;t fit LangChain&rsquo;s mental model</li><li>Bundle size or cold start time matters (serverless)</li></ul><p>At Entropy Labs, we use a hybrid approach: LangChain for complex agentic workflows, raw SDK calls for simple completions. The overhead isn&rsquo;t worth it for a straightforward &ldquo;summarize this text&rdquo; endpoint.</p><hr><h2 id=lcel-the-good-parts><span style=color:#ffb4a2>LCEL: The Good Parts</span><a hidden class=anchor aria-hidden=true href=#lcel-the-good-parts>#</a></h2><p>LangChain Expression Language (LCEL) was a massive improvement over the legacy chain syntax. Here&rsquo;s a pattern that actually works well in production:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.output_parsers <span style=color:#f92672>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_anthropic <span style=color:#f92672>import</span> ChatAnthropic
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables <span style=color:#f92672>import</span> RunnablePassthrough
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Clean, composable chain</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_messages([
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;You are a technical writer. Be concise.&#34;</span>),
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;human&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{input}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> ChatAnthropic(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;claude-sonnet-4-20250514&#34;</span>,
</span></span><span style=display:flex><span>    max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>,
</span></span><span style=display:flex><span>    timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>30.0</span>,  <span style=color:#75715e># Always set timeouts</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;input&#34;</span>: RunnablePassthrough()}
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> model
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> StrOutputParser()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># With retry logic</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables <span style=color:#f92672>import</span> RunnableRetry
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>robust_chain <span style=color:#f92672>=</span> chain<span style=color:#f92672>.</span>with_retry(
</span></span><span style=display:flex><span>    stop_after_attempt<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    wait_exponential_jitter<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>The pipe syntax makes composition clear. You can see data flow. That&rsquo;s the good part.</p><h3 id=streaming-that-actually-works>Streaming that actually works<a hidden class=anchor aria-hidden=true href=#streaming-that-actually-works>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>stream_response</span>(query: str):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> chain<span style=color:#f92672>.</span>astream(query):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> chunk
</span></span></code></pre></div><p>Simple, clean, no surprises. Until you add memory.</p><hr><h2 id=the-problems-nobody-warns-you-about><span style=color:#8ac7db>The Problems Nobody Warns You About</span><a hidden class=anchor aria-hidden=true href=#the-problems-nobody-warns-you-about>#</a></h2><h3 id=1-memory-management-is-a-minefield>1. Memory management is a minefield<a hidden class=anchor aria-hidden=true href=#1-memory-management-is-a-minefield>#</a></h3><p>LangChain&rsquo;s conversation memory abstractions look elegant in docs. In production, they&rsquo;re a footgun.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># This looks innocent</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.memory <span style=color:#f92672>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>memory <span style=color:#f92672>=</span> ConversationBufferMemory()
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> ConversationChain(llm<span style=color:#f92672>=</span>llm, memory<span style=color:#f92672>=</span>memory)
</span></span></code></pre></div><p>Problems:</p><ul><li>Memory is stored in-process by default. Restart your server? Gone.</li><li>No TTL. Chat histories grow unbounded.</li><li>The memory object isn&rsquo;t thread-safe. Concurrent requests? Corruption.</li></ul><p>What we actually use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_message_histories <span style=color:#f92672>import</span> RedisChatMessageHistory
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables.history <span style=color:#f92672>import</span> RunnableWithMessageHistory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_session_history</span>(session_id: str):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> RedisChatMessageHistory(
</span></span><span style=display:flex><span>        session_id,
</span></span><span style=display:flex><span>        url<span style=color:#f92672>=</span>settings<span style=color:#f92672>.</span>REDIS_URL,
</span></span><span style=display:flex><span>        ttl<span style=color:#f92672>=</span><span style=color:#ae81ff>3600</span>  <span style=color:#75715e># 1 hour TTL</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chain_with_history <span style=color:#f92672>=</span> RunnableWithMessageHistory(
</span></span><span style=display:flex><span>    chain,
</span></span><span style=display:flex><span>    get_session_history,
</span></span><span style=display:flex><span>    input_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input&#34;</span>,
</span></span><span style=display:flex><span>    history_messages_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;history&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Redis handles persistence, TTL, and concurrency. LangChain&rsquo;s memory abstractions are just wrappers.</p><h3 id=2-silent-failures-everywhere>2. Silent failures everywhere<a hidden class=anchor aria-hidden=true href=#2-silent-failures-everywhere>#</a></h3><p>This one cost me 8 hours of debugging:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Looks fine, right?</span>
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> chain<span style=color:#f92672>.</span>ainvoke({<span style=color:#e6db74>&#34;query&#34;</span>: user_input})
</span></span></code></pre></div><p>The chain returned an empty string. No error. No exception. Nothing in logs.</p><p>The cause? A malformed prompt template that resulted in an empty message list. The LLM received nothing, returned nothing. LangChain happily passed it through.</p><p><strong>Always validate chain outputs:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>result <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> chain<span style=color:#f92672>.</span>ainvoke({<span style=color:#e6db74>&#34;query&#34;</span>: user_input})
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> result <span style=color:#f92672>or</span> <span style=color:#f92672>not</span> result<span style=color:#f92672>.</span>strip():
</span></span><span style=display:flex><span>    logger<span style=color:#f92672>.</span>error(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Empty response for query: </span><span style=color:#e6db74>{</span>user_input[:<span style=color:#ae81ff>100</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;LLM returned empty response&#34;</span>)
</span></span></code></pre></div><h3 id=3-version-churn-is-exhausting>3. Version churn is exhausting<a hidden class=anchor aria-hidden=true href=#3-version-churn-is-exhausting>#</a></h3><p>LangChain&rsquo;s API changes frequently. Code that worked in 0.1.x might not compile in 0.2.x. Import paths move. Classes get renamed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># v0.1.x</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.chat_models <span style=color:#f92672>import</span> ChatAnthropic
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># v0.2.x</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_anthropic <span style=color:#f92672>import</span> ChatAnthropic
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># v0.3.x</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Who knows? Check the migration guide.</span>
</span></span></code></pre></div><p><strong>Pin your versions aggressively:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span><span style=color:#75715e># pyproject.toml</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>langchain</span> = <span style=color:#e6db74>&#34;==0.2.14&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>langchain-core</span> = <span style=color:#e6db74>&#34;==0.2.33&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>langchain-anthropic</span> = <span style=color:#e6db74>&#34;==0.1.23&#34;</span>
</span></span></code></pre></div><p>And read the changelogs before upgrading.</p><hr><h2 id=cost-tracking-and-observability><span style=color:#ffb4a2>Cost Tracking and Observability</span><a hidden class=anchor aria-hidden=true href=#cost-tracking-and-observability>#</a></h2><p>If you&rsquo;re not tracking costs, you&rsquo;re flying blind. LangSmith is the easiest path:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LANGCHAIN_TRACING_V2&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LANGCHAIN_API_KEY&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;your-key&#34;</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LANGCHAIN_PROJECT&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;production&#34;</span>
</span></span></code></pre></div><p>Every chain execution gets traced. You see latency, token counts, and costs. The callback system also lets you build custom tracking:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.callbacks <span style=color:#f92672>import</span> BaseCallbackHandler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Any
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CostTracker</span>(BaseCallbackHandler):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>total_tokens <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>total_cost <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>on_llm_end</span>(self, response: Any, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>        usage <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>llm_output<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;token_usage&#34;</span>, {})
</span></span><span style=display:flex><span>        input_tokens <span style=color:#f92672>=</span> usage<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;prompt_tokens&#34;</span>, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        output_tokens <span style=color:#f92672>=</span> usage<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;completion_tokens&#34;</span>, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Claude Sonnet pricing (example)</span>
</span></span><span style=display:flex><span>        cost <span style=color:#f92672>=</span> (input_tokens <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.003</span> <span style=color:#f92672>+</span> output_tokens <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.015</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>total_cost <span style=color:#f92672>+=</span> cost
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        logger<span style=color:#f92672>.</span>info(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;LLM call cost: $</span><span style=color:#e6db74>{</span>cost<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>At Entropy Labs, we alert when daily spend exceeds thresholds. One runaway loop can burn through hundreds of dollars.</p><hr><h2 id=alternatives-and-when-to-use-them><span style=color:#8ac7db>Alternatives and When to Use Them</span><a hidden class=anchor aria-hidden=true href=#alternatives-and-when-to-use-them>#</a></h2><h3 id=llamaindex-for-pure-rag>LlamaIndex for pure RAG<a hidden class=anchor aria-hidden=true href=#llamaindex-for-pure-rag>#</a></h3><p>If your use case is &ldquo;query documents and return answers,&rdquo; LlamaIndex is more focused. Less abstraction, more batteries included for retrieval.</p><h3 id=direct-sdk-calls>Direct SDK calls<a hidden class=anchor aria-hidden=true href=#direct-sdk-calls>#</a></h3><p>For simple use cases, the Anthropic/OpenAI SDKs are cleaner:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> anthropic <span style=color:#f92672>import</span> Anthropic
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> Anthropic()
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>messages<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;claude-sonnet-4-20250514&#34;</span>,
</span></span><span style=display:flex><span>    max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: query}]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>No framework, no magic, full control.</p><h3 id=haystack>Haystack<a hidden class=anchor aria-hidden=true href=#haystack>#</a></h3><p>If you need more structure than raw SDKs but less opinion than LangChain, Haystack hits a middle ground. Worth evaluating for production RAG pipelines.</p><hr><h2 id=my-production-stack><span style=color:#ffb4a2>My Production Stack</span><a hidden class=anchor aria-hidden=true href=#my-production-stack>#</a></h2><p>Here&rsquo;s what I actually deploy:</p><pre tabindex=0><code>Simple completions: Anthropic SDK directly
Complex chains: LangChain + LCEL
Retrieval: LlamaIndex or custom (depending on scale)
Memory: Redis with manual management
Observability: LangSmith + custom Prometheus metrics
Rate limiting: Redis-based token bucket
Caching: Response caching for deterministic queries
</code></pre><p>The theme: use LangChain where it adds value, bypass it where it adds complexity.</p><hr><h2 id=the-bottom-line><span style=color:#8ac7db>The Bottom Line</span><a hidden class=anchor aria-hidden=true href=#the-bottom-line>#</a></h2><p>LangChain is a powerful framework with rough edges. The tutorials show the happy path; production is everything else.</p><p>Before adopting it:</p><ol><li>Understand what abstraction you&rsquo;re buying and what control you&rsquo;re giving up</li><li>Set up observability from day one</li><li>Plan for version upgrades (they&rsquo;re frequent and breaking)</li><li>Build escape hatches for when the framework fights you</li></ol><p>The best LangChain code I&rsquo;ve written is the code that uses it sparingly—for the problems it solves well, not for everything.</p></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://mhassan.dev/tags/langchain>LangChain</a></li><li><a href=https://mhassan.dev/tags/python>Python</a></li><li><a href=https://mhassan.dev/tags/ai>AI</a></li><li><a href=https://mhassan.dev/tags/production>Production</a></li><li><a href=https://mhassan.dev/tags/rag>RAG</a></li><li><a href=https://mhassan.dev/tags/llm>LLM</a></li><li><a href=https://mhassan.dev/tags/backend-development>Backend Development</a></li><li><a href=https://mhassan.dev/tags/lcel>LCEL</a></li></ul><nav class=paginav><a class=prev href=https://mhassan.dev/blog/ai-features-users-want/><span class=title>« Prev</span><br><span>AI Features Your Users Actually Want (Hint: Not Another Chatbot)</span>
</a><a class=next href=https://mhassan.dev/blog/claude-opus-45/><span class=title>Next »</span><br><span>Claude Opus 4.5: When an AI Finally Gets It</span></a></nav></footer><div class=giscus-container><script src=https://giscus.app/client.js data-repo=M-Hassan-Raza/Portfolio data-repo-id=R_kgDON3Oajw data-category=General data-category-id=DIC_kwDON3Oaj84Cm3y9 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></div><script>function getTheme(){return document.body.classList.contains("dark")?"dark":"light"}function updateGiscusTheme(){const e=document.querySelector("iframe.giscus-frame");if(!e){setTimeout(updateGiscusTheme,300);return}const t=getTheme();e.contentWindow.postMessage({giscus:{setConfig:{theme:t}}},"https://giscus.app")}localStorage.getItem("pref-theme")==="dark"&&updateGiscusTheme();const observer=new MutationObserver(()=>{updateGiscusTheme()});observer.observe(document.body,{attributes:!0,attributeFilter:["class"]})</script></article></main><footer class=footer><span>&copy; 2026 <a href=https://mhassan.dev/>Muhammad Hassan Raza</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){try{if(window.__hasLoggedConsoleBrand)return;var n=getComputedStyle(document.documentElement),e=(n.getPropertyValue("--primary")||"").trim(),t=(n.getPropertyValue("--secondary")||"").trim(),s=e?"hsl("+e+")":"#4f46e5",o=t?"hsl("+t+")":"#facc15",i="color: "+s+"; font-weight: bold; font-family: monospace; font-size: 12px; line-height: 1.2;",a="color: "+o+"; font-weight: bold; font-family: monospace; font-size: 10px;",r=`%c
███████╗ ██████╗ ██████╗     ████████╗██╗  ██╗███████╗
██╔════╝██╔═══██╗██╔══██╗    ╚══██╔══╝██║  ██║██╔════╝
█████╗  ██║   ██║██████╔╝       ██║   ███████║█████╗  
██╔══╝  ██║   ██║██╔══██╗       ██║   ██╔══██║██╔══╝  
██║     ╚██████╔╝██║  ██║       ██║   ██║  ██║███████╗
╚═╝      ╚═════╝ ╚═╝  ╚═╝       ╚═╝   ╚═╝  ╚═╝╚══════╝
                                                        
███████╗███╗   ███╗██████╗ ███████╗██████╗  ██████╗ ██████╗ 
██╔════╝████╗ ████║██╔══██╗██╔════╝██╔══██╗██╔═══██╗██╔══██╗
█████╗  ██╔████╔██║██████╔╝█████╗  ██████╔╝██║   ██║██████╔╝
██╔══╝  ██║╚██╔╝██║██╔═══╝ ██╔══╝  ██╔══██╗██║   ██║██╔══██╗
███████╗██║ ╚═╝ ██║██║     ███████╗██║  ██║╚██████╔╝██║  ██║
╚══════╝╚═╝     ╚═╝╚═╝     ╚══════╝╚═╝  ╚═╝ ╚═════╝ ╚═╝  ╚═╝
%c
⚔️ In the grim darkness of the dev console, there is only code...`;console.log(r,i,a),window.__hasLoggedConsoleBrand=!0}catch{console.log("⚔️ In the grim darkness of the dev console, there is only code..."),window.__hasLoggedConsoleBrand=!0}})()</script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>