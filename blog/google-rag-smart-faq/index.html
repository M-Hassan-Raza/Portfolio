<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output) | Muhammad Hassan Raza</title><meta name=keywords content="FAQ Bot,RAG,Retrieval Augmented Generation,Gemini API,ChromaDB,Embeddings,Structured Output,Vector Store,Python,AI Agents"><meta name=description content="
  Introduction
  
    If you've ever found yourself digging through product manuals, company
    wikis, or lengthy documents just to find a simple answer, you know the pain.
    The fact you're reading this suggests you're interested in how
    Generative AI can make that process less painful. Stick
    around for a few minutes, and I'll walk you through how we built a smarter
    FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and
    structured output. This isn't just another chatbot; it's designed to give
    reliable, context-aware answers based only on
    provided information, minimizing the risk of making things up
    (hallucination). This example uses Google Car manuals, but the principles
    apply anywhere you have a set of documents you need to query effectively.
    I'm sharing my journey building this; it's a practical demonstration, not a
    definitive guide, so adapt the ideas to your needs!
  
  
  
    The Problem: Dumb Bots and Information Overload
  
  
    Traditional search methods or basic chatbots often fall short when dealing
    with specific document sets:
  "><meta name=author content="Muhammad Hassan Raza"><link rel=canonical href=https://mhassan.dev/blog/google-rag-smart-faq/><link crossorigin=anonymous href=/assets/css/stylesheet.e2484083c568226186863acda8452ec66f0ea5bc7f4834243c6c0f77aaf2a2b7.css integrity="sha256-4khAg8VoImGGhjrNqEUuxm8Opbx/SDQkPGwPd6ryorc=" rel="preload stylesheet" as=style><link rel=icon href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=32x32 href=https://mhassan.dev/assets/favicon.svg><link rel=apple-touch-icon href=https://mhassan.dev/apple-touch-icon.png><link rel=mask-icon href=https://mhassan.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mhassan.dev/blog/google-rag-smart-faq/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=/css/extended/extended.css as=style><link rel=stylesheet href=/css/extended/extended.css><link rel=preload href=/fonts/font-family.css as=style><link rel=stylesheet href=/fonts/font-family.css><link rel=preload href=/fonts/Manrope-Regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Medium.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Bold.woff2 as=font type=font/woff2 crossorigin><script async src=https://cloud.umami.is/script.js data-website-id=30c7d9d6-abac-4c52-b85a-c0234f863d22></script><script async src="https://www.googletagmanager.com/gtag/js?id=%7b%7d"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","{}")</script><meta property="og:url" content="https://mhassan.dev/blog/google-rag-smart-faq/"><meta property="og:site_name" content="Muhammad Hassan Raza"><meta property="og:title" content="Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"><meta property="og:description" content=" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how Generative AI can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give reliable, context-aware answers based only on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets: "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-04-20T10:00:00+05:00"><meta property="article:modified_time" content="2025-04-20T10:00:00+05:00"><meta property="article:tag" content="FAQ Bot"><meta property="article:tag" content="RAG"><meta property="article:tag" content="Retrieval Augmented Generation"><meta property="article:tag" content="Gemini API"><meta property="article:tag" content="ChromaDB"><meta property="article:tag" content="Embeddings"><meta property="og:image" content="https://mhassan.dev/assets/faq-bot.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mhassan.dev/assets/faq-bot.jpg"><meta name=twitter:title content="Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"><meta name=twitter:description content="
  Introduction
  
    If you've ever found yourself digging through product manuals, company
    wikis, or lengthy documents just to find a simple answer, you know the pain.
    The fact you're reading this suggests you're interested in how
    Generative AI can make that process less painful. Stick
    around for a few minutes, and I'll walk you through how we built a smarter
    FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and
    structured output. This isn't just another chatbot; it's designed to give
    reliable, context-aware answers based only on
    provided information, minimizing the risk of making things up
    (hallucination). This example uses Google Car manuals, but the principles
    apply anywhere you have a set of documents you need to query effectively.
    I'm sharing my journey building this; it's a practical demonstration, not a
    definitive guide, so adapt the ideas to your needs!
  
  
  
    The Problem: Dumb Bots and Information Overload
  
  
    Traditional search methods or basic chatbots often fall short when dealing
    with specific document sets:
  "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mhassan.dev/blog/"},{"@type":"ListItem","position":2,"name":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","item":"https://mhassan.dev/blog/google-rag-smart-faq/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","name":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","description":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how Generative AI can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give reliable, context-aware answers based only on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets: ","keywords":["FAQ Bot","RAG","Retrieval Augmented Generation","Gemini API","ChromaDB","Embeddings","Structured Output","Vector Store","Python","AI Agents"],"articleBody":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how Generative AI can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give reliable, context-aware answers based only on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets: Information Overload: Manually searching large documents is time-consuming and inefficient. Generic LLM Limitations: Large Language Models (LLMs) are powerful, but they lack specific, up-to-date knowledge about your documents unless explicitly trained on them (which is often impractical). Hallucination Risk: When asked about information outside their training data, LLMs might confidently invent answers that sound plausible but are incorrect. This is unacceptable for reliable FAQ systems. Inconsistent Outputs: Getting answers in a usable, predictable format can be challenging with free-form text generation. We need a system that answers questions accurately based only on a given set of documents and provides answers in a consistent, structured way. The Solution: RAG + Gemini API Our approach combines Retrieval Augmented Generation (RAG) with the capabilities of the Gemini API. At a high level, the user interacts with the system like this: Figure 1: High-Level RAG Interaction Flow. This involves three main steps in the underlying RAG pipeline: 1. Indexing: Convert the source documents (Google Car manuals) into numerical representations (embeddings) using the Gemini text-embedding-004 model and store them in a vector database (ChromaDB). This allows for efficient similarity searches. This setup process is crucial for enabling fast retrieval later. Figure 2: The Document Indexing Flow. 2. Retrieval: When a user asks a question, embed the question using the same model and search the vector database to find the most relevant document chunks based on semantic similarity. Figure 3: The Query Retrieval Flow. 3. Generation: Pass the original question and the retrieved document chunks as context to a powerful LLM (like gemini-2.0-flash). Instruct the model to answer the question based only on the provided context. Alongside the RAG structure, we leverage specific Gemini API Features: High-Quality Embeddings: text-embedding-004 provides embeddings suitable for finding semantically similar text. Powerful Generation: gemini-2.0-flash can synthesize answers based on the retrieved context. Structured Output (JSON Mode): We instruct Gemini to return the answer and a confidence score in a predictable JSON format, making it easy for applications to use the output. Optional Grounding: We can even add Google Search as a tool if the local documents don't suffice (though our primary goal here is document-based Q\u0026A). Implementation Highlights 1. Custom Embedding Function for ChromaDB:\nWe need to tell ChromaDB how to generate embeddings using the Gemini API. chromadb import Documents, EmbeddingFunction, Embeddings from google.api_core import retry from google import genai from google.genai import types is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503}) class GeminiEmbeddingFunction(EmbeddingFunction): document_mode = True # Toggle between indexing docs and embedding queries @retry.Retry(predicate=is_retriable) def __call__(self, input_texts: Documents) -\u003e Embeddings: task = \"retrieval_document\" if self.document_mode else \"retrieval_query\" print(f\"Embedding {'documents' if self.document_mode else 'query'} ({len(input_texts)})...\") try: # Assuming 'client' is initialized Google GenAI client response = client.models.embed_content( model=\"models/text-embedding-004\", contents=input_texts, config=types.EmbedContentConfig(task_type=task), # Specify task type ) return [e.values for e in response.embeddings] except Exception as e: print(f\"Error during embedding: {e}\") return [[] for _ in input_texts] \u003c/div\u003e 2. Setting up ChromaDB and Indexing:\nWe create a ChromaDB collection and add our documents. get_or_create_collection makes this idempotent.\n# --- 5. Setup ChromaDB Vector Store --- import chromadb import time print(\"Setting up ChromaDB...\") DB_NAME = \"googlecar_faq_db\" embed_fn = GeminiEmbeddingFunction() chroma_client = chromadb.Client() # In-memory client try: db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn) print(f\"Collection '{DB_NAME}' ready. Current count: {db.count()}\") # Assuming 'documents' and 'doc_ids' are defined earlier if db.count() \u003c len(documents): print(f\"Adding/Updating documents in '{DB_NAME}'...\") embed_fn.document_mode = True # Set mode for indexing db.upsert(documents=documents, ids=doc_ids) # Use upsert for safety time.sleep(2) # Allow indexing to settle print(f\"Documents added/updated. New count: {db.count()}\") else: print(\"Documents already seem to be indexed.\") except Exception as e: print(f\"Error setting up ChromaDB collection: {e}\") raise SystemExit(\"ChromaDB setup failed. Exiting.\") 3. Retrieving Relevant Documents:\nThis function takes the user query, embeds it (using document_mode=False), and searches ChromaDB.\n# --- 6. Define Retrieval Function --- def retrieve_documents(query: str, n_results: int = 1) -\u003e list[str]: print(f\"\\nRetrieving documents for query: '{query}'\") embed_fn.document_mode = False # Switch to query mode try: results = db.query(query_texts=[query], n_results=n_results) if results and results.get(\"documents\"): retrieved_docs = results[\"documents\"][0] print(f\"Retrieved {len(retrieved_docs)} documents.\") return retrieved_docs else: print(\"No documents retrieved.\") return [] except Exception as e: print(f\"Error querying ChromaDB: {e}\") return [] 4. Generating the Structured Answer:\nHere's the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.\n# --- 7. Define Structured Output Schema --- from typing_extensions import Literal from pydantic import BaseModel class AnswerWithConfidence(BaseModel): answer: str confidence: Literal[\"High\", \"Medium\", \"Low\"] # --- 8. Define Augmented Generation Function --- def generate_structured_answer(query: str, context_docs: list[str]) -\u003e dict | None: if not context_docs: print(\"No context provided, cannot generate answer.\") return { \"answer\": \"I couldn't find relevant information in the provided documents to answer this question.\", \"confidence\": \"Low\", } context = \"\\n---\\n\".join(context_docs) prompt = f\\\"\\\"\\\"You are an AI assistant answering questions about a Google car based ONLY on the provided documents. Context Documents: --- {context} --- Question: {query} Based *only* on the information in the context documents above, answer the question. Also, assess your confidence in the answer based *only* on the provided text: - \"High\" if the answer is directly and clearly stated in the documents. - \"Medium\" if the answer can be inferred but isn't explicitly stated. - \"Low\" if the documents don't seem to contain the answer or are ambiguous. Return your response ONLY as a JSON object with the keys \"answer\" and \"confidence\". Example format: { \"answer\": \"Your answer here.\", \"confidence\": \"High/Medium/Low\" } \\\"\\\"\\\" try: generation_config = types.GenerateContentConfig( temperature=0.2, response_mime_type=\"application/json\", # Request JSON response_schema=AnswerWithConfidence, # Provide the schema ) # Assuming 'client' is initialized Google GenAI client response = client.models.generate_content( model=\"gemini-2.0-flash\", contents=prompt, generation_config=generation_config, # Pass the config object ) # Safe access to parsed output if ( response.candidates and response.candidates[0].content and response.candidates[0].content.parts ): parsed_output = response.candidates[0].content.parts[0].function_call # Fallback check if .parsed is used if not parsed_output and hasattr( response.candidates[0].content.parts[0], \"parsed\" ): parsed_output = response.candidates[0].content.parts[0].parsed if isinstance(parsed_output, dict) and \"answer\" in parsed_output and \"confidence\" in parsed_output: print(\"Generated Answer:\", parsed_output) return parsed_output else: print(\"Warning: Could not extract valid JSON from response.\") print(\"Raw response part:\", response.candidates[0].content.parts[0]) # Attempt to parse the text part if it exists and looks like JSON try: import json text_part = response.candidates[0].content.parts[0].text if text_part and text_part.strip().startswith(\"{\") and text_part.strip().endswith(\"}\"): parsed_json = json.loads(text_part) if isinstance(parsed_json, dict) and \"answer\" in parsed_json and \"confidence\" in parsed_json: print(\"Recovered JSON from text part:\", parsed_json) return parsed_json except Exception as json_e: print(f\"Could not parse text part as JSON: {json_e}\") print(\"Error: Could not generate/parse structured response correctly.\") return {\"answer\": \"Error: Could not generate or parse the structured response from the AI.\", \"confidence\": \"Low\"} except Exception as e: print(f\"Error during content generation call: {e}\") return {\"answer\": f\"Error during generation API call: {e}\", \"confidence\": \"Low\"} Tip: Ensure your API key is correctly set up in Kaggle Secrets (GOOGLE_API_KEY). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity). Limitations and Future Work This implementation is a great starting point, but it has limitations:\nDocument Quality: The RAG system's effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out. Retrieval Accuracy: Simple similarity search might not always retrieve the perfect chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this. Structured Output Failures: While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added. Limited Context Handling (within LLM): While RAG provides context, the LLM itself still has limits on how much context it can process effectively in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM. Static Knowledge: The bot only knows what's in the ChromaDB index. It doesn't learn automatically. Updates require re-indexing. Future Enhancements: Implement Google Search grounding as a fallback when confidence is low or documents are missing. Add conversation memory for multi-turn interactions. Explore more sophisticated retrieval techniques. Build a simple UI (e.g., using Gradio or Streamlit). Fine-tune an embedding model specifically for the car manual domain (though text-embedding-004 is quite capable). Conclusion Building this FAQ bot demonstrates how combining RAG with Gemini's embedding and generation capabilities, especially its structured output mode, can create powerful and reliable AI-driven Q\u0026A systems. By grounding the LLM's responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience. Key Takeaways:\nRAG grounds LLM answers in your specific data. Gemini Embeddings + ChromaDB enable efficient document retrieval. Structured Output (JSON Mode) enhances reliability and integrability. Confidence Scores add a layer of trustworthiness. This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search. I hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to ask questions or leave a comment with your thoughts or own implementations!\n","wordCount":"1639","inLanguage":"en","image":"https://mhassan.dev/assets/faq-bot.jpg","datePublished":"2025-04-20T10:00:00+05:00","dateModified":"2025-04-20T10:00:00+05:00","author":{"@type":"Person","name":"Muhammad Hassan Raza"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mhassan.dev/blog/google-rag-smart-faq/"},"publisher":{"@type":"Organization","name":"Muhammad Hassan Raza","logo":{"@type":"ImageObject","url":"https://mhassan.dev/assets/favicon.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mhassan.dev/ accesskey=h title="Muhammad Hassan Raza (Alt + H)">Muhammad Hassan Raza</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mhassan.dev/book-a-call/ title="Book a Call"><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M8.26 1.289l-1.564.772c-5.793 3.02 2.798 20.944 9.31 20.944.46.0.904-.094 1.317-.284l1.542-.755-2.898-5.594-1.54.754c-.181.087-.384.134-.597.134-2.561.0-6.841-8.204-4.241-9.596l1.546-.763L8.26 1.289zM16.006 24C10.326 24 3.785 12.886 3.785 6.168c0-2.419.833-4.146 2.457-4.992l2.382-1.176 3.857 7.347-2.437 1.201c-1.439.772 2.409 8.424 3.956 7.68l2.399-1.179 3.816 7.36s-2.36 1.162-2.476 1.215c-.547.251-1.129.376-1.733.376"/></svg>Book a Call</span></a></li><li><a href=https://mhassan.dev/projects/ title=Projects><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M11 5h13v17H0V2h8l3 3zM1 3v18h22V6H10.586l-3-3H1z"/></svg>Projects</span></a></li><li><a href=https://mhassan.dev/about/ title=About><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="-1 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M12 0c6.623.0 12 5.377 12 12s-5.377 12-12 12S0 18.623.0 12 5.377.0 12 0zm8.127 19.41c-.282-.401-.772-.654-1.624-.85-3.848-.906-4.097-1.501-4.352-2.059-.259-.565-.19-1.23.205-1.977 1.726-3.257 2.09-6.024 1.027-7.79C14.709 5.615 13.508 5 12 5c-1.521.0-2.732.626-3.409 1.763-1.066 1.789-.693 4.544 1.049 7.757.402.742.476 1.406.22 1.974-.265.586-.611 1.19-4.365 2.066-.852.196-1.342.449-1.623.848C5.884 21.615 8.782 23 12 23s6.115-1.385 8.127-3.59zm.65-.782C22.172 16.784 23 14.488 23 12c0-6.071-4.929-11-11-11S1 5.929 1 12c0 2.487.827 4.783 2.222 6.626.409-.452 1.049-.81 2.049-1.041 2.025-.462 3.376-.836 3.678-1.502.122-.272.061-.628-.188-1.087-1.917-3.535-2.282-6.641-1.03-8.745C8.584 4.82 10.139 4 12 4c1.845.0 3.391.808 4.24 2.218 1.251 2.079.896 5.195-1 8.774-.245.463-.304.821-.179 1.094.305.668 1.644 1.038 3.667 1.499 1 .23 1.64.59 2.049 1.043z"/></svg>About</span></a></li><li><a href=https://mhassan.dev/search/ title="Search (Alt + /)" accesskey=/><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M15.853 16.56C14.17 18.077 11.942 19 9.5 19 4.257 19 0 14.743.0 9.5S4.257.0 9.5.0 19 4.257 19 9.5c0 2.442-.923 4.67-2.44 6.353l7.44 7.44-.707.707-7.44-7.44zm-6.353-15.56c4.691.0 8.5 3.809 8.5 8.5S14.191 18 9.5 18 1 14.191 1 9.5 4.809 1 9.5 1z"/></svg>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mhassan.dev/>Home</a>&nbsp;»&nbsp;<a href=https://mhassan.dev/blog/>Blog</a></div><h1 class=post-title>Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)</h1><div class=post-meta><span title='2025-04-20 10:00:00 +0500 +0500'>April 20, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Muhammad Hassan Raza</div></header><figure class=entry-cover><img loading=lazy src=https://mhassan.dev/assets/faq-bot.jpg alt="Intelligent FAQ Bot"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=# aria-label=Introduction>Introduction</a></li><li><a href=# aria-label="The Solution: RAG + Gemini API">The Solution: RAG + Gemini API</a></li><li><a href=# aria-label="Implementation Highlights">Implementation Highlights</a></li><li><a href=# aria-label="Limitations and Future Work">Limitations and Future Work</a><ul><li><a href=# aria-label="Future Enhancements:">Future Enhancements:</a></li></ul></li><li><a href=# aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><div style=text-align:justify><h2><span style=color:#ffb4a2>Introduction</span></h2><div style=text-align:justify>If you've ever found yourself digging through product manuals, company
wikis, or lengthy documents just to find a simple answer, you know the pain.
The fact you're reading this suggests you're interested in how
<strong>Generative AI</strong> can make that process less painful. Stick
around for a few minutes, and I'll walk you through how we built a smarter
FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and
structured output. This isn't just another chatbot; it's designed to give
<strong>reliable, context-aware answers</strong> based <em>only</em> on
provided information, minimizing the risk of making things up
(hallucination). This example uses Google Car manuals, but the principles
apply anywhere you have a set of documents you need to query effectively.
I'm sharing my journey building this; it's a practical demonstration, not a
definitive guide, so adapt the ideas to your needs!</div><hr><h2><span style=color:#ffb4a2>The Problem: Dumb Bots and Information Overload</span></h2><p>Traditional search methods or basic chatbots often fall short when dealing
with specific document sets:</p><ul><li><strong><span style=color:#8ac7db>Information Overload:</span></strong>
Manually searching large documents is time-consuming and inefficient.</li><li><strong><span style=color:#8ac7db>Generic LLM Limitations:</span></strong>
Large Language Models (LLMs) are powerful, but they lack specific,
up-to-date knowledge about <em>your</em> documents unless explicitly
trained on them (which is often impractical).</li><li><strong><span style=color:#8ac7db>Hallucination Risk:</span></strong>
When asked about information outside their training data, LLMs might
confidently invent answers that sound plausible but are incorrect. This is
unacceptable for reliable FAQ systems.</li><li><strong><span style=color:#8ac7db>Inconsistent Outputs:</span></strong>
Getting answers in a usable, predictable format can be challenging with
free-form text generation.</li></ul><p>We need a system that answers questions accurately based <em>only</em> on a
given set of documents and provides answers in a consistent, structured way.</p><hr><h2><span style=color:#ffb4a2>The Solution: RAG + Gemini API</span></h2><div style=text-align:justify>Our approach combines
<strong>Retrieval Augmented Generation (RAG)</strong> with the capabilities
of the <strong>Gemini API</strong>. At a high level, the user interacts with
the system like this:</div><figure style=text-align:center><img src=/assets/RAG-Flow.webp alt="High-Level RAG Flow Diagram: User Query -> RAG System -> Grounded Answer" style="max-width:80%;height:auto;margin:1em auto;display:block"><figcaption>Figure 1: High-Level RAG Interaction Flow.</figcaption></figure><div style=text-align:justify>This involves three main steps in the underlying RAG pipeline:<br><br>1. <strong><span style=color:#8ac7db>Indexing:</span></strong> Convert the
source documents (Google Car manuals) into numerical representations
(embeddings) using the Gemini <code>text-embedding-004</code> model and
store them in a vector database (ChromaDB). This allows for efficient
similarity searches. This setup process is crucial for enabling fast
retrieval later.</div><figure style=text-align:center><img src=/assets/Indexing-Flow.webp alt="Indexing Flow Diagram: Documents -> Gemini Embedding -> Vector Embeddings -> ChromaDB Vector Store" style="max-width:80%;height:auto;margin:1em auto;display:block"><figcaption>Figure 2: The Document Indexing Flow.</figcaption></figure><div style=text-align:justify>2. <strong><span style=color:#8ac7db>Retrieval:</span></strong> When a
user asks a question, embed the question using the same model and search the
vector database to find the most relevant document chunks based on semantic
similarity.</div><figure style=text-align:center><img src="/assets/Retreival Flow.webp" alt="Retrieval Flow Diagram: User Query -> Gemini Embedding -> Query Vector -> ChromaDB -> Relevant Document Chunks" style="max-width:80%;height:auto;margin:1em auto;display:block"><figcaption>Figure 3: The Query Retrieval Flow.</figcaption></figure><div style=text-align:justify>3. <strong><span style=color:#8ac7db>Generation:</span></strong> Pass the
original question and the retrieved document chunks as context to a powerful
LLM (like <code>gemini-2.0-flash</code>). Instruct the model to answer the
question <em>based only on the provided context</em>.<br><br>Alongside the RAG structure, we leverage specific
<strong>Gemini API Features</strong>:<ul><li><strong><span style=color:#8ac7db>High-Quality Embeddings:</span></strong>
<code>text-embedding-004</code> provides embeddings suitable for finding
semantically similar text.</li><li><strong><span style=color:#8ac7db>Powerful Generation:</span></strong>
<code>gemini-2.0-flash</code> can synthesize answers based on the
retrieved context.</li><li><strong><span style=color:#8ac7db>Structured Output (JSON Mode):</span></strong>
We instruct Gemini to return the answer and a confidence score in a
predictable JSON format, making it easy for applications to use the
output.</li><li><strong><span style=color:#8ac7db>Optional Grounding:</span></strong>
We can even add Google Search as a tool if the local documents don't
suffice (though our primary goal here is document-based Q&A).</li></ul></div><hr><h2><span style=color:#ffb4a2>Implementation Highlights</span></h2><p><strong>1.
<span style=color:#8ac7db>Custom Embedding Function for ChromaDB:</span></strong><br>We need to tell ChromaDB how to generate embeddings using the Gemini API.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>chromadb <span style=color:#f92672>import</span> Documents<span style=color:#f92672>,</span> EmbeddingFunction<span style=color:#f92672>,</span> Embeddings <span style=color:#f92672>from</span> google.api_core
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> retry <span style=color:#f92672>from</span> google <span style=color:#f92672>import</span> genai <span style=color:#f92672>from</span> google.genai <span style=color:#f92672>import</span> types
</span></span><span style=display:flex><span>is_retriable <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> e: (isinstance(e, genai<span style=color:#f92672>.</span>errors<span style=color:#f92672>.</span>APIError) <span style=color:#f92672>and</span> e<span style=color:#f92672>.</span>code <span style=color:#f92672>in</span>
</span></span><span style=display:flex><span>{<span style=color:#ae81ff>429</span>, <span style=color:#ae81ff>503</span>}) <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GeminiEmbeddingFunction</span>(EmbeddingFunction): document_mode <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>True</span> <span style=color:#75715e># Toggle between indexing docs and embedding queries</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@retry.Retry</span>(predicate<span style=color:#f92672>=</span>is_retriable) <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, input_texts:
</span></span><span style=display:flex><span>Documents) <span style=color:#f92672>-&gt;</span> Embeddings: task <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;retrieval_document&#34;</span> <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>document_mode
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;retrieval_query&#34;</span> print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Embedding </span><span style=color:#e6db74>{</span><span style=color:#e6db74>&#39;documents&#39;</span> <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>document_mode
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;query&#39;</span><span style=color:#e6db74>}</span><span style=color:#e6db74> (</span><span style=color:#e6db74>{</span>len(input_texts)<span style=color:#e6db74>}</span><span style=color:#e6db74>)...&#34;</span>) <span style=color:#66d9ef>try</span>: <span style=color:#75715e># Assuming &#39;client&#39; is</span>
</span></span><span style=display:flex><span>initialized Google GenAI client response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>embed_content(
</span></span><span style=display:flex><span>model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;models/text-embedding-004&#34;</span>, contents<span style=color:#f92672>=</span>input_texts,
</span></span><span style=display:flex><span>config<span style=color:#f92672>=</span>types<span style=color:#f92672>.</span>EmbedContentConfig(task_type<span style=color:#f92672>=</span>task), <span style=color:#75715e># Specify task type ) return</span>
</span></span><span style=display:flex><span>[e<span style=color:#f92672>.</span>values <span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> response<span style=color:#f92672>.</span>embeddings] <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e: print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error</span>
</span></span><span style=display:flex><span>during embedding: {e}<span style=color:#e6db74>&#34;) return [[] for _ in input_texts]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;/</span>div<span style=color:#f92672>&gt;</span>
</span></span></code></pre></div><p><strong>2. <span style=color:#8ac7db>Setting up ChromaDB and Indexing:</span></strong><br>We create a ChromaDB collection and add our documents. <code>get_or_create_collection</code> makes this idempotent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 5. Setup ChromaDB Vector Store ---</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> chromadb
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Setting up ChromaDB...&#34;</span>)
</span></span><span style=display:flex><span>DB_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;googlecar_faq_db&#34;</span>
</span></span><span style=display:flex><span>embed_fn <span style=color:#f92672>=</span> GeminiEmbeddingFunction()
</span></span><span style=display:flex><span>chroma_client <span style=color:#f92672>=</span> chromadb<span style=color:#f92672>.</span>Client()  <span style=color:#75715e># In-memory client</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    db <span style=color:#f92672>=</span> chroma_client<span style=color:#f92672>.</span>get_or_create_collection(name<span style=color:#f92672>=</span>DB_NAME, embedding_function<span style=color:#f92672>=</span>embed_fn)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Collection &#39;</span><span style=color:#e6db74>{</span>DB_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39; ready. Current count: </span><span style=color:#e6db74>{</span>db<span style=color:#f92672>.</span>count()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Assuming &#39;documents&#39; and &#39;doc_ids&#39; are defined earlier</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> db<span style=color:#f92672>.</span>count() <span style=color:#f92672>&lt;</span> len(documents):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Adding/Updating documents in &#39;</span><span style=color:#e6db74>{</span>DB_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;...&#34;</span>)
</span></span><span style=display:flex><span>        embed_fn<span style=color:#f92672>.</span>document_mode <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>  <span style=color:#75715e># Set mode for indexing</span>
</span></span><span style=display:flex><span>        db<span style=color:#f92672>.</span>upsert(documents<span style=color:#f92672>=</span>documents, ids<span style=color:#f92672>=</span>doc_ids)  <span style=color:#75715e># Use upsert for safety</span>
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>2</span>)  <span style=color:#75715e># Allow indexing to settle</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Documents added/updated. New count: </span><span style=color:#e6db74>{</span>db<span style=color:#f92672>.</span>count()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Documents already seem to be indexed.&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error setting up ChromaDB collection: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>SystemExit</span>(<span style=color:#e6db74>&#34;ChromaDB setup failed. Exiting.&#34;</span>)
</span></span></code></pre></div><p><strong>3. <span style=color:#8ac7db>Retrieving Relevant Documents:</span></strong><br>This function takes the user query, embeds it (using <code>document_mode=False</code>), and searches ChromaDB.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 6. Define Retrieval Function ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>retrieve_documents</span>(query: str, n_results: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>-&gt;</span> list[str]:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Retrieving documents for query: &#39;</span><span style=color:#e6db74>{</span>query<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;&#34;</span>)
</span></span><span style=display:flex><span>    embed_fn<span style=color:#f92672>.</span>document_mode <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>  <span style=color:#75715e># Switch to query mode</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> db<span style=color:#f92672>.</span>query(query_texts<span style=color:#f92672>=</span>[query], n_results<span style=color:#f92672>=</span>n_results)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> results <span style=color:#f92672>and</span> results<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;documents&#34;</span>):
</span></span><span style=display:flex><span>            retrieved_docs <span style=color:#f92672>=</span> results[<span style=color:#e6db74>&#34;documents&#34;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Retrieved </span><span style=color:#e6db74>{</span>len(retrieved_docs)<span style=color:#e6db74>}</span><span style=color:#e6db74> documents.&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> retrieved_docs
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;No documents retrieved.&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error querying ChromaDB: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span></code></pre></div><p><strong>4. <span style=color:#8ac7db>Generating the Structured Answer:</span></strong><br>Here's the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 7. Define Structured Output Schema ---</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing_extensions <span style=color:#f92672>import</span> Literal
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AnswerWithConfidence</span>(BaseModel):
</span></span><span style=display:flex><span>    answer: str
</span></span><span style=display:flex><span>    confidence: Literal[<span style=color:#e6db74>&#34;High&#34;</span>, <span style=color:#e6db74>&#34;Medium&#34;</span>, <span style=color:#e6db74>&#34;Low&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- 8. Define Augmented Generation Function ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_structured_answer</span>(query: str, context_docs: list[str]) <span style=color:#f92672>-&gt;</span> dict <span style=color:#f92672>|</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> context_docs:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;No context provided, cannot generate answer.&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;I couldn&#39;t find relevant information in the provided documents to answer this question.&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;Low&#34;</span>,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    context <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>---</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(context_docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> f\<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\&#34;\&#34;</span><span style=color:#e6db74>You are an AI assistant answering questions about a Google car based ONLY on the provided documents.</span>
</span></span><span style=display:flex><span>Context Documents:
</span></span><span style=display:flex><span><span style=color:#f92672>---</span>
</span></span><span style=display:flex><span>{context}
</span></span><span style=display:flex><span><span style=color:#f92672>---</span>
</span></span><span style=display:flex><span>Question: {query}
</span></span><span style=display:flex><span>Based <span style=color:#f92672>*</span>only<span style=color:#f92672>*</span> on the information <span style=color:#f92672>in</span> the context documents above, answer the question<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Also, assess your confidence <span style=color:#f92672>in</span> the answer based <span style=color:#f92672>*</span>only<span style=color:#f92672>*</span> on the provided text:
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#e6db74>&#34;High&#34;</span> <span style=color:#66d9ef>if</span> the answer <span style=color:#f92672>is</span> directly <span style=color:#f92672>and</span> clearly stated <span style=color:#f92672>in</span> the documents<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#e6db74>&#34;Medium&#34;</span> <span style=color:#66d9ef>if</span> the answer can be inferred but isn<span style=color:#e6db74>&#39;t explicitly stated.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#e6db74>&#34;Low&#34;</span> <span style=color:#66d9ef>if</span> the documents don<span style=color:#e6db74>&#39;t seem to contain the answer or are ambiguous.</span>
</span></span><span style=display:flex><span>Return your response ONLY <span style=color:#66d9ef>as</span> a JSON object <span style=color:#66d9ef>with</span> the keys <span style=color:#e6db74>&#34;answer&#34;</span> <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;confidence&#34;</span><span style=color:#f92672>.</span> Example format:
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Your answer here.&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;High/Medium/Low&#34;</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>\<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\&#34;\&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        generation_config <span style=color:#f92672>=</span> types<span style=color:#f92672>.</span>GenerateContentConfig(
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>,
</span></span><span style=display:flex><span>            response_mime_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>,  <span style=color:#75715e># Request JSON</span>
</span></span><span style=display:flex><span>            response_schema<span style=color:#f92672>=</span>AnswerWithConfidence,  <span style=color:#75715e># Provide the schema</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Assuming &#39;client&#39; is initialized Google GenAI client</span>
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>generate_content(
</span></span><span style=display:flex><span>            model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gemini-2.0-flash&#34;</span>,
</span></span><span style=display:flex><span>            contents<span style=color:#f92672>=</span>prompt,
</span></span><span style=display:flex><span>            generation_config<span style=color:#f92672>=</span>generation_config,  <span style=color:#75715e># Pass the config object</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Safe access to parsed output</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (
</span></span><span style=display:flex><span>            response<span style=color:#f92672>.</span>candidates
</span></span><span style=display:flex><span>            <span style=color:#f92672>and</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content
</span></span><span style=display:flex><span>            <span style=color:#f92672>and</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts
</span></span><span style=display:flex><span>        ):
</span></span><span style=display:flex><span>            parsed_output <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>function_call
</span></span><span style=display:flex><span>            <span style=color:#75715e># Fallback check if .parsed is used</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> parsed_output <span style=color:#f92672>and</span> hasattr(
</span></span><span style=display:flex><span>                response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#34;parsed&#34;</span>
</span></span><span style=display:flex><span>            ):
</span></span><span style=display:flex><span>                parsed_output <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>parsed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> isinstance(parsed_output, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;answer&#34;</span> <span style=color:#f92672>in</span> parsed_output <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;confidence&#34;</span> <span style=color:#f92672>in</span> parsed_output:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;Generated Answer:&#34;</span>, parsed_output)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> parsed_output
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;Warning: Could not extract valid JSON from response.&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;Raw response part:&#34;</span>, response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>                <span style=color:#75715e># Attempt to parse the text part if it exists and looks like JSON</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    <span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    text_part <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>text
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> text_part <span style=color:#f92672>and</span> text_part<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#34;{&#34;</span>) <span style=color:#f92672>and</span> text_part<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>endswith(<span style=color:#e6db74>&#34;}&#34;</span>):
</span></span><span style=display:flex><span>                        parsed_json <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(text_part)
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>if</span> isinstance(parsed_json, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;answer&#34;</span> <span style=color:#f92672>in</span> parsed_json <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;confidence&#34;</span> <span style=color:#f92672>in</span> parsed_json:
</span></span><span style=display:flex><span>                            print(<span style=color:#e6db74>&#34;Recovered JSON from text part:&#34;</span>, parsed_json)
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>return</span> parsed_json
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> json_e:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Could not parse text part as JSON: </span><span style=color:#e6db74>{</span>json_e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Error: Could not generate/parse structured response correctly.&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Error: Could not generate or parse the structured response from the AI.&#34;</span>, <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;Low&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during content generation call: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during generation API call: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;Low&#34;</span>}
</span></span></code></pre></div><blockquote><strong>Tip:</strong> Ensure your API key is correctly set up in Kaggle Secrets (<code>GOOGLE_API_KEY</code>). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity).</blockquote><hr><h2><span style=color:#ffb4a2>Limitations and Future Work</span></h2><p>This implementation is a great starting point, but it has limitations:</p><ul><li><strong><span style=color:#8ac7db>Document Quality:</span></strong> The RAG system's effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out.</li><li><strong><span style=color:#8ac7db>Retrieval Accuracy:</span></strong> Simple similarity search might not always retrieve the <em>perfect</em> chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this.</li><li><strong><span style=color:#8ac7db>Structured Output Failures:</span></strong> While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added.</li><li><strong><span style=color:#8ac7db>Limited Context Handling (within LLM):</span></strong> While RAG provides context, the LLM itself still has limits on how much context it can process <em>effectively</em> in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM.</li><li><strong><span style=color:#8ac7db>Static Knowledge:</span></strong> The bot only knows what's in the ChromaDB index. It doesn't learn automatically. Updates require re-indexing.</li></ul><h3><span style=color:#ffb4a2>Future Enhancements:</span></h3><ul><li>Implement Google Search grounding as a fallback when confidence is low or documents are missing.</li><li>Add conversation memory for multi-turn interactions.</li><li>Explore more sophisticated retrieval techniques.</li><li>Build a simple UI (e.g., using Gradio or Streamlit).</li><li>Fine-tune an embedding model specifically for the car manual domain (though <code>text-embedding-004</code> is quite capable).</li></ul><hr><h2><span style=color:#ffb4a2>Conclusion</span></h2><div style=text-align:justify>Building this FAQ bot demonstrates how combining RAG with Gemini's embedding and generation capabilities, especially its structured output mode, can create powerful and <strong>reliable</strong> AI-driven Q&A systems. By grounding the LLM's responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience.</div><p><strong>Key Takeaways:</strong></p><ul><li><strong><span style=color:#8ac7db>RAG</span></strong> grounds LLM answers in your specific data.</li><li><strong><span style=color:#8ac7db>Gemini Embeddings + ChromaDB</span></strong> enable efficient document retrieval.</li><li><strong><span style=color:#8ac7db>Structured Output (JSON Mode)</span></strong> enhances reliability and integrability.</li><li><strong><span style=color:#8ac7db>Confidence Scores</span></strong> add a layer of trustworthiness.</li></ul><div style=text-align:justify>This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search.<p>I hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to <strong>ask questions</strong> or <strong>leave a comment</strong> with your thoughts or own implementations!</p></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://mhassan.dev/tags/faq-bot>FAQ Bot</a></li><li><a href=https://mhassan.dev/tags/rag>RAG</a></li><li><a href=https://mhassan.dev/tags/retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=https://mhassan.dev/tags/gemini-api>Gemini API</a></li><li><a href=https://mhassan.dev/tags/chromadb>ChromaDB</a></li><li><a href=https://mhassan.dev/tags/embeddings>Embeddings</a></li><li><a href=https://mhassan.dev/tags/structured-output>Structured Output</a></li><li><a href=https://mhassan.dev/tags/vector-store>Vector Store</a></li><li><a href=https://mhassan.dev/tags/python>Python</a></li><li><a href=https://mhassan.dev/tags/ai-agents>AI Agents</a></li></ul><nav class=paginav><a class=next href=https://mhassan.dev/blog/fyp-guidance/><span class=title>Next »</span><br><span>Final Year Project (FYP) Guide for Students</span></a></nav></footer><div class=giscus-container><script src=https://giscus.app/client.js data-repo=M-Hassan-Raza/Portfolio data-repo-id=R_kgDON3Oajw data-category=General data-category-id=DIC_kwDON3Oaj84Cm3y9 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></div><script>function getTheme(){return document.body.classList.contains("dark")?"dark":"light"}function updateGiscusTheme(){const e=document.querySelector("iframe.giscus-frame");if(!e){setTimeout(updateGiscusTheme,300);return}const t=getTheme();e.contentWindow.postMessage({giscus:{setConfig:{theme:t}}},"https://giscus.app")}localStorage.getItem("pref-theme")==="dark"&&updateGiscusTheme();const observer=new MutationObserver(()=>{updateGiscusTheme()});observer.observe(document.body,{attributes:!0,attributeFilter:["class"]})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://mhassan.dev/>Muhammad Hassan Raza</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){try{if(window.__hasLoggedConsoleBrand)return;var n=getComputedStyle(document.documentElement),e=(n.getPropertyValue("--primary")||"").trim(),t=(n.getPropertyValue("--secondary")||"").trim(),s=e?"hsl("+e+")":"#4f46e5",o=t?"hsl("+t+")":"#facc15",i="color: "+s+"; font-weight: bold; font-family: monospace; font-size: 12px; line-height: 1.2;",a="color: "+o+"; font-weight: bold; font-family: monospace; font-size: 10px;",r=`%c
███████╗ ██████╗ ██████╗     ████████╗██╗  ██╗███████╗
██╔════╝██╔═══██╗██╔══██╗    ╚══██╔══╝██║  ██║██╔════╝
█████╗  ██║   ██║██████╔╝       ██║   ███████║█████╗  
██╔══╝  ██║   ██║██╔══██╗       ██║   ██╔══██║██╔══╝  
██║     ╚██████╔╝██║  ██║       ██║   ██║  ██║███████╗
╚═╝      ╚═════╝ ╚═╝  ╚═╝       ╚═╝   ╚═╝  ╚═╝╚══════╝
                                                        
███████╗███╗   ███╗██████╗ ███████╗██████╗  ██████╗ ██████╗ 
██╔════╝████╗ ████║██╔══██╗██╔════╝██╔══██╗██╔═══██╗██╔══██╗
█████╗  ██╔████╔██║██████╔╝█████╗  ██████╔╝██║   ██║██████╔╝
██╔══╝  ██║╚██╔╝██║██╔═══╝ ██╔══╝  ██╔══██╗██║   ██║██╔══██╗
███████╗██║ ╚═╝ ██║██║     ███████╗██║  ██║╚██████╔╝██║  ██║
╚══════╝╚═╝     ╚═╝╚═╝     ╚══════╝╚═╝  ╚═╝ ╚═════╝ ╚═╝  ╚═╝
%c
⚔️ In the grim darkness of the dev console, there is only code...`;console.log(r,i,a),window.__hasLoggedConsoleBrand=!0}catch{console.log("⚔️ In the grim darkness of the dev console, there is only code..."),window.__hasLoggedConsoleBrand=!0}})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>