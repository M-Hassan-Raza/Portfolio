<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output) | Muhammad Hassan Raza</title>
<meta name=keywords content="FAQ Bot,RAG,Retrieval Augmented Generation,Gemini API,ChromaDB,Embeddings,Structured Output,Vector Store,Python,AI Agents"><meta name=description content="
Introduction

If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs!


The Problem: Dumb Bots and Information Overload
Traditional search methods or basic chatbots often fall short when dealing with specific document sets:"><meta name=author content="Muhammad Hassan Raza"><link rel=canonical href=https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/><link crossorigin=anonymous href=/assets/css/stylesheet.e2484083c568226186863acda8452ec66f0ea5bc7f4834243c6c0f77aaf2a2b7.css integrity="sha256-4khAg8VoImGGhjrNqEUuxm8Opbx/SDQkPGwPd6ryorc=" rel="preload stylesheet" as=style><link rel=icon href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=16x16 href=https://mhassan.dev/assets/favicon.svg><link rel=icon type=image/png sizes=32x32 href=https://mhassan.dev/assets/favicon.svg><link rel=apple-touch-icon href=https://mhassan.dev/apple-touch-icon.png><link rel=mask-icon href=https://mhassan.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preload href=/css/extended/extended.css as=style><link rel=stylesheet href=/css/extended/extended.css><link rel=preload href=/fonts/font-family.css as=style><link rel=stylesheet href=/fonts/font-family.css><link rel=preload href=/fonts/Manrope-Regular.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Medium.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/Manrope-Bold.woff2 as=font type=font/woff2 crossorigin><script async src=https://cloud.umami.is/script.js data-website-id=30c7d9d6-abac-4c52-b85a-c0234f863d22></script><script async src="https://www.googletagmanager.com/gtag/js?id=%7b%7d"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","{}")</script><meta property="og:url" content="https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/"><meta property="og:site_name" content="Muhammad Hassan Raza"><meta property="og:title" content="Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"><meta property="og:description" content=" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-16T10:00:00+05:00"><meta property="article:modified_time" content="2024-08-16T10:00:00+05:00"><meta property="article:tag" content="FAQ Bot"><meta property="article:tag" content="RAG"><meta property="article:tag" content="Retrieval Augmented Generation"><meta property="article:tag" content="Gemini API"><meta property="article:tag" content="ChromaDB"><meta property="article:tag" content="Embeddings"><meta property="og:image" content="https://mhassan.dev/assets/faq-bot-cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mhassan.dev/assets/faq-bot-cover.jpg"><meta name=twitter:title content="Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"><meta name=twitter:description content="
Introduction

If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs!


The Problem: Dumb Bots and Information Overload
Traditional search methods or basic chatbots often fall short when dealing with specific document sets:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://mhassan.dev/blog/"},{"@type":"ListItem","position":2,"name":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","item":"https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","name":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)","description":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets:\n","keywords":["FAQ Bot","RAG","Retrieval Augmented Generation","Gemini API","ChromaDB","Embeddings","Structured Output","Vector Store","Python","AI Agents"],"articleBody":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets:\nInformation Overload: Manually searching large documents is time-consuming and inefficient. Generic LLM Limitations: Large Language Models (LLMs) are powerful, but they lack specific, up-to-date knowledge about your documents unless explicitly trained on them (which is often impractical). Hallucination Risk: When asked about information outside their training data, LLMs might confidently invent answers that sound plausible but are incorrect. This is unacceptable for reliable FAQ systems. Inconsistent Outputs: Getting answers in a usable, predictable format can be challenging with free-form text generation. We need a system that answers questions accurately based only on a given set of documents and provides answers in a consistent, structured way.\nThe Solution: RAG + Gemini API Our approach combines Retrieval Augmented Generation (RAG) with the capabilities of the Gemini API:\nRAG Pipeline: This involves three main steps:\nIndexing: Convert the source documents (Google Car manuals) into numerical representations (embeddings) using the Gemini text-embedding-004 model and store them in a vector database (ChromaDB). This allows for efficient similarity searches. Retrieval: When a user asks a question, embed the question using the same model and search the vector database to find the most relevant document chunks. Generation: Pass the original question and the retrieved document chunks as context to a powerful LLM (like gemini-2.0-flash). Instruct the model to answer the question based only on the provided context. Gemini API Features:\nHigh-Quality Embeddings: text-embedding-004 provides embeddings suitable for finding semantically similar text. Powerful Generation: gemini-2.0-flash can synthesize answers based on the retrieved context. Structured Output (JSON Mode): We instruct Gemini to return the answer and a confidence score in a predictable JSON format, making it easy for applications to use the output. Optional Grounding: We can even add Google Search as a tool if the local documents don’t suffice (though our primary goal here is document-based Q\u0026A). Implementation Highlights Here are some key code snippets demonstrating the core components:\n1. Custom Embedding Function for ChromaDB: We need to tell ChromaDB how to generate embeddings using the Gemini API.\n# --- 4. Define Gemini Embedding Function for ChromaDB --- from chromadb import Documents, EmbeddingFunction, Embeddings from google.api_core import retry from google import genai from google.genai import types is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503}) class GeminiEmbeddingFunction(EmbeddingFunction): document_mode = True # Toggle between indexing docs and embedding queries @retry.Retry(predicate=is_retriable) def __call__(self, input_texts: Documents) -\u003e Embeddings: task = \"retrieval_document\" if self.document_mode else \"retrieval_query\" print(f\"Embedding {'documents' if self.document_mode else 'query'} ({len(input_texts)})...\") try: response = client.models.embed_content( model=\"models/text-embedding-004\", contents=input_texts, config=types.EmbedContentConfig(task_type=task), # Specify task type ) return [e.values for e in response.embeddings] except Exception as e: print(f\"Error during embedding: {e}\") return [[] for _ in input_texts] 2. Setting up ChromaDB and Indexing: We create a ChromaDB collection and add our documents. get_or_create_collection makes this idempotent.\n# --- 5. Setup ChromaDB Vector Store --- import chromadb import time print(\"Setting up ChromaDB...\") DB_NAME = \"googlecar_faq_db\" embed_fn = GeminiEmbeddingFunction() chroma_client = chromadb.Client() # In-memory client try: db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn) print(f\"Collection '{DB_NAME}' ready. Current count: {db.count()}\") if db.count() \u003c len(documents): print(f\"Adding/Updating documents in '{DB_NAME}'...\") embed_fn.document_mode = True # Set mode for indexing db.upsert(documents=documents, ids=doc_ids) # Use upsert for safety time.sleep(2) # Allow indexing to settle print(f\"Documents added/updated. New count: {db.count()}\") else: print(\"Documents already seem to be indexed.\") except Exception as e: print(f\"Error setting up ChromaDB collection: {e}\") raise SystemExit(\"ChromaDB setup failed. Exiting.\") 3. Retrieving Relevant Documents: This function takes the user query, embeds it (using document_mode=False), and searches ChromaDB.\n# --- 6. Define Retrieval Function --- def retrieve_documents(query: str, n_results: int = 1) -\u003e list[str]: print(f\"\\nRetrieving documents for query: '{query}'\") embed_fn.document_mode = False # Switch to query mode try: results = db.query(query_texts=[query], n_results=n_results) if results and results.get('documents'): retrieved_docs = results['documents'][0] print(f\"Retrieved {len(retrieved_docs)} documents.\") return retrieved_docs else: print(\"No documents retrieved.\") return [] except Exception as e: print(f\"Error querying ChromaDB: {e}\") return [] 4. Generating the Structured Answer: Here’s the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.\n# --- 7. Define Structured Output Schema --- from typing_extensions import Literal from pydantic import BaseModel class AnswerWithConfidence(BaseModel): answer: str confidence: Literal[\"High\", \"Medium\", \"Low\"] # --- 8. Define Augmented Generation Function --- def generate_structured_answer(query: str, context_docs: list[str]) -\u003e dict | None: # ... (prompt construction as shown previously) ... prompt = f\"\"\"You are an AI assistant answering questions about a Google car based ONLY on the provided documents. Context Documents: --- {context} --- Question: {query} Based *only* on the information in the context documents above, answer the question. Also, assess your confidence in the answer based *only* on the provided text: - \"High\" if the answer is directly and clearly stated in the documents. - \"Medium\" if the answer can be inferred but isn't explicitly stated. - \"Low\" if the documents don't seem to contain the answer or are ambiguous. Return your response ONLY as a JSON object with the keys \"answer\" and \"confidence\". Example format: {{ \"answer\": \"Your answer here.\", \"confidence\": \"High/Medium/Low\" }} \"\"\" try: generation_config = types.GenerateContentConfig( temperature=0.2, response_mime_type=\"application/json\", # Request JSON response_schema=AnswerWithConfidence # Provide the schema ) response = client.models.generate_content( model=\"gemini-2.0-flash\", contents=prompt, config=generation_config # Pass the config object ) # ... (response handling as shown previously) ... # Safe access to parsed output if response.candidates and response.candidates[0].content and response.candidates[0].content.parts: parsed_output = response.candidates[0].content.parts[0].parsed if isinstance(parsed_output, dict) and \"answer\" in parsed_output and \"confidence\" in parsed_output: return parsed_output # ... (Error handling/logging) ... return {\"answer\": \"Error: Could not generate/parse structured response.\", \"confidence\": \"Low\"} except Exception as e: print(f\"Error during content generation call: {e}\") return {\"answer\": f\"Error during generation API call: {e}\", \"confidence\": \"Low\"} Tip: Ensure your API key is correctly set up in Kaggle Secrets (GOOGLE_API_KEY). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity).\nWhy Structured Output and Confidence Scores? Forcing the LLM to output JSON with a specific schema (using response_mime_type and response_schema) brings several advantages:\nReliability: The output format is predictable, making it easy to integrate into downstream applications without complex text parsing. Consistency: Ensures the bot always provides both the answer and its confidence level. Trustworthiness: The confidence score gives the user (or the calling application) an indication of how much to trust the answer, based on the grounding provided by the retrieved documents. A “Low” confidence answer might trigger a fallback to human support or a broader search. Limitations and Future Work This implementation is a great starting point, but it has limitations:\nDocument Quality: The RAG system’s effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out. Retrieval Accuracy: Simple similarity search might not always retrieve the perfect chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this. Structured Output Failures: While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added. Limited Context Handling (within LLM): While RAG provides context, the LLM itself still has limits on how much context it can process effectively in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM. Static Knowledge: The bot only knows what’s in the ChromaDB index. It doesn’t learn automatically. Updates require re-indexing. Future Enhancements:\nImplement Google Search grounding as a fallback when confidence is low or documents are missing. Add conversation memory for multi-turn interactions. Explore more sophisticated retrieval techniques. Build a simple UI (e.g., using Gradio or Streamlit). Fine-tune an embedding model specifically for the car manual domain (though text-embedding-004 is quite capable). Conclusion Building this FAQ bot demonstrates how combining RAG with Gemini’s embedding and generation capabilities, especially its structured output mode, can create powerful and reliable AI-driven Q\u0026A systems. By grounding the LLM’s responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience.\nKey Takeaways:\nRAG grounds LLM answers in your specific data. Gemini Embeddings + ChromaDB enable efficient document retrieval. Structured Output (JSON Mode) enhances reliability and integrability. Confidence Scores add a layer of trustworthiness. This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search.\nI hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to ask questions or leave a comment with your thoughts or own implementations!\n","wordCount":"1557","inLanguage":"en","image":"https://mhassan.dev/assets/faq-bot-cover.jpg","datePublished":"2024-08-16T10:00:00+05:00","dateModified":"2024-08-16T10:00:00+05:00","author":{"@type":"Person","name":"Muhammad Hassan Raza"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/"},"publisher":{"@type":"Organization","name":"Muhammad Hassan Raza","logo":{"@type":"ImageObject","url":"https://mhassan.dev/assets/favicon.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mhassan.dev/ accesskey=h title="Muhammad Hassan Raza (Alt + H)">Muhammad Hassan Raza</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mhassan.dev/book-a-call/ title="Book a Call"><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M8.26 1.289l-1.564.772c-5.793 3.02 2.798 20.944 9.31 20.944.46.0.904-.094 1.317-.284l1.542-.755-2.898-5.594-1.54.754c-.181.087-.384.134-.597.134-2.561.0-6.841-8.204-4.241-9.596l1.546-.763L8.26 1.289zM16.006 24C10.326 24 3.785 12.886 3.785 6.168c0-2.419.833-4.146 2.457-4.992l2.382-1.176 3.857 7.347-2.437 1.201c-1.439.772 2.409 8.424 3.956 7.68l2.399-1.179 3.816 7.36s-2.36 1.162-2.476 1.215c-.547.251-1.129.376-1.733.376"/></svg>Book a Call</span></a></li><li><a href=https://mhassan.dev/projects/ title=Projects><span><svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M11 5h13v17H0V2h8l3 3zM1 3v18h22V6H10.586l-3-3H1z"/></svg>Projects</span></a></li><li><a href=https://mhassan.dev/about/ title=About><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="-1 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M12 0c6.623.0 12 5.377 12 12s-5.377 12-12 12S0 18.623.0 12 5.377.0 12 0zm8.127 19.41c-.282-.401-.772-.654-1.624-.85-3.848-.906-4.097-1.501-4.352-2.059-.259-.565-.19-1.23.205-1.977 1.726-3.257 2.09-6.024 1.027-7.79C14.709 5.615 13.508 5 12 5c-1.521.0-2.732.626-3.409 1.763-1.066 1.789-.693 4.544 1.049 7.757.402.742.476 1.406.22 1.974-.265.586-.611 1.19-4.365 2.066-.852.196-1.342.449-1.623.848C5.884 21.615 8.782 23 12 23s6.115-1.385 8.127-3.59zm.65-.782C22.172 16.784 23 14.488 23 12c0-6.071-4.929-11-11-11S1 5.929 1 12c0 2.487.827 4.783 2.222 6.626.409-.452 1.049-.81 2.049-1.041 2.025-.462 3.376-.836 3.678-1.502.122-.272.061-.628-.188-1.087-1.917-3.535-2.282-6.641-1.03-8.745C8.584 4.82 10.139 4 12 4c1.845.0 3.391.808 4.24 2.218 1.251 2.079.896 5.195-1 8.774-.245.463-.304.821-.179 1.094.305.668 1.644 1.038 3.667 1.499 1 .23 1.64.59 2.049 1.043z"/></svg>About</span></a></li><li><a href=https://mhassan.dev/search/ title="Search (Alt + /)" accesskey=/><span><svg class="menu-icon" shape-rendering="geometricPrecision" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M15.853 16.56C14.17 18.077 11.942 19 9.5 19 4.257 19 0 14.743.0 9.5S4.257.0 9.5.0 19 4.257 19 9.5c0 2.442-.923 4.67-2.44 6.353l7.44 7.44-.707.707-7.44-7.44zm-6.353-15.56c4.691.0 8.5 3.809 8.5 8.5S14.191 18 9.5 18 1 14.191 1 9.5 4.809 1 9.5 1z"/></svg>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mhassan.dev/>Home</a>&nbsp;»&nbsp;<a href=https://mhassan.dev/blog/>Blog</a></div><h1 class=post-title>Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)</h1><div class=post-meta><span title='2024-08-16 10:00:00 +0500 +0500'>August 16, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Muhammad Hassan Raza</div></header><figure class=entry-cover><img loading=lazy src=https://mhassan.dev/assets/faq-bot-cover.jpg alt="Intelligent FAQ Bot"></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#the-problem-dumb-bots-and-information-overload aria-label="The Problem: Dumb Bots and Information Overload">The Problem: Dumb Bots and Information Overload</a></li><li><a href=#the-solution-rag--gemini-api aria-label="The Solution: RAG + Gemini API">The Solution: RAG + Gemini API</a></li><li><a href=#implementation-highlights aria-label="Implementation Highlights">Implementation Highlights</a></li><li><a href=#why-structured-output-and-confidence-scores aria-label="Why Structured Output and Confidence Scores?">Why Structured Output and Confidence Scores?</a></li><li><a href=#limitations-and-future-work aria-label="Limitations and Future Work">Limitations and Future Work</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><div style=text-align:justify><h2 id=introduction><span style=color:#ffb4a2>Introduction</span><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><div style=text-align:justify>If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs!</div><hr><h2 id=the-problem-dumb-bots-and-information-overload><span style=color:#ffb4a2>The Problem: Dumb Bots and Information Overload</span><a hidden class=anchor aria-hidden=true href=#the-problem-dumb-bots-and-information-overload>#</a></h2><p>Traditional search methods or basic chatbots often fall short when dealing with specific document sets:</p><ul><li><strong><span style=color:#8ac7db>Information Overload:</span></strong> Manually searching large documents is time-consuming and inefficient.</li><li><strong><span style=color:#8ac7db>Generic LLM Limitations:</span></strong> Large Language Models (LLMs) are powerful, but they lack specific, up-to-date knowledge about <em>your</em> documents unless explicitly trained on them (which is often impractical).</li><li><strong><span style=color:#8ac7db>Hallucination Risk:</span></strong> When asked about information outside their training data, LLMs might confidently invent answers that sound plausible but are incorrect. This is unacceptable for reliable FAQ systems.</li><li><strong><span style=color:#8ac7db>Inconsistent Outputs:</span></strong> Getting answers in a usable, predictable format can be challenging with free-form text generation.</li></ul><p>We need a system that answers questions accurately based <em>only</em> on a given set of documents and provides answers in a consistent, structured way.</p><hr><h2 id=the-solution-rag--gemini-api><span style=color:#ffb4a2>The Solution: RAG + Gemini API</span><a hidden class=anchor aria-hidden=true href=#the-solution-rag--gemini-api>#</a></h2><p>Our approach combines <strong>Retrieval Augmented Generation (RAG)</strong> with the capabilities of the <strong>Gemini API</strong>:</p><ul><li><p><strong><span style=color:#8ac7db>RAG Pipeline:</span></strong> This involves three main steps:</p><ol><li><strong>Indexing:</strong> Convert the source documents (Google Car manuals) into numerical representations (embeddings) using the Gemini <code>text-embedding-004</code> model and store them in a vector database (ChromaDB). This allows for efficient similarity searches.</li><li><strong>Retrieval:</strong> When a user asks a question, embed the question using the same model and search the vector database to find the most relevant document chunks.</li><li><strong>Generation:</strong> Pass the original question and the retrieved document chunks as context to a powerful LLM (like <code>gemini-2.0-flash</code>). Instruct the model to answer the question <em>based only on the provided context</em>.</li></ol></li><li><p><strong><span style=color:#8ac7db>Gemini API Features:</span></strong></p><ul><li><strong>High-Quality Embeddings:</strong> <code>text-embedding-004</code> provides embeddings suitable for finding semantically similar text.</li><li><strong>Powerful Generation:</strong> <code>gemini-2.0-flash</code> can synthesize answers based on the retrieved context.</li><li><strong>Structured Output (JSON Mode):</strong> We instruct Gemini to return the answer and a confidence score in a predictable JSON format, making it easy for applications to use the output.</li><li><strong>Optional Grounding:</strong> We can even add Google Search as a tool if the local documents don&rsquo;t suffice (though our primary goal here is document-based Q&amp;A).</li></ul></li></ul><hr><h2 id=implementation-highlights><span style=color:#ffb4a2>Implementation Highlights</span><a hidden class=anchor aria-hidden=true href=#implementation-highlights>#</a></h2><p>Here are some key code snippets demonstrating the core components:</p><p><strong>1. <span style=color:#8ac7db>Custom Embedding Function for ChromaDB:</span></strong>
We need to tell ChromaDB how to generate embeddings using the Gemini API.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 4. Define Gemini Embedding Function for ChromaDB ---</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> chromadb <span style=color:#f92672>import</span> Documents, EmbeddingFunction, Embeddings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> google.api_core <span style=color:#f92672>import</span> retry
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> google <span style=color:#f92672>import</span> genai
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> google.genai <span style=color:#f92672>import</span> types
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>is_retriable <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> e: (isinstance(e, genai<span style=color:#f92672>.</span>errors<span style=color:#f92672>.</span>APIError) <span style=color:#f92672>and</span> e<span style=color:#f92672>.</span>code <span style=color:#f92672>in</span> {<span style=color:#ae81ff>429</span>, <span style=color:#ae81ff>503</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GeminiEmbeddingFunction</span>(EmbeddingFunction):
</span></span><span style=display:flex><span>    document_mode <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span> <span style=color:#75715e># Toggle between indexing docs and embedding queries</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@retry.Retry</span>(predicate<span style=color:#f92672>=</span>is_retriable)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __call__(self, input_texts: Documents) <span style=color:#f92672>-&gt;</span> Embeddings:
</span></span><span style=display:flex><span>        task <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;retrieval_document&#34;</span> <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>document_mode <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;retrieval_query&#34;</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Embedding </span><span style=color:#e6db74>{</span><span style=color:#e6db74>&#39;documents&#39;</span> <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>document_mode <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;query&#39;</span><span style=color:#e6db74>}</span><span style=color:#e6db74> (</span><span style=color:#e6db74>{</span>len(input_texts)<span style=color:#e6db74>}</span><span style=color:#e6db74>)...&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>embed_content(
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;models/text-embedding-004&#34;</span>,
</span></span><span style=display:flex><span>                contents<span style=color:#f92672>=</span>input_texts,
</span></span><span style=display:flex><span>                config<span style=color:#f92672>=</span>types<span style=color:#f92672>.</span>EmbedContentConfig(task_type<span style=color:#f92672>=</span>task), <span style=color:#75715e># Specify task type</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> [e<span style=color:#f92672>.</span>values <span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> response<span style=color:#f92672>.</span>embeddings]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during embedding: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> input_texts]
</span></span></code></pre></div><p><strong>2. <span style=color:#8ac7db>Setting up ChromaDB and Indexing:</span></strong>
We create a ChromaDB collection and add our documents. <code>get_or_create_collection</code> makes this idempotent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 5. Setup ChromaDB Vector Store ---</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> chromadb
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Setting up ChromaDB...&#34;</span>)
</span></span><span style=display:flex><span>DB_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;googlecar_faq_db&#34;</span>
</span></span><span style=display:flex><span>embed_fn <span style=color:#f92672>=</span> GeminiEmbeddingFunction()
</span></span><span style=display:flex><span>chroma_client <span style=color:#f92672>=</span> chromadb<span style=color:#f92672>.</span>Client() <span style=color:#75715e># In-memory client</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    db <span style=color:#f92672>=</span> chroma_client<span style=color:#f92672>.</span>get_or_create_collection(name<span style=color:#f92672>=</span>DB_NAME, embedding_function<span style=color:#f92672>=</span>embed_fn)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Collection &#39;</span><span style=color:#e6db74>{</span>DB_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39; ready. Current count: </span><span style=color:#e6db74>{</span>db<span style=color:#f92672>.</span>count()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> db<span style=color:#f92672>.</span>count() <span style=color:#f92672>&lt;</span> len(documents):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Adding/Updating documents in &#39;</span><span style=color:#e6db74>{</span>DB_NAME<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;...&#34;</span>)
</span></span><span style=display:flex><span>        embed_fn<span style=color:#f92672>.</span>document_mode <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span> <span style=color:#75715e># Set mode for indexing</span>
</span></span><span style=display:flex><span>        db<span style=color:#f92672>.</span>upsert(documents<span style=color:#f92672>=</span>documents, ids<span style=color:#f92672>=</span>doc_ids) <span style=color:#75715e># Use upsert for safety</span>
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>2</span>) <span style=color:#75715e># Allow indexing to settle</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Documents added/updated. New count: </span><span style=color:#e6db74>{</span>db<span style=color:#f92672>.</span>count()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Documents already seem to be indexed.&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error setting up ChromaDB collection: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>SystemExit</span>(<span style=color:#e6db74>&#34;ChromaDB setup failed. Exiting.&#34;</span>)
</span></span></code></pre></div><p><strong>3. <span style=color:#8ac7db>Retrieving Relevant Documents:</span></strong>
This function takes the user query, embeds it (using <code>document_mode=False</code>), and searches ChromaDB.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 6. Define Retrieval Function ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>retrieve_documents</span>(query: str, n_results: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>-&gt;</span> list[str]:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Retrieving documents for query: &#39;</span><span style=color:#e6db74>{</span>query<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;&#34;</span>)
</span></span><span style=display:flex><span>    embed_fn<span style=color:#f92672>.</span>document_mode <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span> <span style=color:#75715e># Switch to query mode</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        results <span style=color:#f92672>=</span> db<span style=color:#f92672>.</span>query(query_texts<span style=color:#f92672>=</span>[query], n_results<span style=color:#f92672>=</span>n_results)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> results <span style=color:#f92672>and</span> results<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;documents&#39;</span>):
</span></span><span style=display:flex><span>            retrieved_docs <span style=color:#f92672>=</span> results[<span style=color:#e6db74>&#39;documents&#39;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Retrieved </span><span style=color:#e6db74>{</span>len(retrieved_docs)<span style=color:#e6db74>}</span><span style=color:#e6db74> documents.&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> retrieved_docs
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;No documents retrieved.&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error querying ChromaDB: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span></code></pre></div><p><strong>4. <span style=color:#8ac7db>Generating the Structured Answer:</span></strong>
Here&rsquo;s the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># --- 7. Define Structured Output Schema ---</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing_extensions <span style=color:#f92672>import</span> Literal
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AnswerWithConfidence</span>(BaseModel):
</span></span><span style=display:flex><span>    answer: str
</span></span><span style=display:flex><span>    confidence: Literal[<span style=color:#e6db74>&#34;High&#34;</span>, <span style=color:#e6db74>&#34;Medium&#34;</span>, <span style=color:#e6db74>&#34;Low&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- 8. Define Augmented Generation Function ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_structured_answer</span>(query: str, context_docs: list[str]) <span style=color:#f92672>-&gt;</span> dict <span style=color:#f92672>|</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... (prompt construction as shown previously) ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;You are an AI assistant answering questions about a Google car based ONLY on the provided documents.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Context Documents:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ---
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#e6db74>{</span>context<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ---
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Question: </span><span style=color:#e6db74>{</span>query<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Based *only* on the information in the context documents above, answer the question.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Also, assess your confidence in the answer based *only* on the provided text:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - &#34;High&#34; if the answer is directly and clearly stated in the documents.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - &#34;Medium&#34; if the answer can be inferred but isn&#39;t explicitly stated.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - &#34;Low&#34; if the documents don&#39;t seem to contain the answer or are ambiguous.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Return your response ONLY as a JSON object with the keys &#34;answer&#34; and &#34;confidence&#34;. Example format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#ae81ff>{{</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;answer&#34;: &#34;Your answer here.&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;confidence&#34;: &#34;High/Medium/Low&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span><span style=color:#ae81ff>}}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        generation_config <span style=color:#f92672>=</span> types<span style=color:#f92672>.</span>GenerateContentConfig(
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>,
</span></span><span style=display:flex><span>            response_mime_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>, <span style=color:#75715e># Request JSON</span>
</span></span><span style=display:flex><span>            response_schema<span style=color:#f92672>=</span>AnswerWithConfidence <span style=color:#75715e># Provide the schema</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>generate_content(
</span></span><span style=display:flex><span>            model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gemini-2.0-flash&#34;</span>,
</span></span><span style=display:flex><span>            contents<span style=color:#f92672>=</span>prompt,
</span></span><span style=display:flex><span>            config<span style=color:#f92672>=</span>generation_config <span style=color:#75715e># Pass the config object</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... (response handling as shown previously) ...</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Safe access to parsed output</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> response<span style=color:#f92672>.</span>candidates <span style=color:#f92672>and</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content <span style=color:#f92672>and</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts:
</span></span><span style=display:flex><span>             parsed_output <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>candidates[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>content<span style=color:#f92672>.</span>parts[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>parsed
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>if</span> isinstance(parsed_output, dict) <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;answer&#34;</span> <span style=color:#f92672>in</span> parsed_output <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;confidence&#34;</span> <span style=color:#f92672>in</span> parsed_output:
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>return</span> parsed_output
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... (Error handling/logging) ...</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Error: Could not generate/parse structured response.&#34;</span>, <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;Low&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during content generation call: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Error during generation API call: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, <span style=color:#e6db74>&#34;confidence&#34;</span>: <span style=color:#e6db74>&#34;Low&#34;</span>}
</span></span></code></pre></div><blockquote><p><strong>Tip:</strong> Ensure your API key is correctly set up in Kaggle Secrets (<code>GOOGLE_API_KEY</code>). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity).</p></blockquote><hr><h2 id=why-structured-output-and-confidence-scores><span style=color:#ffb4a2>Why Structured Output and Confidence Scores?</span><a hidden class=anchor aria-hidden=true href=#why-structured-output-and-confidence-scores>#</a></h2><p>Forcing the LLM to output JSON with a specific schema (using <code>response_mime_type</code> and <code>response_schema</code>) brings several advantages:</p><ul><li><strong><span style=color:#8ac7db>Reliability:</span></strong> The output format is predictable, making it easy to integrate into downstream applications without complex text parsing.</li><li><strong><span style=color:#8ac7db>Consistency:</span></strong> Ensures the bot always provides both the answer and its confidence level.</li><li><strong><span style=color:#8ac7db>Trustworthiness:</span></strong> The confidence score gives the user (or the calling application) an indication of how much to trust the answer, based on the grounding provided by the retrieved documents. A &ldquo;Low&rdquo; confidence answer might trigger a fallback to human support or a broader search.</li></ul><hr><h2 id=limitations-and-future-work><span style=color:#ffb4a2>Limitations and Future Work</span><a hidden class=anchor aria-hidden=true href=#limitations-and-future-work>#</a></h2><p>This implementation is a great starting point, but it has limitations:</p><ul><li><strong><span style=color:#8ac7db>Document Quality:</span></strong> The RAG system&rsquo;s effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out.</li><li><strong><span style=color:#8ac7db>Retrieval Accuracy:</span></strong> Simple similarity search might not always retrieve the <em>perfect</em> chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this.</li><li><strong><span style=color:#8ac7db>Structured Output Failures:</span></strong> While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added.</li><li><strong><span style=color:#8ac7db>Limited Context Handling (within LLM):</span></strong> While RAG provides context, the LLM itself still has limits on how much context it can process <em>effectively</em> in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM.</li><li><strong><span style=color:#8ac7db>Static Knowledge:</span></strong> The bot only knows what&rsquo;s in the ChromaDB index. It doesn&rsquo;t learn automatically. Updates require re-indexing.</li></ul><p><strong>Future Enhancements:</strong></p><ul><li>Implement Google Search grounding as a fallback when confidence is low or documents are missing.</li><li>Add conversation memory for multi-turn interactions.</li><li>Explore more sophisticated retrieval techniques.</li><li>Build a simple UI (e.g., using Gradio or Streamlit).</li><li>Fine-tune an embedding model specifically for the car manual domain (though <code>text-embedding-004</code> is quite capable).</li></ul><hr><h2 id=conclusion><span style=color:#ffb4a2>Conclusion</span><a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Building this FAQ bot demonstrates how combining RAG with Gemini&rsquo;s embedding and generation capabilities, especially its structured output mode, can create powerful and <strong>reliable</strong> AI-driven Q&amp;A systems. By grounding the LLM&rsquo;s responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience.</p><p><strong>Key Takeaways:</strong></p><ul><li><strong><span style=color:#8ac7db>RAG</span></strong> grounds LLM answers in your specific data.</li><li><strong><span style=color:#8ac7db>Gemini Embeddings + ChromaDB</span></strong> enable efficient document retrieval.</li><li><strong><span style=color:#8ac7db>Structured Output (JSON Mode)</span></strong> enhances reliability and integrability.</li><li><strong><span style=color:#8ac7db>Confidence Scores</span></strong> add a layer of trustworthiness.</li></ul><p>This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search.</p><hr><p>I hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to <strong>ask questions</strong> or <strong>leave a comment</strong> with your thoughts or own implementations!</p></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://mhassan.dev/tags/faq-bot>FAQ Bot</a></li><li><a href=https://mhassan.dev/tags/rag>RAG</a></li><li><a href=https://mhassan.dev/tags/retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=https://mhassan.dev/tags/gemini-api>Gemini API</a></li><li><a href=https://mhassan.dev/tags/chromadb>ChromaDB</a></li><li><a href=https://mhassan.dev/tags/embeddings>Embeddings</a></li><li><a href=https://mhassan.dev/tags/structured-output>Structured Output</a></li><li><a href=https://mhassan.dev/tags/vector-store>Vector Store</a></li><li><a href=https://mhassan.dev/tags/python>Python</a></li><li><a href=https://mhassan.dev/tags/ai-agents>AI Agents</a></li></ul><nav class=paginav><a class=prev href=https://mhassan.dev/blog/web-dev-resources/><span class=title>« Prev</span><br><span>Learn Web Development for Free: Platforms with Certificates (That Aren't Just Tutorials)</span></a></nav></footer><div class=giscus-container><script src=https://giscus.app/client.js data-repo=M-Hassan-Raza/Portfolio data-repo-id=R_kgDON3Oajw data-category=General data-category-id=DIC_kwDON3Oaj84Cm3y9 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async></script></div><script>function getTheme(){return document.body.classList.contains("dark")?"dark":"light"}function updateGiscusTheme(){const e=document.querySelector("iframe.giscus-frame");if(!e){setTimeout(updateGiscusTheme,300);return}const t=getTheme();e.contentWindow.postMessage({giscus:{setConfig:{theme:t}}},"https://giscus.app")}localStorage.getItem("pref-theme")==="dark"&&updateGiscusTheme();const observer=new MutationObserver(()=>{updateGiscusTheme()});observer.observe(document.body,{attributes:!0,attributeFilter:["class"]})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://mhassan.dev/>Muhammad Hassan Raza</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>