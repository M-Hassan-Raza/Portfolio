[{"content":"Polaris is a full-stack ERP and POS system I built for retail businesses. It handles inventory, sales, customer management, and financial reporting—the kind of system where bugs cost money and downtime loses customers.\nTech Stack: Vue 3, Pinia, TanStack Query, Django, Django REST Framework, PostgreSQL, WeasyPrint\nSource: Private (client work) · Book a call to discuss\nThe Hard Problems Building ERP software sounds straightforward until you hit the edge cases that make or break real businesses:\nConcurrent inventory access — Two cashiers selling the last item at the same time. Two stock clerks adjusting quantities. A sale happening while someone generates a report.\nFinancial integrity — Customer balances that must always reconcile. Partial payments, returns, credit notes, and adjustments that all need to balance correctly.\nMulti-tenant data isolation — Multiple organizations sharing infrastructure where one tenant must never see another\u0026rsquo;s data—not through bugs, not through clever URL manipulation, not through anything.\nBatch costing — Products arrive in batches at different prices. When you sell, which cost do you use? FIFO costing with full supplier traceability.\nTechnical Deep Dives Race Condition Prevention: Dual-Layer Locking The inventory system uses a dual-layer locking strategy to prevent race conditions without sacrificing performance.\nLayer 1: Pessimistic Locking with Deadlock Prevention\nWhen a transaction needs to update inventory, we acquire row-level locks in a consistent order:\nclass InventoryService: def update_quantities(self, items: list[tuple[int, Decimal]]): \u0026#34;\u0026#34;\u0026#34; Update multiple product quantities atomically. Items: list of (product_id, quantity_delta) tuples \u0026#34;\u0026#34;\u0026#34; # Sort by ID to prevent deadlocks sorted_items = sorted(items, key=lambda x: x[0]) with transaction.atomic(): for product_id, delta in sorted_items: # NOWAIT fails immediately if lock unavailable # Better to fail fast than queue indefinitely product = ( Product.objects .select_for_update(nowait=True) .get(id=product_id) ) product.quantity += delta product.save(update_fields=[\u0026#39;quantity\u0026#39;, \u0026#39;updated_at\u0026#39;]) The nowait=True is critical. Without it, transactions queue up waiting for locks, creating latency spikes during busy periods. With nowait, we fail fast and let the application retry with exponential backoff.\nLayer 2: Optimistic Locking for Frontend Sync\nFor the frontend, we need to detect when data has changed between read and write:\nclass Product(models.Model): version = models.PositiveIntegerField(default=0) def save(self, *args, **kwargs): if self.pk: updated = Product.objects.filter( pk=self.pk, version=self.version ).update( version=self.version + 1, **{f: getattr(self, f) for f in kwargs.get(\u0026#39;update_fields\u0026#39;, [])} ) if not updated: raise StaleDataError(\u0026#34;Record modified by another user\u0026#34;) super().save(*args, **kwargs) The frontend polls for version changes with a lightweight endpoint that returns ~50 bytes per product—just IDs and versions. When versions mismatch, the UI shows a conflict resolution dialog rather than silently overwriting.\nFinancial System: Advisory Locks and Double-Entry Bookkeeping Customer balances are the most sensitive data in the system. A miscalculation means either overcharging customers or losing money.\nPostgreSQL Advisory Locks for Balance Operations\nDatabase row locks work for single-row updates, but balance calculations touch multiple rows. We use PostgreSQL advisory locks to serialize operations per customer:\nclass LedgerService: def credit_customer(self, customer_id: int, amount: Decimal, reason: str): # Advisory lock scoped to this customer lock_id = hash(f\u0026#34;customer_balance_{customer_id}\u0026#34;) \u0026amp; 0x7FFFFFFF with connection.cursor() as cursor: cursor.execute(\u0026#34;SELECT pg_advisory_lock(%s)\u0026#34;, [lock_id]) try: with transaction.atomic(): entry = LedgerEntry.objects.create( customer_id=customer_id, entry_type=EntryType.CREDIT, amount=amount, reason=reason, running_balance=self._calculate_new_balance(customer_id, amount) ) # Trigger recalculation signals balance_changed.send(sender=self, customer_id=customer_id) return entry finally: with connection.cursor() as cursor: cursor.execute(\u0026#34;SELECT pg_advisory_unlock(%s)\u0026#34;, [lock_id]) Double-Entry with Automatic Reversals\nEvery financial operation creates a reversible entry. Returns don\u0026rsquo;t delete the original sale—they create a counter-entry:\nclass LedgerEntry(models.Model): entry_type = models.CharField(choices=EntryType.choices) amount = models.DecimalField(max_digits=12, decimal_places=2) reversal_of = models.ForeignKey(\u0026#39;self\u0026#39;, null=True, on_delete=models.PROTECT) reversed_by = models.ForeignKey(\u0026#39;self\u0026#39;, null=True, on_delete=models.PROTECT) def reverse(self, reason: str): \u0026#34;\u0026#34;\u0026#34;Create a reversal entry. Original entry remains for audit.\u0026#34;\u0026#34;\u0026#34; reversal = LedgerEntry.objects.create( customer=self.customer, entry_type=self._inverse_type(), amount=self.amount, reason=f\u0026#34;Reversal: {reason}\u0026#34;, reversal_of=self ) self.reversed_by = reversal self.save(update_fields=[\u0026#39;reversed_by\u0026#39;]) return reversal Tolerance-Based Financial Equality\nFloating-point comparisons fail on financial data. We use tolerance-based equality:\nFINANCIAL_TOLERANCE = Decimal(\u0026#39;0.01\u0026#39;) def balances_equal(a: Decimal, b: Decimal) -\u0026gt; bool: return abs(a - b) \u0026lt; FINANCIAL_TOLERANCE def validate_ledger_balance(customer_id: int): \u0026#34;\u0026#34;\u0026#34;Verify running balance matches sum of entries.\u0026#34;\u0026#34;\u0026#34; entries_sum = LedgerEntry.objects.filter( customer_id=customer_id, reversed_by__isnull=True ).aggregate(total=Sum(\u0026#39;signed_amount\u0026#39;))[\u0026#39;total\u0026#39;] or Decimal(\u0026#39;0\u0026#39;) current_balance = Customer.objects.get(id=customer_id).balance if not balances_equal(entries_sum, current_balance): raise LedgerIntegrityError( f\u0026#34;Balance mismatch: entries={entries_sum}, stored={current_balance}\u0026#34; ) Multi-Tenant Architecture The system serves multiple retail businesses from a single deployment. Data isolation isn\u0026rsquo;t optional—it\u0026rsquo;s existential.\nThread-Local Organization Context\nEvery request establishes an organization context that propagates through the entire call stack:\n_org_context = threading.local() class OrganizationMiddleware: def __call__(self, request): org_id = self._extract_org_id(request) _org_context.organization_id = org_id _org_context.organization = Organization.objects.get(id=org_id) try: response = self.get_response(request) finally: _org_context.organization_id = None _org_context.organization = None return response def get_current_organization(): return getattr(_org_context, \u0026#39;organization\u0026#39;, None) Tenant-Aware QuerySet\nAll models inherit from a base that automatically filters by organization:\nclass TenantAwareQuerySet(models.QuerySet): def get_queryset(self): qs = super().get_queryset() org = get_current_organization() if org: return qs.filter(organization=org) return qs.none() # Fail closed, not open class TenantAwareManager(models.Manager): def get_queryset(self): return TenantAwareQuerySet(self.model, using=self._db) class TenantModel(models.Model): organization = models.ForeignKey(Organization, on_delete=models.CASCADE) objects = TenantAwareManager() all_objects = models.Manager() # Escape hatch for admin/migrations class Meta: abstract = True Every query—Product.objects.all(), Customer.objects.filter(name='X')—automatically includes the organization filter. You can\u0026rsquo;t forget it because it\u0026rsquo;s built into the ORM.\nBatch Inventory and FIFO Costing Products arrive in batches at different purchase prices. When calculating cost of goods sold, we use FIFO (First In, First Out):\nclass InventoryBatch(models.Model): product = models.ForeignKey(Product, on_delete=models.CASCADE) purchase = models.ForeignKey(Purchase, on_delete=models.PROTECT) supplier = models.ForeignKey(Supplier, on_delete=models.PROTECT) quantity_received = models.DecimalField(max_digits=10, decimal_places=2) quantity_remaining = models.DecimalField(max_digits=10, decimal_places=2) unit_cost = models.DecimalField(max_digits=10, decimal_places=2) received_at = models.DateTimeField(auto_now_add=True) class Product(TenantModel): @property def current_quantity(self): \u0026#34;\u0026#34;\u0026#34;Computed from batches, not stored.\u0026#34;\u0026#34;\u0026#34; return InventoryBatch.objects.filter( product=self ).aggregate( total=Sum(\u0026#39;quantity_remaining\u0026#39;) )[\u0026#39;total\u0026#39;] or Decimal(\u0026#39;0\u0026#39;) @property def average_cost(self): \u0026#34;\u0026#34;\u0026#34;Weighted average across remaining batches.\u0026#34;\u0026#34;\u0026#34; batches = InventoryBatch.objects.filter( product=self, quantity_remaining__gt=0 ) total_value = sum(b.quantity_remaining * b.unit_cost for b in batches) total_qty = sum(b.quantity_remaining for b in batches) return total_value / total_qty if total_qty else Decimal(\u0026#39;0\u0026#39;) When selling, we consume from oldest batches first:\ndef consume_inventory(product_id: int, quantity: Decimal) -\u0026gt; list[tuple[Batch, Decimal]]: \u0026#34;\u0026#34;\u0026#34; FIFO consumption. Returns list of (batch, quantity_consumed) pairs for cost calculation and audit trail. \u0026#34;\u0026#34;\u0026#34; consumed = [] remaining = quantity batches = ( InventoryBatch.objects .filter(product_id=product_id, quantity_remaining__gt=0) .order_by(\u0026#39;received_at\u0026#39;) .select_for_update() ) for batch in batches: if remaining \u0026lt;= 0: break take = min(batch.quantity_remaining, remaining) batch.quantity_remaining -= take batch.save(update_fields=[\u0026#39;quantity_remaining\u0026#39;]) consumed.append((batch, take)) remaining -= take if remaining \u0026gt; 0: raise InsufficientInventoryError(f\u0026#34;Short {remaining} units\u0026#34;) return consumed The PROTECT on supplier prevents orphaned batches—you can\u0026rsquo;t delete a supplier while inventory from them exists. This maintains the audit trail for traceability.\nSoft Delete with Audit Trail Deleted records aren\u0026rsquo;t actually deleted—they\u0026rsquo;re flagged for audit and recovery:\nclass SoftDeleteQuerySet(TenantAwareQuerySet): def get_queryset(self): return super().get_queryset().filter(deleted_at__isnull=True) def with_deleted(self): return super().get_queryset() class AuditModel(TenantModel): created_at = models.DateTimeField(auto_now_add=True) created_by = models.ForeignKey(User, on_delete=models.PROTECT, related_name=\u0026#39;+\u0026#39;) updated_at = models.DateTimeField(auto_now=True) updated_by = models.ForeignKey(User, on_delete=models.PROTECT, related_name=\u0026#39;+\u0026#39;) deleted_at = models.DateTimeField(null=True, blank=True) deleted_by = models.ForeignKey(User, null=True, on_delete=models.PROTECT, related_name=\u0026#39;+\u0026#39;) deletion_reason = models.TextField(blank=True) objects = SoftDeleteManager() all_objects = models.Manager() def soft_delete(self, user, reason: str): self.deleted_at = timezone.now() self.deleted_by = user self.deletion_reason = reason self.save(update_fields=[\u0026#39;deleted_at\u0026#39;, \u0026#39;deleted_by\u0026#39;, \u0026#39;deletion_reason\u0026#39;]) This combines with tenant-awareness—Product.objects.all() returns non-deleted products for the current organization only.\nFrontend Architecture The Vue 3 frontend separates concerns cleanly between UI state and server state.\nComposables for Everything\nOver 100 composables handle different concerns:\n// useCart.ts - Cart UI state (Pinia) export const useCart = defineStore(\u0026#39;cart\u0026#39;, () =\u0026gt; { const items = ref\u0026lt;CartItem[]\u0026gt;([]) const customerId = ref\u0026lt;number | null\u0026gt;(null) // Persisted to sessionStorage for recovery const persistedState = useSessionStorage(\u0026#39;cart-state\u0026#39;, { items: [], customerId: null }) // Sync with persisted state watch([items, customerId], () =\u0026gt; { persistedState.value = { items: items.value, customerId: customerId.value } }) return { items, customerId, /* ... */ } }) // useProducts.ts - Server state (TanStack Query) export function useProducts(filters: ProductFilters) { return useQuery({ queryKey: [\u0026#39;products\u0026#39;, filters], queryFn: () =\u0026gt; api.products.list(filters), staleTime: 30_000, }) } // useProductVersions.ts - Lightweight polling for conflict detection export function useProductVersions(productIds: number[]) { return useQuery({ queryKey: [\u0026#39;product-versions\u0026#39;, productIds], queryFn: () =\u0026gt; api.products.versions(productIds), refetchInterval: 5_000, // 5s polling }) } Cart Recovery\nSession persistence means abandoned carts survive page refreshes:\nexport function useCartRecovery() { const cart = useCart() onMounted(() =\u0026gt; { const persisted = sessionStorage.getItem(\u0026#39;cart-state\u0026#39;) if (persisted) { const { items, customerId } = JSON.parse(persisted) if (items.length \u0026gt; 0) { showRecoveryDialog({ items, customerId }) } } }) } Real-Time Sync: Hybrid SSE + Polling We use Server-Sent Events for immediate updates with polling as fallback:\nclass RealtimeSync { private eventSource: EventSource | null = null private pollInterval: number | null = null connect() { if (typeof EventSource !== \u0026#39;undefined\u0026#39;) { this.eventSource = new EventSource(\u0026#39;/api/events/\u0026#39;) this.eventSource.onmessage = this.handleEvent this.eventSource.onerror = this.fallbackToPolling } else { this.fallbackToPolling() } } private fallbackToPolling() { this.eventSource?.close() this.pollInterval = setInterval(() =\u0026gt; this.poll(), 5000) } private handleEvent(event: MessageEvent) { const { type, payload } = JSON.parse(event.data) if (type === \u0026#39;inventory_update\u0026#39;) { queryClient.invalidateQueries({ queryKey: [\u0026#39;products\u0026#39;] }) } else if (type === \u0026#39;version_conflict\u0026#39;) { showConflictDialog(payload) } } } Optimistic UI updates show changes immediately, with server confirmation reconciling any conflicts:\nconst updateQuantity = useMutation({ mutationFn: (data) =\u0026gt; api.inventory.update(data), onMutate: async (data) =\u0026gt; { // Cancel outgoing refetches await queryClient.cancelQueries({ queryKey: [\u0026#39;products\u0026#39;, data.productId] }) // Snapshot previous value const previous = queryClient.getQueryData([\u0026#39;products\u0026#39;, data.productId]) // Optimistically update queryClient.setQueryData([\u0026#39;products\u0026#39;, data.productId], (old) =\u0026gt; ({ ...old, quantity: old.quantity + data.delta })) return { previous } }, onError: (err, data, context) =\u0026gt; { // Rollback on error queryClient.setQueryData([\u0026#39;products\u0026#39;, data.productId], context.previous) }, onSettled: () =\u0026gt; { queryClient.invalidateQueries({ queryKey: [\u0026#39;products\u0026#39;] }) } }) Screenshots Dashboard with business insights Product management Checkout with price history Customer balance tracking Sales reporting Generated invoice PDF What I Learned Fail closed, not open. The tenant-aware queryset returns .none() when there\u0026rsquo;s no organization context rather than returning everything. This default-deny approach catches bugs before they become security incidents.\nComputed vs stored quantities. Storing inventory quantities seems simpler, but computed quantities from batch records eliminate an entire class of drift bugs. The performance cost of aggregation is worth the consistency guarantee.\nAdvisory locks solve coordination problems. Row-level locks work for single-record updates, but cross-record operations need explicit coordination. PostgreSQL advisory locks are underused for application-level synchronization.\nVersion fields catch what transactions miss. Transactions prevent concurrent writes, but they don\u0026rsquo;t help when a user stares at stale data for 10 minutes before clicking save. Optimistic locking with version fields closes that gap.\nInterested? If you\u0026rsquo;re looking for similar ERP/POS development or want to discuss the technical details, book a call.\n","permalink":"https://mhassan.dev/projects/polaris/","summary":"\u003cp\u003ePolaris is a full-stack ERP and POS system I built for retail businesses. It handles inventory, sales, customer management, and financial reporting—the kind of system where bugs cost money and downtime loses customers.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTech Stack:\u003c/strong\u003e Vue 3, Pinia, TanStack Query, Django, Django REST Framework, PostgreSQL, WeasyPrint\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSource:\u003c/strong\u003e Private (client work) · \u003ca href=\"/book-a-call/\"\u003eBook a call\u003c/a\u003e to discuss\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"the-hard-problems\"\u003eThe Hard Problems\u003c/h2\u003e\n\u003cp\u003eBuilding ERP software sounds straightforward until you hit the edge cases that make or break real businesses:\u003c/p\u003e","title":"Polaris ERP"},{"content":"Obelisk is an AI-powered marketing platform that helps teams create content with brand consistency. It orchestrates specialized AI agents for SEO, email marketing, brand voice analysis, and strategy—all within a multi-tenant SaaS architecture with space-level isolation.\nTech Stack: FastAPI, LangGraph, PostgreSQL, Vertex AI, Redis, Google Cloud\nSource: Private (commercial product) · Book a call to discuss\nThe Hard Problems Building production AI systems exposes problems that don\u0026rsquo;t appear in tutorials:\nAgent reliability — LLMs hallucinate, get confused, and can be manipulated. How do you build agents that fail gracefully and resist prompt injection?\nRAG latency — Semantic search is slow. Vector similarity, document retrieval, context assembly—each adds latency. Users waiting 20+ seconds for a response won\u0026rsquo;t stick around.\nMulti-tenant AI — Each organization has their own brand voice, documents, and context. That data must never leak between tenants, even through the AI\u0026rsquo;s responses.\nConversation persistence — Long-running agent sessions need to survive server restarts, handle reconnections, and resume mid-conversation without losing context.\nTechnical Deep Dives AI Agent Architecture with LangGraph The system uses four specialized agents that collaborate on content tasks. LangGraph handles the orchestration—state machines for AI workflows.\nAgent Specialization\nEach agent has a focused role with its own system prompt, tools, and retrieval configuration:\nclass AgentType(Enum): SEO = \u0026#34;seo\u0026#34; # Search optimization, keyword research EMAIL = \u0026#34;email\u0026#34; # Campaign copy, subject lines, sequences BRAND = \u0026#34;brand\u0026#34; # Voice consistency, tone analysis STRATEGY = \u0026#34;strategy\u0026#34; # Content planning, competitive analysis @dataclass class AgentConfig: agent_type: AgentType system_prompt: str temperature: float tools: list[Tool] retrieval_config: RetrievalConfig max_iterations: int = 10 AGENT_CONFIGS = { AgentType.SEO: AgentConfig( agent_type=AgentType.SEO, system_prompt=SEO_SYSTEM_PROMPT, temperature=0.3, # Lower for factual accuracy tools=[keyword_research, serp_analysis, content_audit], retrieval_config=RetrievalConfig( collections=[\u0026#34;seo_guidelines\u0026#34;, \u0026#34;competitor_analysis\u0026#34;], top_k=5 ) ), AgentType.BRAND: AgentConfig( agent_type=AgentType.BRAND, system_prompt=BRAND_SYSTEM_PROMPT, temperature=0.7, # Higher for creative suggestions tools=[voice_analyzer, tone_checker, style_guide_lookup], retrieval_config=RetrievalConfig( collections=[\u0026#34;brand_guidelines\u0026#34;, \u0026#34;approved_content\u0026#34;], top_k=8 ) ), # ... } Contamination Detection\nLLMs are vulnerable to prompt injection—malicious instructions hidden in retrieved documents or user input. We detect and block contaminated responses:\nclass ContaminationDetector: \u0026#34;\u0026#34;\u0026#34; Detects signs of prompt injection in LLM outputs. \u0026#34;\u0026#34;\u0026#34; CONTAMINATION_PATTERNS = [ r\u0026#34;ignore previous instructions\u0026#34;, r\u0026#34;disregard (the |your )?system prompt\u0026#34;, r\u0026#34;you are now\u0026#34;, r\u0026#34;new instructions:\u0026#34;, r\u0026#34;\u0026lt;\\|.*?\\|\u0026gt;\u0026#34;, # Common injection delimiters r\u0026#34;\\[INST\\]\u0026#34;, # Instruction markers ] def __init__(self): self.patterns = [re.compile(p, re.I) for p in self.CONTAMINATION_PATTERNS] def check_response(self, response: str, context_docs: list[str]) -\u0026gt; ContaminationResult: # Check for injection patterns in response for pattern in self.patterns: if pattern.search(response): return ContaminationResult( contaminated=True, reason=\u0026#34;Response contains injection pattern\u0026#34;, pattern=pattern.pattern ) # Check if response echoes suspicious document content for doc in context_docs: if self._contains_instruction_leak(response, doc): return ContaminationResult( contaminated=True, reason=\u0026#34;Response echoes suspicious document content\u0026#34; ) return ContaminationResult(contaminated=False) def _contains_instruction_leak(self, response: str, doc: str) -\u0026gt; bool: # Detect if the response is parroting instruction-like content from docs instruction_markers = [\u0026#34;you must\u0026#34;, \u0026#34;always respond\u0026#34;, \u0026#34;your role is\u0026#34;] for marker in instruction_markers: if marker in doc.lower() and marker in response.lower(): # Check similarity of surrounding context if self._context_similarity(response, doc, marker) \u0026gt; 0.8: return True return False Conditional Routing with Multiple Termination Conditions\nAgent loops need multiple exit conditions to prevent runaway execution:\nclass AgentOrchestrator: def __init__(self, config: AgentConfig): self.config = config self.graph = self._build_graph() def _build_graph(self) -\u0026gt; StateGraph: graph = StateGraph(AgentState) graph.add_node(\u0026#34;retrieve\u0026#34;, self.retrieve_context) graph.add_node(\u0026#34;think\u0026#34;, self.agent_think) graph.add_node(\u0026#34;act\u0026#34;, self.agent_act) graph.add_node(\u0026#34;check\u0026#34;, self.check_completion) graph.add_edge(START, \u0026#34;retrieve\u0026#34;) graph.add_edge(\u0026#34;retrieve\u0026#34;, \u0026#34;think\u0026#34;) graph.add_conditional_edges( \u0026#34;think\u0026#34;, self.route_after_think, { \u0026#34;act\u0026#34;: \u0026#34;act\u0026#34;, \u0026#34;complete\u0026#34;: END, \u0026#34;error\u0026#34;: END, } ) graph.add_edge(\u0026#34;act\u0026#34;, \u0026#34;check\u0026#34;) graph.add_conditional_edges( \u0026#34;check\u0026#34;, self.route_after_check, { \u0026#34;continue\u0026#34;: \u0026#34;think\u0026#34;, \u0026#34;complete\u0026#34;: END, \u0026#34;max_iterations\u0026#34;: END, \u0026#34;contaminated\u0026#34;: END, } ) return graph.compile(checkpointer=self.checkpointer) def route_after_check(self, state: AgentState) -\u0026gt; str: # Multiple termination conditions if state.iterations \u0026gt;= self.config.max_iterations: return \u0026#34;max_iterations\u0026#34; if state.contamination_detected: return \u0026#34;contaminated\u0026#34; if state.task_complete: return \u0026#34;complete\u0026#34; if state.needs_more_context: return \u0026#34;continue\u0026#34; return \u0026#34;complete\u0026#34; PostgreSQL Checkpointing for Conversation Resumption\nLangGraph supports checkpointing to persist conversation state. We use PostgreSQL for durability:\nfrom langgraph.checkpoint.postgres import PostgresSaver class ConversationManager: def __init__(self, db_url: str): self.checkpointer = PostgresSaver.from_conn_string(db_url) async def resume_conversation( self, thread_id: str, new_message: str ) -\u0026gt; AsyncIterator[StreamEvent]: \u0026#34;\u0026#34;\u0026#34;Resume a conversation from its last checkpoint.\u0026#34;\u0026#34;\u0026#34; # Load existing state config = {\u0026#34;configurable\u0026#34;: {\u0026#34;thread_id\u0026#34;: thread_id}} # Get the agent graph for this conversation\u0026#39;s type conversation = await self.get_conversation(thread_id) agent = self.get_agent(conversation.agent_type) # Stream from checkpoint async for event in agent.graph.astream( {\u0026#34;messages\u0026#34;: [HumanMessage(content=new_message)]}, config=config, ): yield self._format_event(event) async def get_conversation_history(self, thread_id: str) -\u0026gt; list[Message]: \u0026#34;\u0026#34;\u0026#34;Retrieve full conversation from checkpoints.\u0026#34;\u0026#34;\u0026#34; config = {\u0026#34;configurable\u0026#34;: {\u0026#34;thread_id\u0026#34;: thread_id}} state = await self.checkpointer.aget(config) return state.values.get(\u0026#34;messages\u0026#34;, []) if state else [] Parallel RAG System Standard RAG is slow—retrieve, rank, assemble, generate. We parallelize everything possible.\n4-Stream Parallel Retrieval\nInstead of sequential retrieval, we fire four queries simultaneously:\nclass ParallelRetriever: \u0026#34;\u0026#34;\u0026#34; Retrieves from multiple sources in parallel, merging results. \u0026#34;\u0026#34;\u0026#34; async def retrieve( self, query: str, space_id: str, referenced_doc_ids: list[str] | None = None ) -\u0026gt; RetrievalResult: # Fire all retrievals in parallel tasks = [ self._retrieve_referenced(referenced_doc_ids), # Explicit references self._retrieve_general(query, space_id), # Semantic search self._retrieve_urls(query, space_id), # Web content self._retrieve_business_context(space_id), # Org context ] results = await asyncio.gather(*tasks, return_exceptions=True) # Merge and deduplicate all_chunks = [] for result in results: if isinstance(result, Exception): logger.warning(f\u0026#34;Retrieval stream failed: {result}\u0026#34;) continue all_chunks.extend(result) # Deduplicate by content hash seen = set() unique_chunks = [] for chunk in all_chunks: content_hash = hashlib.md5(chunk.content.encode()).hexdigest() if content_hash not in seen: seen.add(content_hash) unique_chunks.append(chunk) # Re-rank merged results ranked = await self.reranker.rank(query, unique_chunks) return RetrievalResult( chunks=ranked[:self.max_chunks], sources=self._extract_sources(ranked) ) Query Enhancement with Fast Model\nBefore retrieval, we enhance the query using a fast, cheap model:\nclass QueryEnhancer: \u0026#34;\u0026#34;\u0026#34; Expands queries for better retrieval using Gemini Flash. \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.model = GenerativeModel(\u0026#34;gemini-1.5-flash\u0026#34;) async def enhance(self, query: str, context: ConversationContext) -\u0026gt; EnhancedQuery: prompt = f\u0026#34;\u0026#34;\u0026#34; Given this user query and conversation context, generate: 1. An expanded search query with related terms 2. 2-3 alternative phrasings 3. Key entities to look for Query: {query} Recent context: {context.recent_summary} Respond in JSON format. \u0026#34;\u0026#34;\u0026#34; response = await self.model.generate_content_async(prompt) enhanced = json.loads(response.text) return EnhancedQuery( original=query, expanded=enhanced[\u0026#34;expanded\u0026#34;], alternatives=enhanced[\u0026#34;alternatives\u0026#34;], entities=enhanced[\u0026#34;entities\u0026#34;] ) This reduced Time to First Token (TTFT) from 22s to 8-10s—a 50-65% improvement. The fast model call adds ~200ms but saves seconds on retrieval by producing better queries.\n3072D Embeddings via Vertex AI\nWe use Vertex AI\u0026rsquo;s text-embedding-004 model with 3072 dimensions for high-quality semantic matching:\nclass EmbeddingService: def __init__(self): self.model = TextEmbeddingModel.from_pretrained(\u0026#34;text-embedding-004\u0026#34;) self.dimension = 3072 async def embed_batch( self, texts: list[str], task_type: str = \u0026#34;RETRIEVAL_DOCUMENT\u0026#34; ) -\u0026gt; list[list[float]]: \u0026#34;\u0026#34;\u0026#34; Batch embed with automatic chunking for API limits. \u0026#34;\u0026#34;\u0026#34; embeddings = [] batch_size = 250 # Vertex AI limit for i in range(0, len(texts), batch_size): batch = texts[i:i + batch_size] inputs = [ TextEmbeddingInput(text=t, task_type=task_type) for t in batch ] results = await asyncio.to_thread( self.model.get_embeddings, inputs ) embeddings.extend([r.values for r in results]) return embeddings Document Processing Pipeline Marketing teams upload diverse documents—PDFs, slides, spreadsheets, web pages. We need unified processing.\nDocling for Multi-Format Ingestion\nDocling handles format conversion with structure preservation:\nfrom docling.document_converter import DocumentConverter from docling.datamodel.base_models import InputFormat class DocumentProcessor: def __init__(self): self.converter = DocumentConverter() self.chunker = SemanticChunker() self.embedding_service = EmbeddingService() async def process_document( self, file_path: Path, space_id: str, metadata: dict ) -\u0026gt; ProcessedDocument: # Convert to unified format result = self.converter.convert(str(file_path)) doc = result.document # Extract structure structure = DocumentStructure( title=doc.title, headings=self._extract_headings(doc), tables=self._extract_tables(doc), images=self._extract_images(doc), ) # Semantic chunking that respects structure chunks = await self.chunker.chunk( doc, structure=structure, max_chunk_size=1500, overlap=200 ) # Generate embeddings embeddings = await self.embedding_service.embed_batch( [c.content for c in chunks] ) # Store in vector DB await self.store_chunks(chunks, embeddings, space_id, metadata) return ProcessedDocument( id=str(uuid4()), chunks=len(chunks), structure=structure ) Structure-Aware Semantic Chunking\nNaive chunking breaks context. We chunk at semantic boundaries:\nclass SemanticChunker: \u0026#34;\u0026#34;\u0026#34; Chunks documents while preserving semantic structure. \u0026#34;\u0026#34;\u0026#34; def chunk( self, doc: Document, structure: DocumentStructure, max_chunk_size: int, overlap: int ) -\u0026gt; list[Chunk]: chunks = [] current_section = None for element in doc.iterate_items(): if element.is_heading: # Start new chunk at headings if current_section: chunks.extend(self._finalize_section(current_section, max_chunk_size)) current_section = Section(heading=element.text, content=[]) elif element.is_table: # Tables get their own chunks with context chunks.append(Chunk( content=self._table_to_markdown(element), metadata={\u0026#34;type\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;section\u0026#34;: current_section.heading} )) elif element.is_text: if current_section: current_section.content.append(element.text) # Finalize last section if current_section: chunks.extend(self._finalize_section(current_section, max_chunk_size)) return chunks def _finalize_section(self, section: Section, max_size: int) -\u0026gt; list[Chunk]: \u0026#34;\u0026#34;\u0026#34;Split section content while maintaining heading context.\u0026#34;\u0026#34;\u0026#34; full_text = \u0026#34;\\n\u0026#34;.join(section.content) if len(full_text) \u0026lt;= max_size: return [Chunk( content=f\u0026#34;# {section.heading}\\n\\n{full_text}\u0026#34;, metadata={\u0026#34;section\u0026#34;: section.heading} )] # Split at paragraph boundaries paragraphs = full_text.split(\u0026#34;\\n\\n\u0026#34;) chunks = [] current = f\u0026#34;# {section.heading}\\n\\n\u0026#34; for para in paragraphs: if len(current) + len(para) \u0026gt; max_size: chunks.append(Chunk(content=current, metadata={\u0026#34;section\u0026#34;: section.heading})) current = f\u0026#34;# {section.heading} (continued)\\n\\n{para}\\n\\n\u0026#34; else: current += para + \u0026#34;\\n\\n\u0026#34; if current.strip(): chunks.append(Chunk(content=current, metadata={\u0026#34;section\u0026#34;: section.heading})) return chunks Dual Embeddings for Visual Content\nSome documents are image-heavy (presentations, infographics). We generate both text and visual embeddings:\nclass DualEmbeddingService: def __init__(self): self.text_model = TextEmbeddingModel.from_pretrained(\u0026#34;text-embedding-004\u0026#34;) self.clip_model = self._load_clip() async def embed_chunk(self, chunk: Chunk) -\u0026gt; DualEmbedding: embeddings = {\u0026#34;text\u0026#34;: None, \u0026#34;visual\u0026#34;: None} # Always generate text embedding embeddings[\u0026#34;text\u0026#34;] = await self.embed_text(chunk.content) # Generate visual embedding if chunk contains images if chunk.images: image_embeddings = [] for img in chunk.images: img_emb = await self.embed_image(img) image_embeddings.append(img_emb) # Average pool image embeddings embeddings[\u0026#34;visual\u0026#34;] = np.mean(image_embeddings, axis=0).tolist() return DualEmbedding(**embeddings) Multi-Tenancy: Organization → Space → Content The system has a 3-tier hierarchy. Organizations contain spaces, spaces contain content. Each space is fully isolated.\nSpace Context Service\nEvery request resolves its space context with eager-loaded relationships:\nclass SpaceContextService: \u0026#34;\u0026#34;\u0026#34; Resolves and caches space context for requests. \u0026#34;\u0026#34;\u0026#34; def __init__(self, cache: Redis): self.cache = cache self.ttl = 300 # 5 minutes async def get_context(self, space_id: str, user_id: str) -\u0026gt; SpaceContext: cache_key = f\u0026#34;space_context:{space_id}:{user_id}\u0026#34; # Try cache first cached = await self.cache.get(cache_key) if cached: return SpaceContext.model_validate_json(cached) # Load from DB with eager loading async with get_session() as session: result = await session.execute( select(Space) .options( selectinload(Space.organization), selectinload(Space.members), selectinload(Space.brand_settings), selectinload(Space.integrations), ) .where(Space.id == space_id) ) space = result.scalar_one_or_none() if not space: raise SpaceNotFoundError(space_id) # Verify user access if not self._user_has_access(space, user_id): raise AccessDeniedError(f\u0026#34;User {user_id} cannot access space {space_id}\u0026#34;) context = SpaceContext( space_id=space.id, organization_id=space.organization.id, brand_voice=space.brand_settings.voice_profile, integrations=[i.type for i in space.integrations], user_role=self._get_user_role(space, user_id), ) # Cache for subsequent requests await self.cache.setex( cache_key, self.ttl, context.model_dump_json() ) return context Middleware Enforcement\nEvery request validates the X-Space-Id header:\nclass SpaceIsolationMiddleware: \u0026#34;\u0026#34;\u0026#34; Enforces space-level isolation for all requests. \u0026#34;\u0026#34;\u0026#34; def __init__(self, app: ASGIApp): self.app = app self.public_paths = {\u0026#34;/health\u0026#34;, \u0026#34;/auth/token\u0026#34;, \u0026#34;/docs\u0026#34;, \u0026#34;/openapi.json\u0026#34;} async def __call__(self, scope: Scope, receive: Receive, send: Send): if scope[\u0026#34;type\u0026#34;] != \u0026#34;http\u0026#34;: await self.app(scope, receive, send) return path = scope[\u0026#34;path\u0026#34;] if path in self.public_paths: await self.app(scope, receive, send) return headers = dict(scope[\u0026#34;headers\u0026#34;]) space_id = headers.get(b\u0026#34;x-space-id\u0026#34;, b\u0026#34;\u0026#34;).decode() if not space_id: response = JSONResponse( {\u0026#34;error\u0026#34;: \u0026#34;X-Space-Id header required\u0026#34;}, status_code=400 ) await response(scope, receive, send) return # Validate space exists and user has access user = scope.get(\u0026#34;user\u0026#34;) try: context = await self.space_service.get_context(space_id, user.id) scope[\u0026#34;space_context\u0026#34;] = context except (SpaceNotFoundError, AccessDeniedError) as e: response = JSONResponse({\u0026#34;error\u0026#34;: str(e)}, status_code=403) await response(scope, receive, send) return await self.app(scope, receive, send) Query Isolation\nAll database queries are scoped to the current space:\nclass SpaceAwareRepository(Generic[T]): \u0026#34;\u0026#34;\u0026#34; Base repository that enforces space isolation. \u0026#34;\u0026#34;\u0026#34; def __init__(self, model: type[T], session: AsyncSession): self.model = model self.session = session def _base_query(self, space_id: str) -\u0026gt; Select: return select(self.model).where(self.model.space_id == space_id) async def get(self, space_id: str, id: str) -\u0026gt; T | None: result = await self.session.execute( self._base_query(space_id).where(self.model.id == id) ) return result.scalar_one_or_none() async def list( self, space_id: str, filters: dict | None = None, limit: int = 100 ) -\u0026gt; list[T]: query = self._base_query(space_id) if filters: for key, value in filters.items(): query = query.where(getattr(self.model, key) == value) query = query.limit(limit) result = await self.session.execute(query) return list(result.scalars().all()) Authentication: RS256 JWT with Key Rotation Security is non-negotiable for a SaaS platform handling client data.\nAutomated 90-Day Key Rotation\nJWT signing keys rotate automatically:\nclass JWTKeyManager: \u0026#34;\u0026#34;\u0026#34; Manages RS256 key pairs with automated rotation. \u0026#34;\u0026#34;\u0026#34; def __init__(self, secret_manager: SecretManagerClient): self.secret_manager = secret_manager self.rotation_days = 90 self.keys: dict[str, RSAPrivateKey] = {} self.public_keys: dict[str, RSAPublicKey] = {} async def initialize(self): \u0026#34;\u0026#34;\u0026#34;Load current and previous keys for seamless rotation.\u0026#34;\u0026#34;\u0026#34; current = await self._load_or_create_key(\u0026#34;current\u0026#34;) previous = await self._load_key(\u0026#34;previous\u0026#34;) self.keys[\u0026#34;current\u0026#34;] = current self.public_keys[\u0026#34;current\u0026#34;] = current.public_key() if previous: self.keys[\u0026#34;previous\u0026#34;] = previous self.public_keys[\u0026#34;previous\u0026#34;] = previous.public_key() async def rotate_if_needed(self): \u0026#34;\u0026#34;\u0026#34;Check if rotation is needed and perform it.\u0026#34;\u0026#34;\u0026#34; metadata = await self._get_key_metadata(\u0026#34;current\u0026#34;) if self._should_rotate(metadata): await self._rotate_keys() async def _rotate_keys(self): \u0026#34;\u0026#34;\u0026#34;Rotate: current -\u0026gt; previous, generate new current.\u0026#34;\u0026#34;\u0026#34; # Move current to previous current_key = await self._load_key(\u0026#34;current\u0026#34;) await self._store_key(\u0026#34;previous\u0026#34;, current_key) # Generate new current new_key = rsa.generate_private_key( public_exponent=65537, key_size=4096, ) await self._store_key(\u0026#34;current\u0026#34;, new_key) # Reload await self.initialize() logger.info(\u0026#34;JWT signing keys rotated successfully\u0026#34;) Zero-Query User Context\nUser identity and permissions are embedded in the JWT, eliminating database lookups for auth:\nclass TokenPayload(BaseModel): sub: str # User ID org_id: str spaces: dict[str, str] # space_id -\u0026gt; role permissions: list[str] exp: datetime iat: datetime jti: str # Unique token ID for revocation class AuthService: async def create_token(self, user: User) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate token with embedded permissions.\u0026#34;\u0026#34;\u0026#34; spaces = await self._get_user_spaces(user.id) payload = TokenPayload( sub=user.id, org_id=user.organization_id, spaces={s.id: s.role for s in spaces}, permissions=self._compute_permissions(user, spaces), exp=datetime.utcnow() + timedelta(hours=24), iat=datetime.utcnow(), jti=str(uuid4()), ) return jwt.encode( payload.model_dump(), self.key_manager.keys[\u0026#34;current\u0026#34;], algorithm=\u0026#34;RS256\u0026#34;, headers={\u0026#34;kid\u0026#34;: \u0026#34;current\u0026#34;} ) In-Memory TTL Cache for O(1) Verification\nPublic keys are cached for fast verification:\nclass TokenVerifier: def __init__(self, key_manager: JWTKeyManager): self.key_manager = key_manager self.revoked_tokens: TTLCache = TTLCache(maxsize=10000, ttl=86400) async def verify(self, token: str) -\u0026gt; TokenPayload: # Decode header to get key ID header = jwt.get_unverified_header(token) kid = header.get(\u0026#34;kid\u0026#34;, \u0026#34;current\u0026#34;) # Get public key (O(1) lookup) public_key = self.key_manager.public_keys.get(kid) if not public_key: raise InvalidTokenError(\u0026#34;Unknown key ID\u0026#34;) # Check revocation (O(1) lookup) try: unverified = jwt.decode(token, options={\u0026#34;verify_signature\u0026#34;: False}) if unverified.get(\u0026#34;jti\u0026#34;) in self.revoked_tokens: raise TokenRevokedError() except jwt.DecodeError: raise InvalidTokenError(\u0026#34;Malformed token\u0026#34;) # Verify signature try: payload = jwt.decode( token, public_key, algorithms=[\u0026#34;RS256\u0026#34;], ) return TokenPayload.model_validate(payload) except jwt.ExpiredSignatureError: raise TokenExpiredError() except jwt.InvalidTokenError as e: raise InvalidTokenError(str(e)) External Integrations Marketing platforms need to pull data from everywhere. We integrate with major ad and analytics platforms.\nAgentBridge Pattern for Slack\nExternal integrations use a bridge pattern that abstracts the service:\nclass SlackBridge: \u0026#34;\u0026#34;\u0026#34; Bridge between AI agents and Slack workspaces. \u0026#34;\u0026#34;\u0026#34; def __init__(self, credentials: SlackCredentials): self.client = AsyncWebClient(token=credentials.bot_token) async def post_content( self, channel: str, content: GeneratedContent, context: SpaceContext ) -\u0026gt; SlackMessage: \u0026#34;\u0026#34;\u0026#34;Post AI-generated content to Slack for review.\u0026#34;\u0026#34;\u0026#34; blocks = self._format_content_blocks(content) response = await self.client.chat_postMessage( channel=channel, blocks=blocks, text=content.plain_text, # Fallback metadata={ \u0026#34;event_type\u0026#34;: \u0026#34;content_review\u0026#34;, \u0026#34;event_payload\u0026#34;: { \u0026#34;content_id\u0026#34;: content.id, \u0026#34;space_id\u0026#34;: context.space_id, \u0026#34;agent_type\u0026#34;: content.source_agent, } } ) return SlackMessage( ts=response[\u0026#34;ts\u0026#34;], channel=response[\u0026#34;channel\u0026#34;], content_id=content.id ) async def handle_interaction(self, payload: dict) -\u0026gt; InteractionResult: \u0026#34;\u0026#34;\u0026#34;Handle button clicks, approvals, etc.\u0026#34;\u0026#34;\u0026#34; action = payload[\u0026#34;actions\u0026#34;][0] if action[\u0026#34;action_id\u0026#34;] == \u0026#34;approve_content\u0026#34;: return await self._handle_approval(payload) elif action[\u0026#34;action_id\u0026#34;] == \u0026#34;request_revision\u0026#34;: return await self._handle_revision_request(payload) raise UnknownActionError(action[\u0026#34;action_id\u0026#34;]) OAuth2 Flow for Google Analytics 4\nAnalytics integration uses OAuth2 with automatic token refresh:\nclass GA4Integration: \u0026#34;\u0026#34;\u0026#34; Google Analytics 4 integration with OAuth2. \u0026#34;\u0026#34;\u0026#34; TOKEN_URL = \u0026#34;https://oauth2.googleapis.com/token\u0026#34; SCOPES = [\u0026#34;https://www.googleapis.com/auth/analytics.readonly\u0026#34;] async def exchange_code(self, code: str, redirect_uri: str) -\u0026gt; OAuthTokens: \u0026#34;\u0026#34;\u0026#34;Exchange authorization code for tokens.\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: response = await client.post( self.TOKEN_URL, data={ \u0026#34;code\u0026#34;: code, \u0026#34;client_id\u0026#34;: self.client_id, \u0026#34;client_secret\u0026#34;: self.client_secret, \u0026#34;redirect_uri\u0026#34;: redirect_uri, \u0026#34;grant_type\u0026#34;: \u0026#34;authorization_code\u0026#34;, } ) response.raise_for_status() data = response.json() return OAuthTokens( access_token=data[\u0026#34;access_token\u0026#34;], refresh_token=data[\u0026#34;refresh_token\u0026#34;], expires_at=datetime.utcnow() + timedelta(seconds=data[\u0026#34;expires_in\u0026#34;]) ) async def get_report( self, tokens: OAuthTokens, property_id: str, metrics: list[str], dimensions: list[str], date_range: DateRange ) -\u0026gt; AnalyticsReport: \u0026#34;\u0026#34;\u0026#34;Fetch analytics data with automatic token refresh.\u0026#34;\u0026#34;\u0026#34; # Refresh if needed if tokens.expires_at \u0026lt; datetime.utcnow() + timedelta(minutes=5): tokens = await self._refresh_tokens(tokens) async with httpx.AsyncClient() as client: response = await client.post( f\u0026#34;https://analyticsdata.googleapis.com/v1beta/properties/{property_id}:runReport\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {tokens.access_token}\u0026#34;}, json={ \u0026#34;metrics\u0026#34;: [{\u0026#34;name\u0026#34;: m} for m in metrics], \u0026#34;dimensions\u0026#34;: [{\u0026#34;name\u0026#34;: d} for d in dimensions], \u0026#34;dateRanges\u0026#34;: [{\u0026#34;startDate\u0026#34;: date_range.start, \u0026#34;endDate\u0026#34;: date_range.end}], } ) response.raise_for_status() return AnalyticsReport.from_response(response.json()) Real-Time Streaming Users expect to see AI responses as they generate, not after a 10-second wait.\nEvent Persistence for Reconstruction\nAgent run events are persisted for replay and debugging:\nclass AgentRunEvent(Base): __tablename__ = \u0026#34;agent_run_events\u0026#34; id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4) run_id: Mapped[UUID] = mapped_column(ForeignKey(\u0026#34;agent_runs.id\u0026#34;), index=True) sequence: Mapped[int] # For ordering event_type: Mapped[str] # token, tool_call, tool_result, error, complete payload: Mapped[dict] = mapped_column(JSONB) created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow) __table_args__ = ( Index(\u0026#34;ix_run_events_run_seq\u0026#34;, \u0026#34;run_id\u0026#34;, \u0026#34;sequence\u0026#34;), ) class EventPersistence: async def persist_event( self, run_id: UUID, event_type: str, payload: dict ): async with self.session() as session: # Get next sequence number result = await session.execute( select(func.coalesce(func.max(AgentRunEvent.sequence), 0)) .where(AgentRunEvent.run_id == run_id) ) next_seq = result.scalar() + 1 event = AgentRunEvent( run_id=run_id, sequence=next_seq, event_type=event_type, payload=payload ) session.add(event) await session.commit() async def replay_events( self, run_id: UUID, from_sequence: int = 0 ) -\u0026gt; AsyncIterator[AgentRunEvent]: \u0026#34;\u0026#34;\u0026#34;Replay events for a run, optionally from a sequence.\u0026#34;\u0026#34;\u0026#34; async with self.session() as session: result = await session.stream( select(AgentRunEvent) .where(AgentRunEvent.run_id == run_id) .where(AgentRunEvent.sequence \u0026gt; from_sequence) .order_by(AgentRunEvent.sequence) ) async for event in result.scalars(): yield event WebSocket Integration with Reconnection\nClients connect via WebSocket and can reconnect to resume streams:\nclass StreamingEndpoint: async def websocket_handler(self, websocket: WebSocket): await websocket.accept() try: while True: message = await websocket.receive_json() if message[\u0026#34;type\u0026#34;] == \u0026#34;subscribe\u0026#34;: await self._handle_subscribe(websocket, message) elif message[\u0026#34;type\u0026#34;] == \u0026#34;resume\u0026#34;: await self._handle_resume(websocket, message) elif message[\u0026#34;type\u0026#34;] == \u0026#34;message\u0026#34;: await self._handle_message(websocket, message) except WebSocketDisconnect: await self._handle_disconnect(websocket) async def _handle_resume(self, websocket: WebSocket, message: dict): \u0026#34;\u0026#34;\u0026#34;Resume a stream from a specific sequence.\u0026#34;\u0026#34;\u0026#34; run_id = UUID(message[\u0026#34;run_id\u0026#34;]) last_seq = message.get(\u0026#34;last_sequence\u0026#34;, 0) # Replay missed events async for event in self.persistence.replay_events(run_id, last_seq): await websocket.send_json({ \u0026#34;type\u0026#34;: event.event_type, \u0026#34;sequence\u0026#34;: event.sequence, \u0026#34;payload\u0026#34;: event.payload }) # Continue with live stream if still running run = await self.get_run(run_id) if run.status == \u0026#34;running\u0026#34;: await self._subscribe_to_live(websocket, run_id) Performance Optimizations Session Pre-Creation\nLLM sessions have cold start latency. We pre-warm them:\nclass SessionPool: \u0026#34;\u0026#34;\u0026#34; Pool of pre-warmed LLM sessions for reduced latency. \u0026#34;\u0026#34;\u0026#34; def __init__(self, pool_size: int = 5): self.pool_size = pool_size self.available: asyncio.Queue[LLMSession] = asyncio.Queue() self.in_use: set[LLMSession] = set() async def initialize(self): \u0026#34;\u0026#34;\u0026#34;Pre-create sessions on startup.\u0026#34;\u0026#34;\u0026#34; for _ in range(self.pool_size): session = await self._create_session() await self.available.put(session) async def acquire(self) -\u0026gt; LLMSession: \u0026#34;\u0026#34;\u0026#34;Get a pre-warmed session.\u0026#34;\u0026#34;\u0026#34; try: session = self.available.get_nowait() except asyncio.QueueEmpty: # Pool exhausted, create new session session = await self._create_session() self.in_use.add(session) return session async def release(self, session: LLMSession): \u0026#34;\u0026#34;\u0026#34;Return session to pool.\u0026#34;\u0026#34;\u0026#34; self.in_use.discard(session) if self.available.qsize() \u0026lt; self.pool_size: # Reset and return to pool await session.reset() await self.available.put(session) else: # Pool full, close session await session.close() async def _create_session(self) -\u0026gt; LLMSession: \u0026#34;\u0026#34;\u0026#34;Create and warm up a new session.\u0026#34;\u0026#34;\u0026#34; session = LLMSession() # Warm up with minimal prompt await session.generate(\u0026#34;Hello\u0026#34;, max_tokens=1) return session Async PostgreSQL Connection Pool\nWe use asyncpg with connection overflow handling:\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession from sqlalchemy.pool import AsyncAdaptedQueuePool engine = create_async_engine( DATABASE_URL, poolclass=AsyncAdaptedQueuePool, pool_size=15, max_overflow=25, # Allow burst to 40 total pool_timeout=30, pool_recycle=1800, # Recycle connections every 30 min pool_pre_ping=True, # Verify connections before use ) Structured Logging with Correlation IDs\nEvery request gets a correlation ID that flows through all log entries:\nclass CorrelationMiddleware: async def __call__(self, scope: Scope, receive: Receive, send: Send): if scope[\u0026#34;type\u0026#34;] == \u0026#34;http\u0026#34;: correlation_id = scope[\u0026#34;headers\u0026#34;].get( b\u0026#34;x-correlation-id\u0026#34;, str(uuid4()).encode() ).decode() # Set in context var for logging correlation_context.set(correlation_id) # Add to response headers async def send_wrapper(message): if message[\u0026#34;type\u0026#34;] == \u0026#34;http.response.start\u0026#34;: headers = MutableHeaders(scope=message) headers.append(\u0026#34;x-correlation-id\u0026#34;, correlation_id) await send(message) await self.app(scope, receive, send_wrapper) else: await self.app(scope, receive, send) # Logger configuration class CorrelationFilter(logging.Filter): def filter(self, record): record.correlation_id = correlation_context.get(\u0026#34;\u0026#34;) return True What I Learned Contamination detection is essential. LLMs will faithfully execute instructions hidden in retrieved documents. You need explicit detection for prompt injection patterns, or your agents become attack vectors.\nParallel retrieval changes the latency equation. When retrieval is your bottleneck, parallelizing streams has a multiplicative effect. Query enhancement adds latency but saves more by improving retrieval quality.\nSpace-level isolation requires everywhere enforcement. Every query, every cache key, every log entry needs space scoping. A single missed scope check creates a data leak. Middleware is necessary but not sufficient—repositories and services need built-in isolation.\nCheckpointing is worth the complexity. Long-running AI conversations need persistence. LangGraph\u0026rsquo;s PostgreSQL checkpointer handles this cleanly, but you still need event streaming for real-time UI and replay.\nInterested? If you\u0026rsquo;re building AI-powered SaaS products or want to discuss LangGraph patterns, book a call.\n","permalink":"https://mhassan.dev/projects/obelisk/","summary":"\u003cp\u003eObelisk is an AI-powered marketing platform that helps teams create content with brand consistency. It orchestrates specialized AI agents for SEO, email marketing, brand voice analysis, and strategy—all within a multi-tenant SaaS architecture with space-level isolation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTech Stack:\u003c/strong\u003e FastAPI, LangGraph, PostgreSQL, Vertex AI, Redis, Google Cloud\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSource:\u003c/strong\u003e Private (commercial product) · \u003ca href=\"/book-a-call/\"\u003eBook a call\u003c/a\u003e to discuss\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"the-hard-problems\"\u003eThe Hard Problems\u003c/h2\u003e\n\u003cp\u003eBuilding production AI systems exposes problems that don\u0026rsquo;t appear in tutorials:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAgent reliability\u003c/strong\u003e — LLMs hallucinate, get confused, and can be manipulated. How do you build agents that fail gracefully and resist prompt injection?\u003c/p\u003e","title":"Obelisk"},{"content":" Every few months, something gets released that looks like infrastructure plumbing but turns out to matter more than the flashy launches. Model Context Protocol (MCP) is one of those things.\nIf you\u0026rsquo;re a developer working with LLMs, MCP will change how you integrate AI into your workflows. Here\u0026rsquo;s an early-adopter perspective on what it is, why it matters, and how to actually use it.\nWhat Problem Does MCP Solve? Today\u0026rsquo;s AI tools are context-starved. You paste code into ChatGPT, upload files to Claude, manually copy database schemas into prompts. Every session starts from scratch. Every context window is a blank slate.\nThis is absurd.\nYour IDE knows your codebase. Your browser knows your tabs. Your database has a schema. Why are you the middleman copying information between tools that should talk to each other?\nMCP is a protocol for AI models to access external context sources directly. Instead of \u0026ldquo;paste your code here,\u0026rdquo; the model can ask \u0026ldquo;what files are in this project?\u0026rdquo; and get an answer.\nIt\u0026rsquo;s not magic—it\u0026rsquo;s plumbing. But it\u0026rsquo;s plumbing that enables magic.\nMCP Architecture for Developers MCP has three components:\n1. MCP Hosts: Applications that want to use AI (Claude Desktop, IDEs, custom apps)\n2. MCP Servers: Programs that expose data/functionality to AI (database connectors, file system access, API wrappers)\n3. The Protocol: JSON-RPC communication between hosts and servers\n┌─────────────┐ MCP Protocol ┌─────────────┐ │ MCP Host │◄───────────────────►│ MCP Server │ │ (Claude) │ JSON-RPC/stdio │ (Your DB) │ └─────────────┘ └─────────────┘ The key insight: MCP servers are just programs that respond to standard queries. You can write one in any language, for any data source.\nHow it differs from function calling Function calling (tool use) lets models invoke specific functions you\u0026rsquo;ve defined. MCP is broader—it lets models discover what\u0026rsquo;s available and query it dynamically.\n# Function calling: You predefine everything tools = [ {\u0026#34;name\u0026#34;: \u0026#34;get_user\u0026#34;, \u0026#34;parameters\u0026#34;: {...}}, {\u0026#34;name\u0026#34;: \u0026#34;list_orders\u0026#34;, \u0026#34;parameters\u0026#34;: {...}}, ] # MCP: Model discovers capabilities # \u0026#34;What resources are available?\u0026#34; # → [\u0026#34;users\u0026#34;, \u0026#34;orders\u0026#34;, \u0026#34;products\u0026#34;, \u0026#34;analytics\u0026#34;] # \u0026#34;What can I do with users?\u0026#34; # → [list, get, search, ...] MCP is more flexible, but requires more trust. You\u0026rsquo;re giving the model keys to explore, not just execute predefined actions.\nBuilding an MCP Server Let\u0026rsquo;s build a simple MCP server that exposes a PostgreSQL database. This is genuinely useful—suddenly Claude can query your database schema, understand relationships, and help with queries.\n# db_mcp_server.py import asyncio import json from mcp.server import Server from mcp.types import Resource, Tool, TextContent import asyncpg server = Server(\u0026#34;postgres-explorer\u0026#34;) pool = None @server.list_resources() async def list_resources(): \u0026#34;\u0026#34;\u0026#34;List available database resources.\u0026#34;\u0026#34;\u0026#34; async with pool.acquire() as conn: tables = await conn.fetch(\u0026#34;\u0026#34;\u0026#34; SELECT table_name FROM information_schema.tables WHERE table_schema = \u0026#39;public\u0026#39; \u0026#34;\u0026#34;\u0026#34;) return [ Resource( uri=f\u0026#34;postgres://tables/{t[\u0026#39;table_name\u0026#39;]}\u0026#34;, name=t[\u0026#39;table_name\u0026#39;], description=f\u0026#34;Table: {t[\u0026#39;table_name\u0026#39;]}\u0026#34; ) for t in tables ] @server.read_resource() async def read_resource(uri: str): \u0026#34;\u0026#34;\u0026#34;Get schema for a specific table.\u0026#34;\u0026#34;\u0026#34; table_name = uri.split(\u0026#34;/\u0026#34;)[-1] async with pool.acquire() as conn: columns = await conn.fetch(\u0026#34;\u0026#34;\u0026#34; SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE table_name = $1 ORDER BY ordinal_position \u0026#34;\u0026#34;\u0026#34;, table_name) schema = \u0026#34;\\n\u0026#34;.join([ f\u0026#34; {c[\u0026#39;column_name\u0026#39;]}: {c[\u0026#39;data_type\u0026#39;]}\u0026#34; f\u0026#34; {\u0026#39;(nullable)\u0026#39; if c[\u0026#39;is_nullable\u0026#39;] == \u0026#39;YES\u0026#39; else \u0026#39;\u0026#39;}\u0026#34; for c in columns ]) return TextContent( type=\u0026#34;text\u0026#34;, text=f\u0026#34;Table: {table_name}\\n\\n{schema}\u0026#34; ) @server.list_tools() async def list_tools(): \u0026#34;\u0026#34;\u0026#34;Expose query capability as a tool.\u0026#34;\u0026#34;\u0026#34; return [ Tool( name=\u0026#34;query\u0026#34;, description=\u0026#34;Execute a read-only SQL query\u0026#34;, inputSchema={ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;sql\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;SQL query\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;sql\u0026#34;] } ) ] @server.call_tool() async def call_tool(name: str, arguments: dict): \u0026#34;\u0026#34;\u0026#34;Execute the query tool.\u0026#34;\u0026#34;\u0026#34; if name != \u0026#34;query\u0026#34;: raise ValueError(f\u0026#34;Unknown tool: {name}\u0026#34;) sql = arguments[\u0026#34;sql\u0026#34;] # Safety: only allow SELECT if not sql.strip().upper().startswith(\u0026#34;SELECT\u0026#34;): return TextContent( type=\u0026#34;text\u0026#34;, text=\u0026#34;Error: Only SELECT queries allowed\u0026#34; ) async with pool.acquire() as conn: rows = await conn.fetch(sql) return TextContent( type=\u0026#34;text\u0026#34;, text=json.dumps([dict(r) for r in rows], default=str) ) async def main(): global pool pool = await asyncpg.create_pool( \u0026#34;postgresql://user:pass@localhost/mydb\u0026#34; ) await server.run() if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) Configure Claude Desktop to use this server:\n// claude_desktop_config.json { \u0026#34;mcpServers\u0026#34;: { \u0026#34;postgres\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;/path/to/db_mcp_server.py\u0026#34;] } } } Now Claude can explore your database schema, understand table relationships, and help write queries—with real context, not guesses.\nUse Cases That Make Sense After building several MCP integrations, here\u0026rsquo;s where they shine:\nDatabase introspection The example above. Claude understanding your actual schema instead of inventing tables that don\u0026rsquo;t exist. Worth the setup for any database-heavy work.\nDocumentation access Point an MCP server at your internal docs, and Claude can reference them when answering questions. Better than pasting docs into prompts.\n@server.list_resources() async def list_resources(): docs_path = Path(\u0026#34;/path/to/docs\u0026#34;) return [ Resource( uri=f\u0026#34;docs://{p.relative_to(docs_path)}\u0026#34;, name=p.stem, description=f\u0026#34;Documentation: {p.stem}\u0026#34; ) for p in docs_path.rglob(\u0026#34;*.md\u0026#34;) ] Git repository analysis Give Claude read access to your git history. \u0026ldquo;What changed in the last week?\u0026rdquo; \u0026ldquo;Who usually works on this file?\u0026rdquo; \u0026ldquo;Show me the commit that introduced this function.\u0026rdquo;\nExternal API wrappers Wrap frequently-used APIs as MCP tools. Instead of explaining API semantics in every prompt, Claude can discover and call them directly.\nAt Entropy Labs, we built an MCP server for our internal APIs. Now Claude can check deployment status, query metrics, and look up customer data—all without manual copy-paste.\nCurrent Limitations MCP is early-stage. Expect rough edges:\n1. Ecosystem is immature. Pre-built servers exist for common tools (filesystem, GitHub, Slack), but you\u0026rsquo;ll write custom ones for anything domain-specific.\n2. Security requires thought. MCP servers have access to real systems. A poorly-written server can expose sensitive data or allow unintended actions. Audit carefully.\n3. Performance varies. Large resource lists or slow database queries can delay responses. Implement timeouts and pagination.\n4. Discovery UX is evolving. Models don\u0026rsquo;t always explore available resources effectively. Sometimes you need to prompt \u0026ldquo;check the available MCP resources\u0026rdquo; explicitly.\n5. Not all hosts support it equally. Claude Desktop has full support. Third-party integrations are catching up.\nSecurity Considerations MCP gives AI models access to real systems. Take security seriously:\n1. Principle of least privilege. Your MCP server for database exploration should use a read-only database user. Don\u0026rsquo;t give Claude DROP TABLE permissions.\n2. Input validation. The model controls tool inputs. Validate everything. SQL injection is a real risk if you\u0026rsquo;re not careful.\n# Bad: Direct interpolation sql = f\u0026#34;SELECT * FROM {arguments[\u0026#39;table\u0026#39;]}\u0026#34; # Good: Whitelist validation allowed_tables = [\u0026#34;users\u0026#34;, \u0026#34;orders\u0026#34;, \u0026#34;products\u0026#34;] if arguments[\u0026#34;table\u0026#34;] not in allowed_tables: raise ValueError(\u0026#34;Invalid table\u0026#34;) 3. Rate limiting. Models can call tools repeatedly. Implement rate limits to prevent runaway queries.\n4. Audit logging. Log every MCP call with timestamps and context. You\u0026rsquo;ll want this when debugging or investigating unexpected behavior.\n5. Network isolation. Run MCP servers with minimal network access. They shouldn\u0026rsquo;t be able to reach arbitrary endpoints.\nThe Bigger Picture MCP is part of a larger trend: AI systems becoming first-class participants in developer workflows, not just chat windows you paste into.\nWhere this leads:\nIDE integration. Your editor\u0026rsquo;s AI isn\u0026rsquo;t just autocompleting—it\u0026rsquo;s understanding your entire project, git history, and documentation.\nAgentic workflows. AI that can actually do things: run tests, check CI status, deploy to staging. MCP provides the plumbing.\nCustom enterprise AI. Organizations can expose internal knowledge bases, APIs, and databases to AI tools without sending data to external services.\nStandardization. MCP is open spec. If it gains adoption, we\u0026rsquo;ll see interoperability between AI tools that currently don\u0026rsquo;t talk to each other.\nThis is infrastructure work. It\u0026rsquo;s not as exciting as a new model release. But it\u0026rsquo;s what turns capable models into capable systems.\nGetting Started If you want to experiment:\nInstall Claude Desktop and enable MCP in settings Try existing servers: Filesystem, GitHub, and Postgres servers exist as reference implementations Build something small: Start with a read-only server exposing one data source Iterate on discovery: Watch how Claude explores resources and tune your server\u0026rsquo;s descriptions The learning curve is manageable if you\u0026rsquo;ve built APIs before. The mental shift is thinking about what context would help the model, then building servers to provide it.\nThe Bottom Line MCP solves the \u0026ldquo;context starvation\u0026rdquo; problem that limits current AI tools. It\u0026rsquo;s not a product—it\u0026rsquo;s infrastructure that makes better products possible.\nFor developers, the implications are:\nLess copy-paste, more integration AI tools that understand your environment, not generic examples Custom workflows that weren\u0026rsquo;t possible before If you\u0026rsquo;re building AI-powered developer tools, MCP is worth learning now. The ecosystem is early, but the direction is clear.\nAnd honestly? Building MCP servers is kind of fun. There\u0026rsquo;s something satisfying about giving Claude access to your systems and watching it figure things out.\n","permalink":"https://mhassan.dev/blog/model-context-protocol/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003cp\u003eEvery few months, something gets released that looks like infrastructure plumbing but turns out to matter more than the flashy launches. Model Context Protocol (MCP) is one of those things.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re a developer working with LLMs, MCP will change how you integrate AI into your workflows. Here\u0026rsquo;s an early-adopter perspective on what it is, why it matters, and how to actually use it.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-problem-does-mcp-solve\"\u003e\u003cspan style=\"color:#8ac7db\"\u003eWhat Problem Does MCP Solve?\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eToday\u0026rsquo;s AI tools are context-starved. You paste code into ChatGPT, upload files to Claude, manually copy database schemas into prompts. Every session starts from scratch. Every context window is a blank slate.\u003c/p\u003e","title":"Model Context Protocol: Why This Matters More Than You Think"},{"content":" Extended thinking isn\u0026rsquo;t just \u0026ldquo;model thinks longer\u0026rdquo;—it\u0026rsquo;s a fundamentally different interaction model. If you\u0026rsquo;re prompting extended thinking models (Claude Opus, o1) the same way you prompt standard models, you\u0026rsquo;re leaving most of the value on the table.\nThis post is a developer\u0026rsquo;s mental model for working with these systems: when to use them, how to prompt them, and what trade-offs to expect.\nHow Extended Thinking Actually Works Standard LLMs generate tokens one at a time, each token conditioned on everything before it. The model \u0026ldquo;thinks\u0026rdquo; only as fast as it speaks. Ask it to solve a complex problem, and it often commits to an approach in the first few tokens, then rationalizes that approach even if it\u0026rsquo;s wrong.\nExtended thinking models add an intermediate step: they generate a reasoning trace before producing the final answer. Think of it as the model writing notes in the margin before responding.\nThis matters because:\n1. More tokens = more \u0026ldquo;working memory.\u0026rdquo; The reasoning trace gives the model space to consider alternatives, check its work, and revise approaches. It\u0026rsquo;s like the difference between solving math problems in your head vs. on scratch paper.\n2. The reasoning trace isn\u0026rsquo;t shown to you. You see the polished output, not the exploratory thinking. This is both a feature (cleaner responses) and a limitation (harder to debug when it goes wrong).\n3. The model can \u0026ldquo;backtrack\u0026rdquo; conceptually. Standard models can\u0026rsquo;t unsay tokens. Extended thinking can reason through an approach, decide it\u0026rsquo;s wrong, and try another—all before responding.\nClaude Opus vs. o1: Different approaches Both are \u0026ldquo;thinking models,\u0026rdquo; but they work differently:\nClaude Opus 4.5:\nExtended thinking via longer internal reasoning Configurable thinking budget Reasoning partially visible via API (for debugging) Optimized for helpfulness and safety alongside reasoning OpenAI o1:\nChain-of-thought at massive scale Hidden reasoning trace (not exposed via API) Optimized for benchmark performance Tends toward longer, more verbose responses In practice, I find Opus better for interactive development work (code review, debugging) and o1 better for competition-style problems (math, algorithms). Your mileage will vary.\nWhen Extended Thinking Helps Not every task benefits from extended thinking. Here\u0026rsquo;s a rough heuristic:\nGreat for: Complex code generation. Multi-file changes, refactoring, implementing algorithms with edge cases. The model can reason about interactions between components.\nArchitecture decisions. \u0026ldquo;Here\u0026rsquo;s our system. What are the tradeoffs of adding a cache here vs. there?\u0026rdquo; Extended thinking models consider more factors before committing.\nBug diagnosis. Given error messages, logs, and code, the model can reason through possible causes rather than pattern-matching to the most common fix.\nMulti-step reasoning. Any task where the answer depends on intermediate conclusions: tax calculations, game theory, proof verification.\nNot worth it for: Simple lookups. \u0026ldquo;What\u0026rsquo;s the syntax for a Python list comprehension?\u0026rdquo; Fast models give the same answer instantly.\nLatency-sensitive applications. Extended thinking takes 10-60 seconds. If your UX requires sub-second responses, it won\u0026rsquo;t work.\nHigh-volume, low-complexity tasks. Summarizing 10,000 documents? Use the cheapest, fastest model that works. Extended thinking is expensive overkill.\nTasks requiring current information. Thinking longer doesn\u0026rsquo;t give the model access to information it doesn\u0026rsquo;t have. Use retrieval augmentation instead.\nPrompting for Extended Thinking Here\u0026rsquo;s the counterintuitive part: stop giving step-by-step instructions.\nStandard prompting advice says to break down tasks, provide explicit steps, guide the model through your reasoning. This backfires with extended thinking models.\nWhy detailed instructions hurt When you say \u0026ldquo;First do X, then do Y, then do Z,\u0026rdquo; you\u0026rsquo;re constraining the model\u0026rsquo;s reasoning. You\u0026rsquo;ve decided the approach before it can think. If your approach is suboptimal, the model will faithfully execute your suboptimal plan.\nExtended thinking models are better at figuring out approaches than you are (for many tasks). Let them.\nWhat to do instead Provide context, not instructions:\n# Bad \u0026#34;First, read through the codebase structure. Then, identify files that handle authentication. Next, look for potential security vulnerabilities. Finally, provide recommendations.\u0026#34; # Good \u0026#34;Here\u0026#39;s our authentication system [files]. We\u0026#39;re concerned about security. Analyze this thoroughly and identify any vulnerabilities or improvements.\u0026#34; The second version lets the model decide how to approach the analysis. It might find approaches you wouldn\u0026rsquo;t have specified.\nState the goal, not the process:\n# Bad \u0026#34;Use the following algorithm: first sort by date, then group by category, then calculate averages...\u0026#34; # Good \u0026#34;I need to understand spending patterns in this transaction data. What insights can you find?\u0026#34; Trust the model\u0026rsquo;s reasoning:\n# Bad \u0026#34;Think step by step. First consider X. Then consider Y. Show your work.\u0026#34; # Good \u0026#34;This is a complex problem. Take your time reasoning through it.\u0026#34; The \u0026ldquo;think step by step\u0026rdquo; prompt was designed for models that didn\u0026rsquo;t think before answering. Extended thinking models already do this internally. You\u0026rsquo;re just adding noise.\nReal Prompt Comparison Let me show a concrete example. Task: review a Django view for potential issues.\nStandard model prompt (optimized for GPT-4): Review this Django view for issues. Check for: 1. N+1 queries 2. Missing error handling 3. Security vulnerabilities 4. Performance problems 5. Code style issues For each issue found, explain the problem and provide a fix. [code] This works okay with GPT-4. You\u0026rsquo;ve told it what to look for.\nExtended thinking prompt (optimized for Opus): Here\u0026#39;s a Django view from our production system. This view handles user dashboard data and is called ~5000 times/day. We\u0026#39;ve had intermittent timeouts but haven\u0026#39;t identified the cause. Please thoroughly analyze this code. [code] Notice what changed:\nRemoved the checklist (let the model decide what to look for) Added context (production, call volume, symptom) Broader request (thoroughly analyze vs. check for X) The extended thinking model will likely check everything on the first list plus things you didn\u0026rsquo;t think to ask about. And it\u0026rsquo;ll prioritize based on the context (intermittent timeouts → probably an intermittent performance issue, not a style problem).\nCost-Performance Trade-offs Extended thinking is expensive. A complex analysis might use 10-50K tokens of thinking, plus input/output tokens. That\u0026rsquo;s $0.50-2.00 per query at current Opus pricing.\nWhen to pay for thinking: High-stakes decisions (architecture, security audits) Complex debugging that would take you hours Problems you\u0026rsquo;ve failed to solve with faster models When to use fast models: Routine code generation Simple Q\u0026amp;A High-volume batch processing Hybrid approach: Use fast models for initial attempts. Escalate to extended thinking when:\nFast model gives wrong answer Fast model\u0026rsquo;s confidence is low Task is in extended thinking\u0026rsquo;s sweet spot At Entropy Labs, we route queries based on estimated complexity. Simple queries go to Haiku (fast, cheap). Complex queries with keywords like \u0026ldquo;debug,\u0026rdquo; \u0026ldquo;analyze,\u0026rdquo; or \u0026ldquo;architecture\u0026rdquo; go to Opus.\nPractical Integration Patterns Fallback chains async def analyze_code(code: str, context: str) -\u0026gt; str: # Try fast model first fast_response = await sonnet.analyze(code, context) if fast_response.confidence \u0026lt; 0.7 or \u0026#34;uncertain\u0026#34; in fast_response: # Escalate to extended thinking return await opus.analyze(code, context, extended_thinking=True) return fast_response Streaming extended thinking Extended thinking can take 30-60 seconds. Don\u0026rsquo;t leave users staring at a spinner:\nasync def stream_analysis(query: str): # Send status updates while thinking yield {\u0026#34;status\u0026#34;: \u0026#34;analyzing\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;reading code\u0026#34;} async for event in opus.stream(query): if event.type == \u0026#34;thinking\u0026#34;: yield {\u0026#34;status\u0026#34;: \u0026#34;analyzing\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;reasoning\u0026#34;} elif event.type == \u0026#34;response\u0026#34;: yield {\u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, \u0026#34;result\u0026#34;: event.content} Managing user expectations Extended thinking responses are worth waiting for—but users don\u0026rsquo;t know that. Set expectations:\n\u0026#34;This is a complex analysis. I\u0026#39;ll take 30-60 seconds to think through this carefully. For quick questions, ask me directly instead.\u0026#34; The Bottom Line Extended thinking is the biggest practical advance in LLMs since GPT-4. But it requires a mindset shift:\nStop micromanaging. Provide context, not instructions. Use it selectively. Extended thinking for complex problems, fast models for everything else. Budget for latency. 30-60 seconds is the new normal for hard problems. Trust but verify. Extended thinking is more reliable, not infallible. The models that think before they speak are finally here. Learn to let them think.\n","permalink":"https://mhassan.dev/blog/extended-thinking-llms/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003cp\u003eExtended thinking isn\u0026rsquo;t just \u0026ldquo;model thinks longer\u0026rdquo;—it\u0026rsquo;s a fundamentally different interaction model. If you\u0026rsquo;re prompting extended thinking models (Claude Opus, o1) the same way you prompt standard models, you\u0026rsquo;re leaving most of the value on the table.\u003c/p\u003e\n\u003cp\u003eThis post is a developer\u0026rsquo;s mental model for working with these systems: when to use them, how to prompt them, and what trade-offs to expect.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"how-extended-thinking-actually-works\"\u003e\u003cspan style=\"color:#8ac7db\"\u003eHow Extended Thinking Actually Works\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eStandard LLMs generate tokens one at a time, each token conditioned on everything before it. The model \u0026ldquo;thinks\u0026rdquo; only as fast as it speaks. Ask it to solve a complex problem, and it often commits to an approach in the first few tokens, then rationalizes that approach even if it\u0026rsquo;s wrong.\u003c/p\u003e","title":"Extended Thinking in LLMs: A Mental Model for Developers"},{"content":" The graveyard of failed AI features is full of chatbots nobody asked for.\nEvery product team I talk to has the same story: leadership watched a GPT demo, got excited, and mandated \u0026ldquo;we need AI in the product.\u0026rdquo; Three months later, there\u0026rsquo;s a chatbot in the corner of the app that 3% of users have tried and 0.5% use regularly.\nAs CPO at Entropy Labs, I\u0026rsquo;ve been on both sides of this. I\u0026rsquo;ve built AI features that users loved and killed features that seemed brilliant in demos but died in production. Here\u0026rsquo;s what I\u0026rsquo;ve learned about the difference.\nThe Chatbot Trap Why does everyone default to chatbots? Because they\u0026rsquo;re visible. They feel \u0026ldquo;AI-y.\u0026rdquo; Stakeholders can immediately understand what changed.\nBut chatbots are a terrible default for most products:\n1. They require users to change behavior. Your users learned your product\u0026rsquo;s UI. Now you\u0026rsquo;re asking them to type natural language queries instead of clicking buttons. That\u0026rsquo;s friction, not improvement.\n2. Conversational interfaces are slow. Typing \u0026ldquo;show me last month\u0026rsquo;s sales\u0026rdquo; is slower than clicking a date picker and a report button. You\u0026rsquo;ve made the product worse.\n3. Error handling is awkward. When a form validation fails, you show a red border. When a chatbot misunderstands, you get an uncanny valley response that erodes trust.\n4. The happy path is narrow. Chatbots demo well because demos follow scripts. Real users ask questions your training data never anticipated.\nThis isn\u0026rsquo;t to say chatbots are always wrong. Customer support, complex search interfaces, and genuinely conversational use cases benefit from them. But \u0026ldquo;we should add AI\u0026rdquo; shouldn\u0026rsquo;t automatically mean \u0026ldquo;we should add a chatbot.\u0026rdquo;\nAI Features That Actually Work The best AI features are invisible. They make the product smarter without making users work harder.\nSmart Defaults and Autocomplete This is the highest-ROI AI feature for most products. Example:\nWithout AI: User fills out a 12-field form to create a new report.\nWith AI: Based on their past behavior, 8 fields are pre-populated. They adjust 2-3 things and submit.\nYou\u0026rsquo;ve saved 90% of their time without teaching them a new interaction model. Gmail\u0026rsquo;s Smart Compose is the canonical example—it doesn\u0026rsquo;t ask users to chat, it just helps them type faster.\nAt Entropy Labs, we added predictive field completion to our data entry flows. Form completion time dropped 40%. Users didn\u0026rsquo;t need a tutorial; the feature just appeared and helped.\nBackground Processing and Summarization Instead of asking users to request summaries, generate them proactively:\nWhen a document is uploaded, automatically extract key entities and surface them After a meeting recording is processed, show highlights without being asked When data changes significantly, notify users with a human-readable summary The pattern: do the AI work before the user asks. They open their dashboard and insights are already there.\nAnomaly Detection That Surfaces Action Most AI anomaly detection is useless because it surfaces noise. \u0026ldquo;Revenue increased 12%!\u0026rdquo; Great, that\u0026rsquo;s normal seasonality.\nUseful anomaly detection:\nFilters out known patterns (seasonality, promotions, etc.) Ranks by business impact, not statistical significance Suggests specific actions, not just observations We rebuilt our alerting system around this. Instead of \u0026ldquo;metric X changed,\u0026rdquo; users see \u0026ldquo;Conversion rate dropped 18% in the mobile checkout flow—this is unusual and affecting ~$X in daily revenue. The drop started after deploy #1234.\u0026rdquo;\nThat\u0026rsquo;s actionable. That\u0026rsquo;s worth interrupting someone for.\nContent Generation in Context \u0026ldquo;Generate marketing copy\u0026rdquo; is a feature. \u0026ldquo;Pre-draft an email based on this customer\u0026rsquo;s history and your previous conversations\u0026rdquo; is a useful feature.\nThe difference:\nFeature: Blank slate + AI = generic output Useful feature: Context + AI = relevant output When AI knows the context—what the user is working on, what they\u0026rsquo;ve done before, what their goals are—generation becomes useful rather than gimmicky.\nThe Build vs. Buy Calculation Every AI feature has three implementation paths:\n1. API calls (fastest, least differentiated) Use OpenAI/Anthropic/etc. directly. Good for:\nQuick prototypes Features where the AI isn\u0026rsquo;t the value prop When you need to ship fast Cost: $0.01-0.10 per query, depends on model and tokens.\n2. Fine-tuning (medium effort, better quality) Train a model on your data for specific tasks. Good for:\nDomain-specific language (legal, medical, technical) Consistent output formatting Cost reduction at scale Cost: Training costs + inference costs (often lower than base models).\n3. Build your own (almost never worth it) Unless you\u0026rsquo;re at Google/Anthropic scale, don\u0026rsquo;t. Even if you have ML engineers, their time is better spent on fine-tuning and application logic than training foundation models.\nAt Entropy Labs, 90% of our AI features are API calls with good prompting. 10% are fine-tuned for specific classification tasks where we needed consistent output format. We haven\u0026rsquo;t built a model from scratch—and we probably never will.\nUser Research for AI Features Standard user research methods break down for AI features because users don\u0026rsquo;t know what they want.\n\u0026ldquo;Would you use an AI feature that [describes feature]?\u0026rdquo; yields meaningless answers. Users imagine the best-case scenario and say yes. In production, they encounter edge cases and stop using it.\nWhat actually works: 1. Watch behavior, not stated preferences. Add the feature as a quiet experiment. Measure adoption over 4+ weeks, not just trial rate. Initial curiosity doesn\u0026rsquo;t equal sustained usage.\n2. Measure task completion, not satisfaction. \u0026ldquo;Do you like the AI suggestions?\u0026rdquo; matters less than \u0026ldquo;Did you complete the task faster with AI suggestions enabled?\u0026rdquo;\n3. A/B test carefully. AI features often have novelty effects. Test against holdout groups and measure over longer periods than you would for UI changes.\n4. Listen for specific complaints, not general praise. \u0026ldquo;This is cool\u0026rdquo; is noise. \u0026ldquo;It always gets company names wrong\u0026rdquo; is signal.\nWhen to Kill an AI Feature This is the hardest part. You built something cool. It took engineering months. Leadership is invested. But nobody\u0026rsquo;s using it.\nKill signals: Adoption flatlines after launch. Initial spike, then nothing. Users tried it, it didn\u0026rsquo;t stick. Engagement is shallow. Users trigger the feature but don\u0026rsquo;t act on outputs. Support tickets mention it negatively. \u0026ldquo;How do I turn off the AI suggestions?\u0026rdquo; It\u0026rsquo;s slow and users work around it. They\u0026rsquo;ve learned to ignore it. Sunk cost fallacy is real. We killed an AI feature at Entropy Labs after 4 months of development because adoption was 2% and declining. It hurt. But maintaining unloved features is more expensive than admitting failure.\nThe reframe: you learned what doesn\u0026rsquo;t work. That\u0026rsquo;s valuable. Document it, share it, and don\u0026rsquo;t repeat it.\nAvoiding the AI Winter Within Your Product Here\u0026rsquo;s a pattern I\u0026rsquo;ve seen multiple times:\nCompany ships AI features with fanfare Features underperform expectations Leadership loses enthusiasm AI work gets deprioritized Good AI opportunities get lumped in with the failed ones This is the \u0026ldquo;AI winter\u0026rdquo; at company scale. How to avoid it:\nStart small and prove value. One well-executed smart default beats ten chatbot experiments.\nSet realistic expectations. AI features have lower accuracy than traditional software. If stakeholders expect 99.9% accuracy, they\u0026rsquo;ll be disappointed at 90% (which might be excellent for the use case).\nMeasure the right things. \u0026ldquo;Users tried the feature\u0026rdquo; is not success. \u0026ldquo;Users completed tasks faster\u0026rdquo; is success.\nBuild escape hatches. Every AI feature should have a way to fall back to manual. Users tolerate AI mistakes if they can easily correct them.\nThe Bottom Line The best AI product thinking isn\u0026rsquo;t \u0026ldquo;where can we add AI?\u0026rdquo; It\u0026rsquo;s \u0026ldquo;where are users doing tedious cognitive work that we could automate?\u0026rdquo;\nThat reframe changes everything:\nInstead of chatbots, you build smart defaults Instead of generic generation, you build contextual assistance Instead of features that demo well, you build features that ship well The AI hype cycle rewards announcements. Product thinking rewards sustained value. They\u0026rsquo;re often in tension.\nBuild for the latter.\n","permalink":"https://mhassan.dev/blog/ai-features-users-want/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003cp\u003eThe graveyard of failed AI features is full of chatbots nobody asked for.\u003c/p\u003e\n\u003cp\u003eEvery product team I talk to has the same story: leadership watched a GPT demo, got excited, and mandated \u0026ldquo;we need AI in the product.\u0026rdquo; Three months later, there\u0026rsquo;s a chatbot in the corner of the app that 3% of users have tried and 0.5% use regularly.\u003c/p\u003e\n\u003cp\u003eAs CPO at Entropy Labs, I\u0026rsquo;ve been on both sides of this. I\u0026rsquo;ve built AI features that users loved and killed features that seemed brilliant in demos but died in production. Here\u0026rsquo;s what I\u0026rsquo;ve learned about the difference.\u003c/p\u003e","title":"AI Features Your Users Actually Want (Hint: Not Another Chatbot)"},{"content":" Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, \u0026ldquo;Cool, I\u0026rsquo;ll ship this by Friday.\u0026rdquo; Three weeks later, you\u0026rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.\nI\u0026rsquo;ve shipped LangChain-based features to production at multiple companies. Here\u0026rsquo;s what I wish someone had told me before I started.\nWhen to Use LangChain (And When Not To) Let\u0026rsquo;s start with the uncomfortable truth: you probably don\u0026rsquo;t need LangChain.\nLangChain is an abstraction layer. Abstractions are great when they simplify common patterns and terrible when they obscure what\u0026rsquo;s actually happening. For LangChain, it depends entirely on your use case.\nUse LangChain when: You\u0026rsquo;re building complex chains with multiple LLM calls, tools, and conditional logic You need observability and tracing (LangSmith integration is genuinely good) You\u0026rsquo;re prototyping rapidly and might switch LLM providers Your team is already familiar with the framework Skip LangChain when: You\u0026rsquo;re making simple API calls to one model You need fine-grained control over request/response handling Your use case doesn\u0026rsquo;t fit LangChain\u0026rsquo;s mental model Bundle size or cold start time matters (serverless) At Entropy Labs, we use a hybrid approach: LangChain for complex agentic workflows, raw SDK calls for simple completions. The overhead isn\u0026rsquo;t worth it for a straightforward \u0026ldquo;summarize this text\u0026rdquo; endpoint.\nLCEL: The Good Parts LangChain Expression Language (LCEL) was a massive improvement over the legacy chain syntax. Here\u0026rsquo;s a pattern that actually works well in production:\nfrom langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_anthropic import ChatAnthropic from langchain_core.runnables import RunnablePassthrough # Clean, composable chain prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;You are a technical writer. Be concise.\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) model = ChatAnthropic( model=\u0026#34;claude-sonnet-4-20250514\u0026#34;, max_tokens=1024, timeout=30.0, # Always set timeouts ) chain = ( {\u0026#34;input\u0026#34;: RunnablePassthrough()} | prompt | model | StrOutputParser() ) # With retry logic from langchain_core.runnables import RunnableRetry robust_chain = chain.with_retry( stop_after_attempt=3, wait_exponential_jitter=True ) The pipe syntax makes composition clear. You can see data flow. That\u0026rsquo;s the good part.\nStreaming that actually works async def stream_response(query: str): async for chunk in chain.astream(query): yield chunk Simple, clean, no surprises. Until you add memory.\nThe Problems Nobody Warns You About 1. Memory management is a minefield LangChain\u0026rsquo;s conversation memory abstractions look elegant in docs. In production, they\u0026rsquo;re a footgun.\n# This looks innocent from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() chain = ConversationChain(llm=llm, memory=memory) Problems:\nMemory is stored in-process by default. Restart your server? Gone. No TTL. Chat histories grow unbounded. The memory object isn\u0026rsquo;t thread-safe. Concurrent requests? Corruption. What we actually use:\nfrom langchain_community.chat_message_histories import RedisChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory def get_session_history(session_id: str): return RedisChatMessageHistory( session_id, url=settings.REDIS_URL, ttl=3600 # 1 hour TTL ) chain_with_history = RunnableWithMessageHistory( chain, get_session_history, input_messages_key=\u0026#34;input\u0026#34;, history_messages_key=\u0026#34;history\u0026#34;, ) Redis handles persistence, TTL, and concurrency. LangChain\u0026rsquo;s memory abstractions are just wrappers.\n2. Silent failures everywhere This one cost me 8 hours of debugging:\n# Looks fine, right? result = await chain.ainvoke({\u0026#34;query\u0026#34;: user_input}) The chain returned an empty string. No error. No exception. Nothing in logs.\nThe cause? A malformed prompt template that resulted in an empty message list. The LLM received nothing, returned nothing. LangChain happily passed it through.\nAlways validate chain outputs:\nresult = await chain.ainvoke({\u0026#34;query\u0026#34;: user_input}) if not result or not result.strip(): logger.error(f\u0026#34;Empty response for query: {user_input[:100]}\u0026#34;) raise ValueError(\u0026#34;LLM returned empty response\u0026#34;) 3. Version churn is exhausting LangChain\u0026rsquo;s API changes frequently. Code that worked in 0.1.x might not compile in 0.2.x. Import paths move. Classes get renamed.\n# v0.1.x from langchain.chat_models import ChatAnthropic # v0.2.x from langchain_anthropic import ChatAnthropic # v0.3.x # Who knows? Check the migration guide. Pin your versions aggressively:\n# pyproject.toml langchain = \u0026#34;==0.2.14\u0026#34; langchain-core = \u0026#34;==0.2.33\u0026#34; langchain-anthropic = \u0026#34;==0.1.23\u0026#34; And read the changelogs before upgrading.\nCost Tracking and Observability If you\u0026rsquo;re not tracking costs, you\u0026rsquo;re flying blind. LangSmith is the easiest path:\nimport os os.environ[\u0026#34;LANGCHAIN_TRACING_V2\u0026#34;] = \u0026#34;true\u0026#34; os.environ[\u0026#34;LANGCHAIN_API_KEY\u0026#34;] = \u0026#34;your-key\u0026#34; os.environ[\u0026#34;LANGCHAIN_PROJECT\u0026#34;] = \u0026#34;production\u0026#34; Every chain execution gets traced. You see latency, token counts, and costs. The callback system also lets you build custom tracking:\nfrom langchain_core.callbacks import BaseCallbackHandler from typing import Any class CostTracker(BaseCallbackHandler): def __init__(self): self.total_tokens = 0 self.total_cost = 0.0 def on_llm_end(self, response: Any, **kwargs): usage = response.llm_output.get(\u0026#34;token_usage\u0026#34;, {}) input_tokens = usage.get(\u0026#34;prompt_tokens\u0026#34;, 0) output_tokens = usage.get(\u0026#34;completion_tokens\u0026#34;, 0) # Claude Sonnet pricing (example) cost = (input_tokens * 0.003 + output_tokens * 0.015) / 1000 self.total_cost += cost logger.info(f\u0026#34;LLM call cost: ${cost:.4f}\u0026#34;) At Entropy Labs, we alert when daily spend exceeds thresholds. One runaway loop can burn through hundreds of dollars.\nAlternatives and When to Use Them LlamaIndex for pure RAG If your use case is \u0026ldquo;query documents and return answers,\u0026rdquo; LlamaIndex is more focused. Less abstraction, more batteries included for retrieval.\nDirect SDK calls For simple use cases, the Anthropic/OpenAI SDKs are cleaner:\nfrom anthropic import Anthropic client = Anthropic() response = client.messages.create( model=\u0026#34;claude-sonnet-4-20250514\u0026#34;, max_tokens=1024, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query}] ) No framework, no magic, full control.\nHaystack If you need more structure than raw SDKs but less opinion than LangChain, Haystack hits a middle ground. Worth evaluating for production RAG pipelines.\nMy Production Stack Here\u0026rsquo;s what I actually deploy:\nSimple completions: Anthropic SDK directly Complex chains: LangChain + LCEL Retrieval: LlamaIndex or custom (depending on scale) Memory: Redis with manual management Observability: LangSmith + custom Prometheus metrics Rate limiting: Redis-based token bucket Caching: Response caching for deterministic queries The theme: use LangChain where it adds value, bypass it where it adds complexity.\nThe Bottom Line LangChain is a powerful framework with rough edges. The tutorials show the happy path; production is everything else.\nBefore adopting it:\nUnderstand what abstraction you\u0026rsquo;re buying and what control you\u0026rsquo;re giving up Set up observability from day one Plan for version upgrades (they\u0026rsquo;re frequent and breaking) Build escape hatches for when the framework fights you The best LangChain code I\u0026rsquo;ve written is the code that uses it sparingly—for the problems it solves well, not for everything.\n","permalink":"https://mhassan.dev/blog/langchain-production/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003cp\u003eEvery LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, \u0026ldquo;Cool, I\u0026rsquo;ll ship this by Friday.\u0026rdquo; Three weeks later, you\u0026rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve shipped LangChain-based features to production at multiple companies. Here\u0026rsquo;s what I wish someone had told me before I started.\u003c/p\u003e","title":"LangChain in Production: What the Tutorials Don't Tell You"},{"content":" I\u0026rsquo;ve been skeptical of every \u0026ldquo;game-changing AI release\u0026rdquo; for the past two years. Every few months, a new model drops and Twitter explodes with claims that AGI is here. Spoiler: it never is. But when Anthropic released Opus 4.5, something actually shifted in how I work. Not because it\u0026rsquo;s AGI—it\u0026rsquo;s decidedly not—but because it\u0026rsquo;s the first model that consistently delivers on complex, multi-step reasoning without falling apart halfway through.\nThis isn\u0026rsquo;t a hype piece. This is a practitioner\u0026rsquo;s field notes from someone who uses these tools daily to ship product at Entropy Labs.\nWhat Makes Opus 4.5 Different The headline feature is extended thinking—the model\u0026rsquo;s ability to spend more compute on harder problems before responding. But that undersells what\u0026rsquo;s actually happening.\nMost models optimize for fast, plausible responses. Opus 4.5 optimizes for correct responses, even if it takes longer. The difference becomes obvious when you\u0026rsquo;re debugging a gnarly race condition or trying to understand why your database query is slow despite having all the \u0026ldquo;right\u0026rdquo; indexes.\nHere\u0026rsquo;s what I noticed after a month of daily use:\n1. It actually reads the context you give it. Previous models would skim your codebase dump and give generic advice. Opus 4.5 will reference specific lines, notice inconsistencies between files, and ask clarifying questions that show it understood the architecture.\n2. It reasons through edge cases unprompted. Ask it to implement a feature, and it\u0026rsquo;ll often say \u0026ldquo;but what about X scenario?\u0026rdquo; before you\u0026rsquo;ve thought of it yourself. This used to be GPT-4\u0026rsquo;s strength; Opus does it better.\n3. It admits uncertainty differently. Instead of confident hallucinations, it\u0026rsquo;ll say \u0026ldquo;I\u0026rsquo;m not certain about the API behavior here—let me reason through what the docs suggest.\u0026rdquo; That hedging has saved me hours of debugging bad suggestions.\nThe Art of Context Control Here\u0026rsquo;s the thing most people get wrong: Opus 4.5 isn\u0026rsquo;t magic. It\u0026rsquo;s a tool that scales with how well you wield it. The unlock isn\u0026rsquo;t the model itself—it\u0026rsquo;s understanding that you need to provide context, not instructions.\nWhat doesn\u0026rsquo;t work \u0026#34;Write me a function to handle user authentication\u0026#34; This gives you generic boilerplate. The model has no idea about your stack, your security requirements, or your existing patterns.\nWhat works \u0026#34;Here\u0026#39;s our current auth setup [paste relevant files]. We\u0026#39;re using Django with DRF, JWT tokens stored in httpOnly cookies, and we have a custom User model. I need to add support for API key authentication for programmatic access. Our existing pattern for middleware is in auth/middleware.py.\u0026#34; The model now understands constraints. It\u0026rsquo;ll generate code that fits your patterns, not generic Stack Overflow answers.\nAt Entropy Labs, I\u0026rsquo;ve started maintaining a CLAUDE.md file in every project—a living document that describes the architecture, conventions, and gotchas. When I start a Claude Code session, that context loads automatically. The quality difference is night and day.\nThe context hierarchy Project-level context: Architecture docs, conventions, tech stack Session-level context: What you\u0026rsquo;re trying to accomplish, relevant files Query-level context: Specific question with enough detail to answer precisely Most people only provide #3 and wonder why responses are generic.\nWhen It Fails (Yes, It Does) Let\u0026rsquo;s be real: Opus 4.5 still hallucinates. Here are failure patterns I\u0026rsquo;ve observed:\nPackage versions and APIs: It confidently suggested using a deprecated Anthropic API three times in one session. When I gave it the actual docs, it apologized and fixed it. Always verify against current documentation.\nComplex async flows: Give it a sufficiently tangled async/await scenario with multiple event loops, and it can lose track. It\u0026rsquo;s better than previous models, but not foolproof.\nDomain-specific knowledge cutoffs: Its training data has a cutoff. Ask about a library released last month, and it\u0026rsquo;ll make educated guesses that are sometimes wrong.\nCost: Extended thinking uses more tokens. A complex debugging session can easily burn through $10-20 in API credits. At Entropy Labs, we\u0026rsquo;ve implemented caching for common queries to manage costs.\nThe pattern I\u0026rsquo;ve learned: trust but verify. Use Opus for the heavy lifting of understanding codebases, generating first drafts, and reasoning through architecture. But run the code, check the docs, and test the edge cases.\nPractical Setup with Claude Code Here\u0026rsquo;s my actual workflow:\n1. Install Claude Code CLI brew install claude-code claude auth login 2. Create your project context Create a CLAUDE.md in your project root:\n# Project: Entropy Dashboard ## Stack - Backend: Django 5.x, DRF, Celery - Frontend: Vue 3, Pinia, TailwindCSS - DB: PostgreSQL 16, Redis - Infra: AWS ECS, RDS, ElastiCache ## Conventions - Use class-based views for DRF - Pinia stores follow the composition API pattern - All API endpoints versioned under /api/v1/ - Tests use pytest with factory_boy ## Current Pain Points - N+1 queries in /api/v1/reports/ endpoint - WebSocket reconnection logic is flaky 3. Use it conversationally Don\u0026rsquo;t one-shot complex tasks. Build up context through conversation:\n\u0026#34;I\u0026#39;m debugging slow report generation. Can you read the reports/views.py and reports/serializers.py to understand the current implementation?\u0026#34; [Claude reads and summarizes] \u0026#34;Okay, now let\u0026#39;s profile this. What queries should I look for in django-debug-toolbar?\u0026#34; [Discussion continues, building shared understanding] This back-and-forth is where Opus shines. It maintains context across a long session better than any model I\u0026rsquo;ve used.\nThe Bottom Line Opus 4.5 isn\u0026rsquo;t AGI. It\u0026rsquo;s not going to replace engineers. But it\u0026rsquo;s the first AI tool that consistently makes me faster at complex tasks rather than faster at generating code I need to rewrite.\nThe real unlock is treating it as a collaborator that needs explicit context, not a magic box that reads your mind. Give it the context. Let it reason. Verify the output. Iterate.\nIf you\u0026rsquo;re still prompting AI like it\u0026rsquo;s a search engine, you\u0026rsquo;re missing the point. These models reward investment in context the same way a good colleague rewards clear communication.\nNow if only it could attend my standup meetings for me.\n","permalink":"https://mhassan.dev/blog/claude-opus-45/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003cp\u003eI\u0026rsquo;ve been skeptical of every \u0026ldquo;game-changing AI release\u0026rdquo; for the past two years. Every few months, a new model drops and Twitter explodes with claims that AGI is here. Spoiler: it never is. But when Anthropic released Opus 4.5, something actually shifted in how I work. Not because it\u0026rsquo;s AGI—it\u0026rsquo;s decidedly not—but because it\u0026rsquo;s the first model that consistently delivers on complex, multi-step reasoning without falling apart halfway through.\u003c/p\u003e","title":"Claude Opus 4.5: When an AI Finally Gets It"},{"content":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how Generative AI can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give reliable, context-aware answers based only on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets: Information Overload: Manually searching large documents is time-consuming and inefficient. Generic LLM Limitations: Large Language Models (LLMs) are powerful, but they lack specific, up-to-date knowledge about your documents unless explicitly trained on them (which is often impractical). Hallucination Risk: When asked about information outside their training data, LLMs might confidently invent answers that sound plausible but are incorrect. This is unacceptable for reliable FAQ systems. Inconsistent Outputs: Getting answers in a usable, predictable format can be challenging with free-form text generation. We need a system that answers questions accurately based only on a given set of documents and provides answers in a consistent, structured way. The Solution: RAG + Gemini API Our approach combines Retrieval Augmented Generation (RAG) with the capabilities of the Gemini API. At a high level, the user interacts with the system like this: Figure 1: High-Level RAG Interaction Flow. This involves three main steps in the underlying RAG pipeline: 1. Indexing: Convert the source documents (Google Car manuals) into numerical representations (embeddings) using the Gemini text-embedding-004 model and store them in a vector database (ChromaDB). This allows for efficient similarity searches. This setup process is crucial for enabling fast retrieval later. Figure 2: The Document Indexing Flow. 2. Retrieval: When a user asks a question, embed the question using the same model and search the vector database to find the most relevant document chunks based on semantic similarity. Figure 3: The Query Retrieval Flow. 3. Generation: Pass the original question and the retrieved document chunks as context to a powerful LLM (like gemini-2.0-flash). Instruct the model to answer the question based only on the provided context. Alongside the RAG structure, we leverage specific Gemini API Features: High-Quality Embeddings: text-embedding-004 provides embeddings suitable for finding semantically similar text. Powerful Generation: gemini-2.0-flash can synthesize answers based on the retrieved context. Structured Output (JSON Mode): We instruct Gemini to return the answer and a confidence score in a predictable JSON format, making it easy for applications to use the output. Optional Grounding: We can even add Google Search as a tool if the local documents don't suffice (though our primary goal here is document-based Q\u0026A). Implementation Highlights 1. Custom Embedding Function for ChromaDB:\nWe need to tell ChromaDB how to generate embeddings using the Gemini API. chromadb import Documents, EmbeddingFunction, Embeddings from google.api_core import retry from google import genai from google.genai import types is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503}) class GeminiEmbeddingFunction(EmbeddingFunction): document_mode = True # Toggle between indexing docs and embedding queries @retry.Retry(predicate=is_retriable) def __call__(self, input_texts: Documents) -\u0026gt; Embeddings: task = \u0026#34;retrieval_document\u0026#34; if self.document_mode else \u0026#34;retrieval_query\u0026#34; print(f\u0026#34;Embedding {\u0026#39;documents\u0026#39; if self.document_mode else \u0026#39;query\u0026#39;} ({len(input_texts)})...\u0026#34;) try: # Assuming \u0026#39;client\u0026#39; is initialized Google GenAI client response = client.models.embed_content( model=\u0026#34;models/text-embedding-004\u0026#34;, contents=input_texts, config=types.EmbedContentConfig(task_type=task), # Specify task type ) return [e.values for e in response.embeddings] except Exception as e: print(f\u0026#34;Error during embedding: {e}\u0026#34;) return [[] for _ in input_texts] \u0026lt;/div\u0026gt; 2. Setting up ChromaDB and Indexing:\nWe create a ChromaDB collection and add our documents. get_or_create_collection makes this idempotent.\n# --- 5. Setup ChromaDB Vector Store --- import chromadb import time print(\u0026#34;Setting up ChromaDB...\u0026#34;) DB_NAME = \u0026#34;googlecar_faq_db\u0026#34; embed_fn = GeminiEmbeddingFunction() chroma_client = chromadb.Client() # In-memory client try: db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn) print(f\u0026#34;Collection \u0026#39;{DB_NAME}\u0026#39; ready. Current count: {db.count()}\u0026#34;) # Assuming \u0026#39;documents\u0026#39; and \u0026#39;doc_ids\u0026#39; are defined earlier if db.count() \u0026lt; len(documents): print(f\u0026#34;Adding/Updating documents in \u0026#39;{DB_NAME}\u0026#39;...\u0026#34;) embed_fn.document_mode = True # Set mode for indexing db.upsert(documents=documents, ids=doc_ids) # Use upsert for safety time.sleep(2) # Allow indexing to settle print(f\u0026#34;Documents added/updated. New count: {db.count()}\u0026#34;) else: print(\u0026#34;Documents already seem to be indexed.\u0026#34;) except Exception as e: print(f\u0026#34;Error setting up ChromaDB collection: {e}\u0026#34;) raise SystemExit(\u0026#34;ChromaDB setup failed. Exiting.\u0026#34;) 3. Retrieving Relevant Documents:\nThis function takes the user query, embeds it (using document_mode=False), and searches ChromaDB.\n# --- 6. Define Retrieval Function --- def retrieve_documents(query: str, n_results: int = 1) -\u0026gt; list[str]: print(f\u0026#34;\\nRetrieving documents for query: \u0026#39;{query}\u0026#39;\u0026#34;) embed_fn.document_mode = False # Switch to query mode try: results = db.query(query_texts=[query], n_results=n_results) if results and results.get(\u0026#34;documents\u0026#34;): retrieved_docs = results[\u0026#34;documents\u0026#34;][0] print(f\u0026#34;Retrieved {len(retrieved_docs)} documents.\u0026#34;) return retrieved_docs else: print(\u0026#34;No documents retrieved.\u0026#34;) return [] except Exception as e: print(f\u0026#34;Error querying ChromaDB: {e}\u0026#34;) return [] 4. Generating the Structured Answer:\nHere's the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.\n# --- 7. Define Structured Output Schema --- from typing_extensions import Literal from pydantic import BaseModel class AnswerWithConfidence(BaseModel): answer: str confidence: Literal[\u0026#34;High\u0026#34;, \u0026#34;Medium\u0026#34;, \u0026#34;Low\u0026#34;] # --- 8. Define Augmented Generation Function --- def generate_structured_answer(query: str, context_docs: list[str]) -\u0026gt; dict | None: if not context_docs: print(\u0026#34;No context provided, cannot generate answer.\u0026#34;) return { \u0026#34;answer\u0026#34;: \u0026#34;I couldn\u0026#39;t find relevant information in the provided documents to answer this question.\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;Low\u0026#34;, } context = \u0026#34;\\n---\\n\u0026#34;.join(context_docs) prompt = f\\\u0026#34;\\\u0026#34;\\\u0026#34;You are an AI assistant answering questions about a Google car based ONLY on the provided documents. Context Documents: --- {context} --- Question: {query} Based *only* on the information in the context documents above, answer the question. Also, assess your confidence in the answer based *only* on the provided text: - \u0026#34;High\u0026#34; if the answer is directly and clearly stated in the documents. - \u0026#34;Medium\u0026#34; if the answer can be inferred but isn\u0026#39;t explicitly stated. - \u0026#34;Low\u0026#34; if the documents don\u0026#39;t seem to contain the answer or are ambiguous. Return your response ONLY as a JSON object with the keys \u0026#34;answer\u0026#34; and \u0026#34;confidence\u0026#34;. Example format: { \u0026#34;answer\u0026#34;: \u0026#34;Your answer here.\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;High/Medium/Low\u0026#34; } \\\u0026#34;\\\u0026#34;\\\u0026#34; try: generation_config = types.GenerateContentConfig( temperature=0.2, response_mime_type=\u0026#34;application/json\u0026#34;, # Request JSON response_schema=AnswerWithConfidence, # Provide the schema ) # Assuming \u0026#39;client\u0026#39; is initialized Google GenAI client response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=prompt, generation_config=generation_config, # Pass the config object ) # Safe access to parsed output if ( response.candidates and response.candidates[0].content and response.candidates[0].content.parts ): parsed_output = response.candidates[0].content.parts[0].function_call # Fallback check if .parsed is used if not parsed_output and hasattr( response.candidates[0].content.parts[0], \u0026#34;parsed\u0026#34; ): parsed_output = response.candidates[0].content.parts[0].parsed if isinstance(parsed_output, dict) and \u0026#34;answer\u0026#34; in parsed_output and \u0026#34;confidence\u0026#34; in parsed_output: print(\u0026#34;Generated Answer:\u0026#34;, parsed_output) return parsed_output else: print(\u0026#34;Warning: Could not extract valid JSON from response.\u0026#34;) print(\u0026#34;Raw response part:\u0026#34;, response.candidates[0].content.parts[0]) # Attempt to parse the text part if it exists and looks like JSON try: import json text_part = response.candidates[0].content.parts[0].text if text_part and text_part.strip().startswith(\u0026#34;{\u0026#34;) and text_part.strip().endswith(\u0026#34;}\u0026#34;): parsed_json = json.loads(text_part) if isinstance(parsed_json, dict) and \u0026#34;answer\u0026#34; in parsed_json and \u0026#34;confidence\u0026#34; in parsed_json: print(\u0026#34;Recovered JSON from text part:\u0026#34;, parsed_json) return parsed_json except Exception as json_e: print(f\u0026#34;Could not parse text part as JSON: {json_e}\u0026#34;) print(\u0026#34;Error: Could not generate/parse structured response correctly.\u0026#34;) return {\u0026#34;answer\u0026#34;: \u0026#34;Error: Could not generate or parse the structured response from the AI.\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;Low\u0026#34;} except Exception as e: print(f\u0026#34;Error during content generation call: {e}\u0026#34;) return {\u0026#34;answer\u0026#34;: f\u0026#34;Error during generation API call: {e}\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;Low\u0026#34;} Tip: Ensure your API key is correctly set up in Kaggle Secrets (GOOGLE_API_KEY). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity). Limitations and Future Work This implementation is a great starting point, but it has limitations:\nDocument Quality: The RAG system's effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out. Retrieval Accuracy: Simple similarity search might not always retrieve the perfect chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this. Structured Output Failures: While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added. Limited Context Handling (within LLM): While RAG provides context, the LLM itself still has limits on how much context it can process effectively in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM. Static Knowledge: The bot only knows what's in the ChromaDB index. It doesn't learn automatically. Updates require re-indexing. Future Enhancements: Implement Google Search grounding as a fallback when confidence is low or documents are missing. Add conversation memory for multi-turn interactions. Explore more sophisticated retrieval techniques. Build a simple UI (e.g., using Gradio or Streamlit). Fine-tune an embedding model specifically for the car manual domain (though text-embedding-004 is quite capable). Conclusion Building this FAQ bot demonstrates how combining RAG with Gemini's embedding and generation capabilities, especially its structured output mode, can create powerful and reliable AI-driven Q\u0026A systems. By grounding the LLM's responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience. Key Takeaways:\nRAG grounds LLM answers in your specific data. Gemini Embeddings + ChromaDB enable efficient document retrieval. Structured Output (JSON Mode) enhances reliability and integrability. Confidence Scores add a layer of trustworthiness. This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search. I hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to ask questions or leave a comment with your thoughts or own implementations!\n","permalink":"https://mhassan.dev/blog/google-rag-smart-faq/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n  \u003ch2\u003e\u003cspan style=\"color:#FFB4A2\"\u003eIntroduction\u003c/span\u003e\u003c/h2\u003e\n  \u003cdiv style=\"text-align: justify;\"\u003e\n    If you've ever found yourself digging through product manuals, company\n    wikis, or lengthy documents just to find a simple answer, you know the pain.\n    The fact you're reading this suggests you're interested in how\n    \u003cstrong\u003eGenerative AI\u003c/strong\u003e can make that process less painful. Stick\n    around for a few minutes, and I'll walk you through how we built a smarter\n    FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and\n    structured output. This isn't just another chatbot; it's designed to give\n    \u003cstrong\u003ereliable, context-aware answers\u003c/strong\u003e based \u003cem\u003eonly\u003c/em\u003e on\n    provided information, minimizing the risk of making things up\n    (hallucination). This example uses Google Car manuals, but the principles\n    apply anywhere you have a set of documents you need to query effectively.\n    I'm sharing my journey building this; it's a practical demonstration, not a\n    definitive guide, so adapt the ideas to your needs!\n  \u003c/div\u003e\n  \u003chr /\u003e\n  \u003ch2\u003e\n    \u003cspan style=\"color:#FFB4A2\"\n      \u003eThe Problem: Dumb Bots and Information Overload\u003c/span\n    \u003e\n  \u003c/h2\u003e\n  \u003cp\u003e\n    Traditional search methods or basic chatbots often fall short when dealing\n    with specific document sets:\n  \u003c/p\u003e","title":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"},{"content":" Explore My Resources Here, you'll find a curated collection of my past writings, book recommendations, and other useful content. Whether you're looking for in-depth articles or insightful reads, this page serves as a gateway to valuable knowledge. My Blog Dive into my blog, where I share my thoughts on technology, software engineering, productivity, and personal growth. Explore a wide range of topics and discover new perspectives on the ever-evolving world of tech. Visit Blog\nBook Recommendations I’ve compiled a list of books that have significantly impacted my thinking and professional journey. These books cover a range of topics including software engineering, business, productivity, and personal growth. Explore Books\nArchive Looking for older articles? My archive contains all my past writings on technology, business, and problem-solving. Browse through different categories and find content that resonates with your interests. View Archive\n","permalink":"https://mhassan.dev/resources/","summary":"\u003cdiv class=\"resources-container\"\u003e\n\u003ch2 id=\"explore-my-resources\"\u003eExplore My Resources\u003c/h2\u003e\n\u003cp class=\"justified-text\"\u003e\nHere, you'll find a curated collection of my past writings, book recommendations, and other useful content. Whether you're looking for in-depth articles or insightful reads, this page serves as a gateway to valuable knowledge.\n\u003c/p\u003e\n\u003ch3 id=\"my-blog\"\u003eMy Blog\u003c/h3\u003e\n\u003cp class=\"justified-text\"\u003e\nDive into my blog, where I share my thoughts on technology, software engineering, productivity, and personal growth. Explore a wide range of topics and discover new perspectives on the ever-evolving world of tech.\n\u003c/p\u003e","title":"Resources"},{"content":" Introduction The fact that you have decided to read this behemoth of an article deserves admiration and tells me that you're serious about your academics and career (or are procrastinating on something else). Give me the next 20 mins of your life and I'll make you into a much more informed individual. Your Final Year Project (FYP) is one of the most important academic tasks in your degree. It can shape your future career, boost your portfolio, and improve your problem-solving skills. This guide will help you choose the right topic, advisor, tech stack, and strategy to ensure your FYP stands out. This guide is mainly targets FASTians because of my experience, but the advice can be applied to any university. Also, I claim to be no expert in this field. I'm just a student who's been through the process and wants to help others navigate it. So take my very opinionated advice with a grain of salt. Research \u0026amp; Development vs. Development When planning your FYP, it’s essential to decide which type of project best aligns with your interests, skills, and career goals. Generally, there are two broad categories:\nResearch \u0026amp; Development (R\u0026amp;D) Projects: These combine a theoretical research component with a practical implementation aspect. They often involve exploring new concepts, conducting literature reviews, and testing hypotheses alongside building a functional prototype. Pure Development Projects: These focus mainly on building a fully functional product or system. The emphasis here is on applying existing technologies and methodologies to create a tangible solution, often with less emphasis on novel research. Below is a comparison of the pros and cons of each approach:\nAspect R\u0026amp;D Projects Pure Development Focus Research + prototype Build a complete product Innovation Novel contributions, possible publications Applies proven methods Time \u0026amp; Risk Longer, uncertain outcomes Predictable, lower risk Career Fit Academia, research roles Industry, hands-on coding Documentation Research-heavy reports Technical \u0026amp; project docs Tip: Choose a Research \u0026amp; Development project if you’re passionate about exploring new ideas and contributing to academic knowledge—even if it means facing higher uncertainty. Opt for a Pure Development project if you prefer focusing on practical application and building a market-ready product while showcasing your technical skills to future employers.\nHow to Choose a Topic Picking the right topic is critical for a successful FYP. A good topic should be:\nInteresting: Choose something you\u0026rsquo;re passionate about. No one wants to work on a boring project, let alone for a whole year. Feasible: Consider your time, skills, and available resources. Scope is one of the most important factors when it comes to your FYP. It will dictate your life for the next year. And what\u0026rsquo;s more? It will be the first thing your advisor(s) and the evaluation committee will ask you about. Relevant: Align with industry trends or research needs. As an example, I am an avid \u0026ldquo;AI\u0026rdquo; pessimist but I can\u0026rsquo;t deny the fact that it\u0026rsquo;s the hottest trend in the industry right now. So, if you\u0026rsquo;re into AI, you might want to consider a project in that domain. Look at the recent explosion of AI Agents and Generative AI. I have many concerns about the ethical implications of these technologies but that\u0026rsquo;s a topic for another day. You want a skillset that\u0026rsquo;s in demand, or at the very least will allow you to have a good conversation with a potential employer. However, this is not to say that you should pick the next hottest thing and chase after it, because, by the time you\u0026rsquo;re done with your project, it might not be the hottest thing anymore. You should pick something that you\u0026rsquo;re passionate about and what aligns with your academic and career goals. Problem-Solving: Your project should address a real problem or offer innovation. Please don\u0026rsquo;t just build another e-commerce website or a basic recommendation system. Building a novel project will not only make you stand out but also give you a sense of accomplishment (and probably a few sleepless nights but you\u0026rsquo;re a FASTian, it\u0026rsquo;s your life anyway). Tip: Browse research papers, GitHub projects, and tech blogs for inspiration. Look at what the students from foreign universities are doing. They\u0026rsquo;re not necessarily smarter than you but they have access to better resources and they\u0026rsquo;re more likely to be working on cutting-edge projects. You don\u0026rsquo;t have to copy them, but you should never give up on the opportunity to learn from them. A good way to shortlist a project is to do a preliminary literature review. This will give you an idea of what\u0026rsquo;s already been done and what hasn\u0026rsquo;t. Don\u0026rsquo;t wanna jump into an overcrowded room now, do you?\nHow to Choose an Advisor Your advisor plays a huge role in your project\u0026rsquo;s success. A good advisor should be:\nExperienced in your chosen field: If your idea is novel enough and complex enough, you might want to consider an advisor who has experience in that domain. They can guide you better and help you avoid common pitfalls. A good example of this is the HODs who excel in their respective fields. They\u0026rsquo;re usually very busy but if you can get them to agree to be your advisor, you\u0026rsquo;re in for a treat. However, many many other faculty members who are just as good (if not better, considering they have more time to dedicate to you) and you should consider them as well. Responsive and willing to guide you properly: This is a big one. You don\u0026rsquo;t want an advisor who\u0026rsquo;s too busy to meet with you or doesn\u0026rsquo;t respond to your emails. And let me be clear, this isn\u0026rsquo;t a slight against any of the faculty members. They\u0026rsquo;re all extremely helpful and knowledgeable. But some of them are just too busy to give you the attention you need. And here is where clear communication with your potential advisor comes in. You need to be upfront about your expectations and ask them about theirs. So the advice I give is to inquire about the advisor’s current project load and typical response times. Supportive of your ideas but also critical when needed: Everyone loves a chill time but you shouldn\u0026rsquo;t try to chill with your FYP. You need someone who can guide you properly and point out the flaws in your project. You need someone who can push you to do better. You need someone who can be your mentor. And you need someone who can be your friend. And you need someone who can be your critic. And you need someone who can be your cheerleader. That was a lot of \u0026ldquo;ands\u0026rdquo; but you get the point. FAST is filled with faculty members who can be all of these things and more. You just need to find the right one for you. Co-Advisor: If possible, consider having a co-advisor. This can help you get different perspectives and more support. This is especially useful if you\u0026rsquo;re working on a project that requires expertise in multiple domains. For example, if you\u0026rsquo;re working on a project that involves both AI and Web Development, you might want to consider having an AI expert as your primary advisor and a Web Development expert as your co-advisor. This way, you get the best of both worlds. External Advisor: If you can get an external advisor, that\u0026rsquo;s even better. Don\u0026rsquo;t know where to find them? Your contact should be your team leads or managers at the companies you interned at. If you build a trustworthy and professional relationship with them, they might be willing to help you out. They can provide you with real-world insights and help you align your project with industry needs. They can also help you with your job search after graduation. So if you\u0026rsquo;re smart, it always comes down to networking and building relationships. If you provide value to others, they\u0026rsquo;ll reciprocate (mostly). How to find one?\nCheck faculty profiles and see their published work. Ask seniors about their experiences with different advisors. This is the most important one. Seniors can give you the best advice on who to choose as your advisor. They\u0026rsquo;ve been through the process and they know who\u0026rsquo;s good and who\u0026rsquo;s not. But still, they can be biased towards or against certain faculty members so take their advice with a grain of salt. Maybe the advisor who was too strict for them is the perfect fit for you. Approach multiple advisors with a short, well-prepared proposal. Get a printed copy, please. It\u0026rsquo;s a small thing but it shows that you\u0026rsquo;re serious about your project. And it\u0026rsquo;s a good habit to get into. You should always have a printed copy of your proposal, your project plan, your project report, your project presentation, your project defence, your project\u0026hellip; you get the point. Choosing the Right Tech Stack Your tech stack should be practical and aligned with your project\u0026rsquo;s scope. Consider:\nIndustry Demand: Is the tech widely used (e.g., Python, React, Django)? Community Support and Libraries: Are there resources available for learning and troubleshooting? Don\u0026rsquo;t pick an obscure tool where you\u0026rsquo;re stuck bashing your head against a wall because there\u0026rsquo;s no documentation. You want to pick a tool that has a large community behind it. That way, if you run into any issues, you can just Google it and you\u0026rsquo;ll find a solution. Scalability: Can your tech handle future growth? Let me be clear, not all projects need to be scalable. If you\u0026rsquo;re building a project that\u0026rsquo;s meant to be used by a small group of people, scalability might not be a concern for you. But if you\u0026rsquo;re building a project that\u0026rsquo;s meant to be used by a large group of people, scalability should be one of your top concerns. So please don\u0026rsquo;t over-engineer or under-engineer your project. I have a habit of designing projects (even hobby ones) that can scale to thousands of users. But that\u0026rsquo;s not always necessary. Learning Curve: Will you have time to master it? FASTians are built different. They can learn anything in a short amount of time. But that doesn\u0026rsquo;t mean you should try to learn everything. You should stick to what you know and what you can learn in a reasonable amount of time. You don\u0026rsquo;t want to be learning a new language or framework in the middle of your project. Imagine flying a plane with your right while holding the manual on your left. However, if you have some time before your FYP starts, you can use that time to learn a new language or framework. Example Tech Stacks:\nWeb Apps: Vue.js / Django / PostgreSQL. I love this stack. It\u0026rsquo;s simple, it\u0026rsquo;s powerful, and it\u0026rsquo;s easy to learn. AI/ML: Python (TensorFlow, PyTorch) + Flask / FastAPI Mobile Apps: Kotlin / Swift. I prefer native apps as compared to hybrid apps. But that\u0026rsquo;s just me. You might prefer hybrid apps and that\u0026rsquo;s perfectly fine. I want to squeeze every bit of performance out of my apps. But you might not care about that. Tip: Stick to technologies you are comfortable with or have enough time to learn.\nCan an FYP Help with Employment and Master\u0026rsquo;s Admissions? Absolutely! A well-executed FYP can:\nImpress recruiters: Companies prefer candidates who have built real-world projects that they can check out. Boost your resume \u0026amp; portfolio: Especially if it’s on GitHub with good documentation. code quality, a working demo, and a good commit history. Support your Master\u0026rsquo;s application: A strong project with a research component can help in admissions and scholarships. Tip: Try to publish a paper or present your project in a competition for extra credibility.\nHow to Write a Strong Proposal A solid proposal increases your chances of approval and sets a clear roadmap for your project. It should include:\nProject Title: Clear and concise. Problem Statement: What issue are you solving? Be specific about the challenges or gaps your project aims to address. Objectives: Define clear, measurable goals. Instead of vague aims like \u0026ldquo;I want to build a website,\u0026rdquo; specify \u0026ldquo;I want to build a website that allows users to register, search, and interact with community content.\u0026rdquo; Tech Stack: Justify your choice of tools. Explain why you selected one database over another, a specific language, or a particular framework. This demonstrates that you\u0026rsquo;ve thoroughly researched the technical needs of your project. Expected Outcomes: Describe the anticipated impact of your project. What problems will it solve, and how will it benefit users or contribute to your field? Methodology: Outline your approach—whether it’s a research-driven study, iterative software development, or an experimental design. This helps clarify how you\u0026rsquo;ll achieve your objectives. Literature Review: Provide a brief overview of existing work or research related to your topic. This not only justifies the novelty of your project but also shows that you’re building on a solid foundation. Timeline \u0026amp; Milestones: Include a realistic timeline with specific milestones. Breaking down your project into phases demonstrates that you have a clear plan for completing your work on schedule. Risk Management \u0026amp; Contingency Plans: Identify potential challenges and outline strategies to address them. Whether it’s technical hurdles or resource constraints, showing that you’ve thought ahead will impress evaluators. Resource \u0026amp; Budget Considerations (if applicable): Detail any additional resources or funding you may require, such as software licenses, hardware, or access to special datasets. Tip: Avoid overly ambitious goals—keep your proposal realistic and achievable. Remember, \u0026ldquo;Underpromise and overdeliver\u0026rdquo; is a solid strategy, but overpromising what you can achieve may lead to unnecessary stress and setbacks.\nProject Management Best Practices Managing your project efficiently ensures smooth progress.\nUse Agile/Scrum: Break work into small, manageable tasks. Version Control: Use Git/GitHub for tracking changes. For the love of all that\u0026rsquo;s holy, don\u0026rsquo;t send each other code files over WhatsApp. Use Git. It\u0026rsquo;s not that hard to learn and it will save you a lot of time and effort. Task Management: Use Trello, Notion, or Jira for planning. You can mostly get away with using Trello. It\u0026rsquo;s simple, it\u0026rsquo;s easy to use, and it\u0026rsquo;s free. But if you want more features, you can use Notion or Jira. Code Documentation: Write clean, well-commented code for maintainability. Look into the tools that can help you with this. For example, you can use JSDoc for JavaScript, Sphinx for Python, and Doxygen for C++. Usecase-based development: Develop your project based on the use cases divided amongst each member. Don\u0026rsquo;t commit the cardinal sin of assigning a whole layer to a single member (my X member did the frontend). This is a recipe for disaster. Tip: Meet with your team/advisor weekly to track progress.\nTesting \u0026amp; Deployment Strategies Testing is often overlooked but is critical to ensuring your project’s success. Implement a comprehensive testing strategy that includes:\nUnit Testing: Test individual components to ensure they work as expected. Integration Testing: Verify that different modules interact correctly. Performance Testing: Assess scalability and speed; explore techniques like skeleton and lazy loading for optimization. Security Testing: Ensure your application is robust against vulnerabilities, especially if it handles sensitive data. Usability Testing: Gather user feedback to ensure the project is intuitive and user-friendly. For deployment:\nReliable Platforms: Choose trusted platforms like AWS, Firebase, Heroku, or DigitalOcean. For a more hands-on approach, consider setting up your own VPS. Automated Testing \u0026amp; Deployment: Leverage CI/CD pipelines to automate the testing and deployment process. This reduces manual errors and ensures consistent releases. Backup \u0026amp; Rollback Plans: Always have contingency measures in place. In case of deployment issues, a well-documented rollback plan can save valuable time and prevent data loss. Tip: Document any issues encountered during testing and deployment, and note how you resolved them. This documentation can be invaluable during your project defence and for future projects.\nPreparing for Your FYP Defense Your FYP defence is where you showcase your hard work. A great presentation includes:\nClear Project Demo: Focus on the key features and functionality of your project. Prepare backup materials like screenshots or a recorded demo in case of technical glitches. Performance Insights: Present efficiency improvements, benchmarking data, or any performance metrics that validate your project’s success. Comprehensive Understanding: Ensure every team member is well-versed in all aspects of the project. This prepares you for both technical and theoretical questions from the panel. Presentation Design \u0026amp; Narrative: Craft a clear, concise, and visually engaging presentation. A strong narrative that explains the problem, your approach, and the impact of your project can make a lasting impression. Handling Q\u0026amp;A: Anticipate common questions and practice your responses. Prepare to discuss challenges, lessons learned, and possible future enhancements. Tip: Practice your defense multiple times before presenting. This practice will help you remain calm and confident, even when facing unexpected questions. Make sure all the members know all the details of the project. You don\u0026rsquo;t want to be caught off guard by a question you should know the answer to.\nCompany Sponsorships and Mentorship Programs Looking beyond traditional academic support can add a real-world edge to your FYP. Consider these strategies:\nInternship Sponsorship: If you\u0026rsquo;ve interned at a company, approach them with your project proposal. They might be interested in sponsoring your project, providing financial support and ensuring your work aligns with industry needs. This is how it worked for me and how I got my external advisor. Professor Connections: Many professors have strong links with software houses. Occasionally, they secure projects from these companies that can be developed as part of a student\u0026rsquo;s FYP. It\u0026rsquo;s worth asking your advisor if such opportunities exist. Industry Mentorship Programs: In Pakistan, several software houses now run FYP mentorship programs. These initiatives connect students with experienced industry professionals who can offer guidance, technical insights, and real-world perspectives throughout the project. LinkedIn Networking: Reach out to professionals in your field on LinkedIn. Many are willing to mentor students and provide valuable advice on project development, tech stack selection, and career opportunities. This way, even if you can\u0026rsquo;t become a part of a mentorship program, you can still get the benefits of having a mentor. Many professionals are willing to help students. You just need to ask. Tip: Leverage your network and inquire with both your past internship supervisors and professors about these opportunities—they could open doors to valuable industry partnerships.\nConclusion Your FYP is more than just an academic requirement—it’s a launchpad for your career.\nChoose the right topic and tech stack. Work with a supportive advisor. Follow best coding \u0026amp; project management practices. Use it to boost your resume and career prospects. Final Tip: Start early, be consistent, and seek feedback from peers and advisors.\nI hope this guide helps you navigate your Final Year Project with confidence. If you have questions, feel free to reach out or leave a comment!\n","permalink":"https://mhassan.dev/blog/fyp-guidance/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cspan style=\"color:#FFB4A2\"\u003eIntroduction\u003c/span\u003e\u003c/h2\u003e\n\u003cdiv style=\"text-align: justify;\"\u003e\nThe fact that you have decided to read this behemoth of an article deserves admiration and tells me that you're serious about your academics and career (or are procrastinating on something else). Give me the next 20 mins of your life and I'll make you into a much more informed individual. Your \u003cstrong\u003eFinal Year Project (FYP)\u003c/strong\u003e is one of the most important academic tasks in your degree. It can shape your future career, boost your portfolio, and improve your problem-solving skills.  \nThis guide will help you \u003cstrong\u003echoose the right topic, advisor, tech stack, and strategy\u003c/strong\u003e to ensure your FYP stands out. This guide is mainly targets FASTians because of my experience, but the advice can be applied to any university. Also, I claim to be no expert in this field. I'm just a student who's been through the process and wants to help others navigate it. So take my very opinionated advice with a grain of salt.\n\u003c/div\u003e\n\u003chr\u003e\n\u003ch2 id=\"research--development-vs-development\"\u003e\u003cspan style=\"color:#FFB4A2\"\u003eResearch \u0026amp; Development vs. Development\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eWhen planning your FYP, it’s essential to decide which type of project best aligns with your interests, skills, and career goals. Generally, there are two broad categories:\u003c/p\u003e","title":"Final Year Project (FYP) Guide for Students"},{"content":"Introduction As my POS system scaled, performance bottlenecks became increasingly apparent. Customers began complaining about slow bill generation times, which made checkout frustratingly sluggish. After profiling my Django APIs, I discovered that inefficient ORM queries were causing unnecessary database overhead, leading to significant slowdowns. This prompted a deep dive into ORM optimizations to reduce query execution time and improve the overall user experience.\nIn this post, I\u0026rsquo;ll share how I optimized my Django ORM queries using select_related, prefetch_related, bulk operations, and query profiling tools to enhance the efficiency of my refund API and bill generation process.\nThe N+1 Query Problem and Its Impact One major issue I faced was the N+1 query problem, which happens when querying related objects in a loop, leading to excessive database hits.\nFor example, in my refund API:\nfor item_data in refund_items_data: bill_item = BillItem.objects.get(id=item_data[\u0026#34;bill_item_id\u0026#34;]) product = bill_item.product # This triggers a separate query each time Since each iteration fetched a related product separately, it resulted in multiple queries—one for each bill_item. When processing large refunds, this approach led to severe slowdowns.\nSolution: Using select_related for ForeignKey Joins Replacing individual lookups with select_related drastically reduced query counts by using SQL joins to fetch related data in a single query:\nfor item_data in refund_items_data: bill_item = BillItem.objects.select_related(\u0026#39;product\u0026#39;).get(id=item_data[\u0026#34;bill_item_id\u0026#34;]) product = bill_item.product # Now fetched in the same query This improved performance by minimizing redundant database hits.\nOptimizing Many-to-Many Queries with prefetch_related Another issue arose when fetching refund items along with related bill_items and products. Since Django’s ORM performs multiple queries for Many-to-Many relationships, prefetch_related was a better choice than select_related.\nrefund_items = RefundItem.objects.prefetch_related(\u0026#39;bill_item__product\u0026#39;).filter(refund=refund) This preloads related objects efficiently, reducing database queries and improving response times.\nWhen to Use Raw SQL vs. Django ORM Django ORM is powerful, but sometimes raw SQL is necessary for complex aggregations. For instance, if I needed to sum refunded amounts efficiently, a raw SQL query would outperform multiple ORM calls:\nfrom django.db import connection def get_total_refunded_amount(refund_id): with connection.cursor() as cursor: cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT SUM(refunded_amount) FROM refund_item WHERE refund_id = %s \u0026#34;\u0026#34;\u0026#34;, [refund_id]) return cursor.fetchone()[0] This direct approach was faster than filtering and aggregating with Django ORM in certain scenarios.\nEfficient Bulk Inserts and Updates Handling refunds involved updating multiple products at once. Initially, I updated each product inside a loop, leading to excessive database writes:\nfor product in products_to_update: product.save() Switching to bulk_update allowed batch updates, significantly improving performance:\nProduct.objects.bulk_update(products_to_update, [\u0026#34;quantity_units\u0026#34;, \u0026#34;quantity_subunits\u0026#34;]) Similarly, bulk_create was useful for inserting multiple refund items efficiently:\nRefundItem.objects.bulk_create(refund_items) Profiling Queries with Django Debug Toolbar and Silk To identify bottlenecks, I used Django Debug Toolbar and Silk:\nDjango Debug Toolbar\nInstalled via: pip install django-debug-toolbar Added to INSTALLED_APPS and middleware, it revealed query counts and execution times. Silk\nInstalled via: pip install django-silk Enabled query logging, helping me pinpoint slow database operations. These tools were invaluable in detecting inefficient queries and refining my ORM usage.\nResults and Performance Gains After implementing these optimizations, the refund API saw a 70% reduction in query execution time, significantly improving bill generation speed. Customers immediately noticed smoother transactions, and complaints about slow processing vanished.\nConclusion Optimizing Django ORM queries is crucial for scaling large applications. By leveraging select_related, prefetch_related, bulk operations, and profiling tools, I was able to fine-tune my APIs for high performance. If you\u0026rsquo;re experiencing slow database operations, consider these strategies to improve efficiency and responsiveness in your Django projects. These techniques ended up saving my business from potential losses (customer were fuming a little ngl) due to poor user experience, and I\u0026rsquo;m confident they can help you too.\n","permalink":"https://mhassan.dev/blog/django-orm-optimizaitons/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs my POS system scaled, performance bottlenecks became increasingly apparent. Customers began complaining about slow bill generation times, which made checkout frustratingly sluggish. After profiling my Django APIs, I discovered that inefficient ORM queries were causing unnecessary database overhead, leading to significant slowdowns. This prompted a deep dive into ORM optimizations to reduce query execution time and improve the overall user experience.\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll share how I optimized my Django ORM queries using \u003ccode\u003eselect_related\u003c/code\u003e, \u003ccode\u003eprefetch_related\u003c/code\u003e, bulk operations, and query profiling tools to enhance the efficiency of my refund API and bill generation process.\u003c/p\u003e","title":"Optimizing Django ORM Queries for Large Applications"},{"content":"Introduction During my university coursework, I developed a C++-based grayscale image editor capable of performing fundamental image processing tasks. This project was an exploration into file handling, image manipulation, and efficient data structures in C++.\nFeatures of the Image Editor The application supports:\nLoading and saving grayscale images in PGM format. Applying filters like mean and median filtering. Performing transformations such as flipping, rotating, and resizing. Combining images either side-by-side or top-to-bottom. Adjusting brightness and generating negative images. Core Implementation The backbone of the editor is the grayImage struct, which stores pixel data and provides functions for image operations. Here’s a snippet demonstrating how pixels are set and retrieved:\nunsigned short setPixel(unsigned short value, int r, int c) { if (r \u0026gt;= Rows || c \u0026gt;= Cols || r \u0026lt; 0 || c \u0026lt; 0) { return -1; } Image[r][c] = value; return value; } int getPixel(int r, int c) { if (r \u0026gt;= Rows || c \u0026gt;= Cols || r \u0026lt; 0 || c \u0026lt; 0) { return -1; } return Image[r][c]; } Loading and Saving Images The editor reads and writes images in PGM format. The load() and Save() functions handle file I/O:\nint load(string File_Name) { ifstream Input(File_Name.c_str()); if (!Input) { return 1; } string MagicNumber, comment; int columns, rows, MaxValue, currentValue; getline(Input, MagicNumber); getline(Input, comment); Input \u0026gt;\u0026gt; columns \u0026gt;\u0026gt; rows \u0026gt;\u0026gt; MaxValue; setRows(rows); setCols(columns); Maximum = MaxValue; for (int i = 0; i \u0026lt; Rows; i++) { for (int j = 0; j \u0026lt; Cols; j++) { Input \u0026gt;\u0026gt; currentValue; Image[i][j] = currentValue; } } Input.close(); Loaded = true; return 0; } Applying a Negative Filter One of the simplest transformations in image processing is creating a negative image, achieved using:\nvoid Negative(grayImage\u0026amp; Result) { for (int row = 0; row \u0026lt; Rows; row++) { for (int column = 0; column \u0026lt; Cols; column++) { Result.Image[row][column] = Maximum - Image[row][column]; } } Result.Rows = Rows; Result.Cols = Cols; Result.Maximum = Maximum; } Future Improvements While this project successfully implements several essential image processing functions, future improvements could include:\nAdding support for colored images (PPM format). Implementing more advanced filters (e.g., Gaussian blur, edge detection). Providing a GUI using a library like Qt or OpenCV. Conclusion This C++ image editor was a great learning experience in working with image data, file I/O, and algorithm optimization. It’s a stepping stone towards more advanced image processing applications.\nCheck out the full source code on my GitHub!\nHave feedback or suggestions? Drop a comment below!\n","permalink":"https://mhassan.dev/projects/pgm-editor/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eDuring my university coursework, I developed a C++-based grayscale image editor capable of performing fundamental image processing tasks. This project was an exploration into file handling, image manipulation, and efficient data structures in C++.\u003c/p\u003e\n\u003ch3 id=\"features-of-the-image-editor\"\u003eFeatures of the Image Editor\u003c/h3\u003e\n\u003cp\u003eThe application supports:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLoading and saving grayscale images in PGM format.\u003c/li\u003e\n\u003cli\u003eApplying filters like mean and median filtering.\u003c/li\u003e\n\u003cli\u003ePerforming transformations such as flipping, rotating, and resizing.\u003c/li\u003e\n\u003cli\u003eCombining images either side-by-side or top-to-bottom.\u003c/li\u003e\n\u003cli\u003eAdjusting brightness and generating negative images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"core-implementation\"\u003eCore Implementation\u003c/h3\u003e\n\u003cp\u003eThe backbone of the editor is the \u003ccode\u003egrayImage\u003c/code\u003e struct, which stores pixel data and provides functions for image operations. Here’s a snippet demonstrating how pixels are set and retrieved:\u003c/p\u003e","title":"Building a C++ Image Editor"},{"content":"Obfuscating Images with Django \u0026amp; Azure Cloud Storage Introduction For a recent project, I developed a Django-based web application that allows users to obfuscate images by embedding executable files within them. The processed images are then stored securely on Azure Blob Storage. This project blends Django’s powerful templating system with Azure’s cloud storage to offer a unique and secure way of handling sensitive data.\nFeatures Upload images and executable files via a Django form. Embed executables within images using a custom obfuscation technique. Store and retrieve obfuscated images on Azure Blob Storage. Deobfuscate images, extracting the original files. Secure file handling and cloud storage integration. Setting Up Azure Blob Storage in Django We start by configuring Azure Blob Storage in our Django project. Using dotenv, we securely load credentials from an .env file:\nfrom azure.storage.blob import BlobServiceClient from dotenv import load_dotenv import os # Load environment variables load_dotenv() AZURE_CONNECTION_STRING = os.getenv(\u0026#34;AZURE_CONNECTION_STRING\u0026#34;) CONTAINER_NAME = os.getenv(\u0026#34;CONTAINER_NAME\u0026#34;) # Initialize BlobServiceClient BLOB_SERVICE_CLIENT = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING) Uploading Files to Azure Blob Storage A helper function to upload files to Azure Blob Storage:\ndef upload_to_azure_blob(file, blob_name): try: container_client = BLOB_SERVICE_CLIENT.get_container_client(CONTAINER_NAME) blob_client = container_client.get_blob_client(blob=blob_name) blob_client.upload_blob(file, overwrite=True) print(f\u0026#34;File {blob_name} uploaded successfully!\u0026#34;) except Exception as e: print(f\u0026#34;Error uploading file: {e}\u0026#34;) Handling File Uploads in Django A Django view to handle user file uploads and obfuscation:\nfrom django.shortcuts import render, redirect from .forms import UploadFileForm def upload_file(request): if request.method == \u0026#34;POST\u0026#34;: form = UploadFileForm(request.POST, request.FILES) if form.is_valid(): image_file = request.FILES.get(\u0026#34;image\u0026#34;) executable_file = request.FILES.get(\u0026#34;executable\u0026#34;) upload_to_azure_blob(image_file, image_file.name) upload_to_azure_blob(executable_file, executable_file.name) obfuscate_images(image_file.name, executable_file.name) else: form = UploadFileForm() return render(request, \u0026#34;core/obfuscator.html\u0026#34;, {\u0026#34;form\u0026#34;: form}) Obfuscating Images We embed the executable file within the image using a delimiter, then upload the modified file back to Azure:\ndef obfuscate_images(image_blob_name, executable_blob_name): try: image_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, image_blob_name) executable_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, executable_blob_name) image_data = image_blob_client.download_blob().readall() executable_data = executable_blob_client.download_blob().readall() delimiter = b\u0026#34;---EXECUTABLE_BOUNDARY---\u0026#34; obfuscated_data = image_data + delimiter + executable_data obfuscated_blob_name = f\u0026#34;{image_blob_name}_OBFUSCATED.jpg\u0026#34; obfuscated_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, obfuscated_blob_name) obfuscated_blob_client.upload_blob(obfuscated_data, overwrite=True) print(f\u0026#34;File {obfuscated_blob_name} obfuscated and uploaded successfully!\u0026#34;) except Exception as e: print(f\u0026#34;Error obfuscating file: {e}\u0026#34;) Deobfuscating Images The function below extracts the original image and executable file from the obfuscated image:\ndef deobfuscate_images(obfuscated_blob_name): try: obfuscated_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, obfuscated_blob_name) obfuscated_data = obfuscated_blob_client.download_blob().readall() delimiter = b\u0026#34;---EXECUTABLE_BOUNDARY---\u0026#34; delimiter_index = obfuscated_data.find(delimiter) image_data = obfuscated_data[:delimiter_index] executable_data = obfuscated_data[delimiter_index + len(delimiter) :] image_name, image_extension = obfuscated_blob_name.split(\u0026#34;_OBFUSCATED.\u0026#34;) deobfuscated_image_blob_name = f\u0026#34;{image_name}.{image_extension}\u0026#34; deobfuscated_executable_blob_name = f\u0026#34;{image_name}_EXECUTABLE.exe\u0026#34; deobfuscated_image_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, deobfuscated_image_blob_name) deobfuscated_executable_blob_client = BLOB_SERVICE_CLIENT.get_blob_client(CONTAINER_NAME, deobfuscated_executable_blob_name) deobfuscated_image_blob_client.upload_blob(image_data, overwrite=True) deobfuscated_executable_blob_client.upload_blob(executable_data, overwrite=True) print(f\u0026#34;Deobfuscated files uploaded successfully!\u0026#34;) return image_data, executable_data except Exception as e: print(f\u0026#34;Error deobfuscating file: {e}\u0026#34;) Future Enhancements Adding user authentication to restrict access to uploaded files. Implementing additional security measures, such as file encryption. Developing a frontend UI with Django templates and JavaScript for a seamless user experience. Enhancing storage management with Azure lifecycle policies. Conclusion This project showcases how Django and Azure Blob Storage can be leveraged to create a secure and efficient image obfuscation system. By embedding executables within images, we can transmit sensitive data in a unique and obscured format while utilizing the power of cloud storage.\nCheck out the full source code on my GitHub!\nGot feedback or suggestions? Drop a comment below!\n","permalink":"https://mhassan.dev/projects/pixelcryptor/","summary":"\u003ch2 id=\"obfuscating-images-with-django--azure-cloud-storage\"\u003eObfuscating Images with Django \u0026amp; Azure Cloud Storage\u003c/h2\u003e\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eFor a recent project, I developed a Django-based web application that allows users to obfuscate images by embedding executable files within them. The processed images are then stored securely on Azure Blob Storage. This project blends Django’s powerful templating system with Azure’s cloud storage to offer a unique and secure way of handling sensitive data.\u003c/p\u003e\n\u003ch3 id=\"features\"\u003eFeatures\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUpload images and executable files via a Django form.\u003c/li\u003e\n\u003cli\u003eEmbed executables within images using a custom obfuscation technique.\u003c/li\u003e\n\u003cli\u003eStore and retrieve obfuscated images on Azure Blob Storage.\u003c/li\u003e\n\u003cli\u003eDeobfuscate images, extracting the original files.\u003c/li\u003e\n\u003cli\u003eSecure file handling and cloud storage integration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"setting-up-azure-blob-storage-in-django\"\u003eSetting Up Azure Blob Storage in Django\u003c/h3\u003e\n\u003cp\u003eWe start by configuring Azure Blob Storage in our Django project. Using \u003ccode\u003edotenv\u003c/code\u003e, we securely load credentials from an \u003ccode\u003e.env\u003c/code\u003e file:\u003c/p\u003e","title":"Obfuscating Images with Django \u0026 Azure Cloud Storage"},{"content":"Building a Smart Dustbin with IoT Introduction This project involves the development of a Smart Dustbin using Arduino, Python, Kotlin (Android App), and Azure IoT to monitor and manage waste levels in real-time. By integrating sensors, cloud computing, and mobile applications, this system enhances waste management efficiency.\nComponents \u0026amp; Technologies Used Arduino Uno \u0026amp; ESP8266 WiFi Module for sensor data collection. Ultrasonic Sensors to measure garbage levels. Servo Motor to control the bin lid. Python \u0026amp; Tkinter GUI for real-time monitoring. Kotlin Android App for mobile notifications. ThingSpeak \u0026amp; Azure IoT Hub for cloud-based monitoring. Hardware Implementation Arduino Code for Garbage Level Detection \u0026amp; Smart Lid Control The Arduino Uno, paired with an HC-SR04 ultrasonic sensor, measures the bin’s fill level. The servo motor automatically opens the lid when an object is detected nearby.\n#include \u0026lt;Servo.h\u0026gt; Servo s1; const int trigPin = 7; const int echoPin = 8; const int doorTrigPin = 11; const int doorEchoPin = 12; const int MAX_CAPACITY_CM = 22; const int DOOR_DISTANCE_THRESHOLD = 10; void setup() { Serial.begin(9600); pinMode(trigPin, OUTPUT); pinMode(echoPin, INPUT); pinMode(doorTrigPin, OUTPUT); pinMode(doorEchoPin, INPUT); s1.attach(10); } void loop() { long duration, cm; digitalWrite(trigPin, LOW); delayMicroseconds(2); digitalWrite(trigPin, HIGH); delayMicroseconds(10); digitalWrite(trigPin, LOW); duration = pulseIn(echoPin, HIGH); cm = duration / 29 / 2; Serial.println(cm); delay(100); } WiFi Connectivity with ESP8266 The ESP8266 module enables real-time data transmission to a laptop or cloud.\n#include \u0026lt;ESP8266WiFi.h\u0026gt; const char* ssid = \u0026#34;wifi_name\u0026#34;; const char* password = \u0026#34;12345678\u0026#34;; const char* serverIP = \u0026#34;laptop_wifi_adapter_IP\u0026#34;; const int serverPort = 1234; WiFiClient client; void setup() { Serial.begin(9600); WiFi.begin(ssid, password); } void loop() { if (Serial.available() \u0026gt; 0) { int distance = Serial.parseInt(); if (distance \u0026gt;= 0 \u0026amp;\u0026amp; distance \u0026lt;= 400) { sendToLaptop(distance); } } } void sendToLaptop(int distance) { if (!client.connected()) { if (!client.connect(serverIP, serverPort)) return; } client.println(distance); } Cloud Integration with ThingSpeak \u0026amp; Azure IoT Hub Sending Data to ThingSpeak Data is published to ThingSpeak via MQTT.\ndef send_to_thingspeak(data): topic = \u0026#34;channels/\u0026#34; + CHANNEL_ID + \u0026#34;/publish\u0026#34; payload = \u0026#34;field1=\u0026#34; + str(data) publish.single(topic, payload, hostname=MQTT_HOST, auth={\u0026#34;username\u0026#34;: MQTT_USERNAME, \u0026#34;password\u0026#34;: MQTT_PASSWORD}) Sending Data to Azure IoT Hub For scalable IoT integration, data is sent to Azure IoT Hub.\nfrom azure.iot.device.aio import IoTHubDeviceClient async def send_to_azure_iot_hub(data): device_client = IoTHubDeviceClient.create_from_connection_string(CONNECTION_STRING) await device_client.connect() await device_client.send_message(data) await device_client.disconnect() GUI for Real-Time Monitoring Tkinter-based Desktop App A Python Tkinter application provides a user-friendly interface to monitor bin levels.\nimport tkinter as tk root = tk.Tk() root.title(\u0026#34;Garbage Level Monitoring\u0026#34;) garbage_label = tk.Label(root, text=\u0026#34;Garbage Level: 0%\u0026#34;) garbage_label.pack() root.mainloop() Android App Development (Kotlin) The Kotlin-based Android app receives real-time bin status and sends notifications when the bin is full.\nfun checkGarbageLevel(level: Int) { if (level \u0026gt; 80) { Toast.makeText(this, \u0026#34;Garbage bin almost full!\u0026#34;, Toast.LENGTH_LONG).show() } } Future Enhancements AI-based Smart Sorting: Automatic waste segregation. Solar-Powered Bin: Sustainable energy source. GPS-enabled Tracking: Optimize garbage collection routes. Conclusion This smart dustbin project showcases the power of IoT, cloud computing, and mobile app integration in waste management. By leveraging Arduino, Python, Kotlin, and Azure IoT, we enhance urban sustainability.\nCheck out the full project on GitHub at Here, Here, and Here!\n","permalink":"https://mhassan.dev/projects/iot-garbage-monitoring/","summary":"\u003ch2 id=\"building-a-smart-dustbin-with-iot\"\u003eBuilding a Smart Dustbin with IoT\u003c/h2\u003e\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis project involves the development of a \u003cstrong\u003eSmart Dustbin\u003c/strong\u003e using \u003cstrong\u003eArduino, Python, Kotlin (Android App), and Azure IoT\u003c/strong\u003e to monitor and manage waste levels in real-time. By integrating sensors, cloud computing, and mobile applications, this system enhances waste management efficiency.\u003c/p\u003e\n\u003ch3 id=\"components--technologies-used\"\u003eComponents \u0026amp; Technologies Used\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eArduino Uno \u0026amp; ESP8266 WiFi Module\u003c/strong\u003e for sensor data collection.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUltrasonic Sensors\u003c/strong\u003e to measure garbage levels.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eServo Motor\u003c/strong\u003e to control the bin lid.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePython \u0026amp; Tkinter GUI\u003c/strong\u003e for real-time monitoring.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKotlin Android App\u003c/strong\u003e for mobile notifications.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThingSpeak \u0026amp; Azure IoT Hub\u003c/strong\u003e for cloud-based monitoring.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"hardware-implementation\"\u003eHardware Implementation\u003c/h2\u003e\n\u003ch3 id=\"arduino-code-for-garbage-level-detection--smart-lid-control\"\u003eArduino Code for Garbage Level Detection \u0026amp; Smart Lid Control\u003c/h3\u003e\n\u003cp\u003eThe Arduino Uno, paired with an \u003cstrong\u003eHC-SR04 ultrasonic sensor\u003c/strong\u003e, measures the bin’s fill level. The \u003cstrong\u003eservo motor\u003c/strong\u003e automatically opens the lid when an object is detected nearby.\u003c/p\u003e","title":"Smart Dustbin with IoT: Arduino, Python, Kotlin \u0026 Azure"},{"content":" The Sole Spokesman: Jinnah, the Muslim League, and the Demand for Pakistan Author: Ayesha Jalal\nA penetrating analysis of the role of a singular spokesperson in shaping public discourse. Examines Jinnah’s influence through political rhetoric and media engagement. The Making of Pakistan: A Study in Nationalism Author: K.K. Aziz\nA fascinating exploration of Pakistan’s creation filled with historical insights, newspaper excerpts, and personal interviews. Delivers nuanced perspectives on identity and nation‐building. ","permalink":"https://mhassan.dev/books/books-pakistan/","summary":"\u003cdiv class=\"book-container\"\u003e\n\u003ch3 id=\"the-sole-spokesman-jinnah-the-muslim-league-and-the-demand-for-pakistan\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Sole Spokesman: Jinnah, the Muslim League, and the Demand for Pakistan\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/sole.webp\" alt=\"The Sole Spokesman Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Ayesha Jalal\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA penetrating analysis of the role of a singular spokesperson in shaping public discourse. Examines Jinnah’s influence through political rhetoric and media engagement.\n\u003c/blockquote\u003e\n\u003ch3 id=\"the-making-of-pakistan-a-study-in-nationalism\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Making of Pakistan: A Study in Nationalism\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/makingofpakistan.webp\" alt=\"The Making of Pakistan Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: K.K. Aziz\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA fascinating exploration of Pakistan’s creation filled with historical insights, newspaper excerpts, and personal interviews. Delivers nuanced perspectives on identity and nation‐building.\n\u003c/blockquote\u003e\n\u003c/div\u003e","title":"Books on Pakistan"},{"content":" The Brothers Karamazov Author: Fyodor Dostoevsky\nA masterpiece that delves deep into human nature, morality, and faith. Features profound philosophical debates and unforgettable characters. The Sword of Kaigen Author: M.L. Wang\nA gripping tale of family, honor, and magic set in a war-torn world. Combines intense battle scenes with heartfelt familial bonds. The Will of the Many Author: James Islington\nAn epic blend of political intrigue and high fantasy that leaves you craving more—need part 2 yesterday. Mistborn Saga: Complete Trilogy Author: Brandon Sanderson\nPeak fiction right here. Masterfully blends intricate magic systems with dynamic character arcs—gourmet storytelling at its finest. Angels \u0026amp; Demons Author: Dan Brown\nA riveting adventure pitting ancient secrets against modern science. Fast-paced puzzles and historical enigmas keep you on your toes. 1984 Author: George Orwell\nA chilling dystopian vision of totalitarianism that remains alarmingly relevant. Its prophetic warnings spark ongoing debates about surveillance and freedom. Project Hail Mary Author: Andy Weir\nA thrilling sci-fi adventure that combines humor, heart, and hard science. A space odyssey that challenges the limits of scientific ingenuity and human spirit. ","permalink":"https://mhassan.dev/books/books-fiction/","summary":"\u003cdiv class=\"book-container\"\u003e\n\u003ch3 id=\"the-brothers-karamazov\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Brothers Karamazov\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/brotherskaramazov.webp\" alt=\"The Brothers Karamazov Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Fyodor Dostoevsky\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA masterpiece that delves deep into human nature, morality, and faith. Features profound philosophical debates and unforgettable characters.\n\u003c/blockquote\u003e\n\u003ch3 id=\"the-sword-of-kaigen\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Sword of Kaigen\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/swordofkaigen.webp\" alt=\"The Sword of Kaigen Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: M.L. Wang\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA gripping tale of family, honor, and magic set in a war-torn world. Combines intense battle scenes with heartfelt familial bonds.\n\u003c/blockquote\u003e\n\u003ch3 id=\"the-will-of-the-many\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Will of the Many\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/thewillofthemany.webp\" alt=\"The Will of the Many Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: James Islington\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nAn epic blend of political intrigue and high fantasy that leaves you craving more—need part 2 yesterday.\n\u003c/blockquote\u003e\n\u003ch3 id=\"mistborn-saga-complete-trilogy\"\u003e\u003cspan class=\"book-subtitle\"\u003eMistborn Saga: Complete Trilogy\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/mistborn.webp\" alt=\"Mistborn Saga Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Brandon Sanderson\u003c/p\u003e","title":"Fiction Books"},{"content":" Clash of Civilizations Author: Samuel P. Huntington\nA provocative thesis that reshaped global political discourse in the post–Cold War era. Examines cultural fault lines and their implications for international relations, despite its controversial tone. The Limits of Power Author: Andrew J. Bacevich\nA critical examination of the constraints on national power and the perils of overreach. Uses historical case studies to illustrate the dangers of unchecked military ambition. Grit: The Power of Passion and Perseverance Author: Angela Duckworth\nA groundbreaking exploration of how passion and perseverance drive success. Provides actionable insights on cultivating resilience in both personal and professional spheres. Thinking, Fast and Slow Author: Daniel Kahneman\nA Nobel laureate explaining why humans are gloriously irrational. Introduces System 1 and System 2 thinking—the fast, intuitive brain vs. the slow, analytical one. Essential for understanding cognitive biases, decision-making, and why users do what they do. Also makes you realize how often you're wrong about everything. ","permalink":"https://mhassan.dev/books/books-non-fiction/","summary":"\u003cdiv class=\"book-container\"\u003e\n\u003ch3 id=\"clash-of-civilizations\"\u003e\u003cspan class=\"book-subtitle\"\u003eClash of Civilizations\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/clash.webp\" alt=\"Clash of Civilizations Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Samuel P. Huntington\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA provocative thesis that reshaped global political discourse in the post–Cold War era. Examines cultural fault lines and their implications for international relations, despite its controversial tone.\n\u003c/blockquote\u003e\n\u003ch3 id=\"the-limits-of-power\"\u003e\u003cspan class=\"book-subtitle\"\u003eThe Limits of Power\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/limitofpower.webp\" alt=\"The Limits of Power Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Andrew J. Bacevich\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA critical examination of the constraints on national power and the perils of overreach. Uses historical case studies to illustrate the dangers of unchecked military ambition.\n\u003c/blockquote\u003e\n\u003ch3 id=\"grit-the-power-of-passion-and-perseverance\"\u003e\u003cspan class=\"book-subtitle\"\u003eGrit: The Power of Passion and Perseverance\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/gritpower.webp\" alt=\"Grit: The Power of Passion and Perseverance Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Angela Duckworth\u003c/p\u003e","title":"Non-Fiction Books"},{"content":" Database Internals: A Deep Dive into How Distributed Data Systems Work Author: Alex Petrov\nA comprehensive guide to the internal workings of distributed data systems. Covers storage engines, replication, and consensus algorithms like Raft and Paxos. A must-read for anyone working with databases. Since I'm big on databases, this was a no-brainer. Designing Data-Intensive Applications Author: Martin Kleppmann\nIt's all about data these days. It's the new elixir of life. Covers OLTP vs. OLAP, distributed databases, caching strategies, event-driven architectures, and stream processing. If you don't understand this stuff, you're missing out. Crafting Interpreters Author: Robert Nystrom\nMost don't care about how their code gets executed, but they should. This book takes you from lexing, parsing, and ASTs to building a full interpreter from scratch. Writing a line that looks like English and making it do things is pure magic. Every programmer should go through that euphoric feeling at least once. Clean Code: A Handbook of Agile Software Craftsmanship Author: Robert C. Martin\nA definitive guide to writing code that is both elegant and maintainable. Talks about meaningful naming, reducing dependencies, writing small functions, and avoiding unnecessary complexity. I will continue to write garbage code though. Design Patterns: Elements of Reusable Object-Oriented Software Authors: Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides\nThe daddy of all design pattern books. Introduces concepts like Singleton, Factory, Observer, Decorator, and Command patterns, which still dominate software architecture today. If you haven't read this, you’ve definitely used its patterns without realizing it. The Clean Coder: A Code of Conduct for Professional Programmers Author: Robert C. Martin\nA call to professional discipline and ethical behavior. Talks about saying \"no\" when needed, handling deadlines, and taking full responsibility for your work. More than just a book—it's a mindset shift. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow Author: Aurelien Geron\nThe Swiss Army knife for anyone serious about ML. Takes you from linear regression to transformers with code you can actually run. Less math-heavy academia, more real-world problem solving. If you work in AI and haven't read this, you're doing it wrong. The Staff Engineer\u0026rsquo;s Path Author: Tanya Reilly\nFinally, a book that explains what happens after senior engineer. Covers technical vision, navigating ambiguity, and leading without authority. Essential reading for anyone who wants to grow without becoming a full-time manager. The chapters on \"being glue\" hit different. Natural Language Processing with Transformers Authors: Lewis Tunstall, Leandro von Werra, Thomas Wolf\nWritten by the Hugging Face team themselves. Covers everything from attention mechanisms to fine-tuning LLMs. If you're building anything with transformers, this is your bible. The world runs on attention now—might as well understand it. ","permalink":"https://mhassan.dev/books/books-technical/","summary":"\u003cdiv class=\"book-container\"\u003e\n\u003ch3 id=\"database-internals-a-deep-dive-into-how-distributed-data-systems-work\"\u003e\u003cspan class=\"book-subtitle\"\u003eDatabase Internals: A Deep Dive into How Distributed Data Systems Work\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/designing-db-internals.webp\" alt=\"Database Internals Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Alex Petrov\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nA comprehensive guide to the internal workings of distributed data systems. Covers \u003cstrong\u003estorage engines, replication, and consensus algorithms\u003c/strong\u003e like \u003cstrong\u003eRaft and Paxos\u003c/strong\u003e. A must-read for anyone working with databases. Since I'm big on databases, this was a no-brainer.\n\u003c/blockquote\u003e\n\u003ch3 id=\"designing-data-intensive-applications\"\u003e\u003cspan class=\"book-subtitle\"\u003eDesigning Data-Intensive Applications\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/designing-data-intensive-applications.webp\" alt=\"Designing Data-Intensive Applications Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Martin Kleppmann\u003c/p\u003e\n\u003cblockquote class=\"review\"\u003e\nIt's all about data these days. It's the new \u003cstrong\u003eelixir of life\u003c/strong\u003e. Covers \u003cstrong\u003eOLTP vs. OLAP, distributed databases, caching strategies, event-driven architectures, and stream processing\u003c/strong\u003e. If you don't understand this stuff, you're missing out.\n\u003c/blockquote\u003e\n\u003ch3 id=\"crafting-interpreters\"\u003e\u003cspan class=\"book-subtitle\"\u003eCrafting Interpreters\u003c/span\u003e\u003c/h3\u003e\n\u003cdiv class=\"book-cover\"\u003e\n  \u003cimg src=\"/assets/crafting-interpreters.webp\" alt=\"Crafting Interpreters Cover\" width=\"200\" height=\"300\"\u003e\n\u003c/div\u003e\n\u003cp class=\"author\"\u003eAuthor: Robert Nystrom\u003c/p\u003e","title":"Technical Books"},{"content":"Introduction As a developer, having a portfolio website is crucial for showcasing skills, projects, and expertise. I wanted a fast, minimal, and easily maintainable site, so I chose Hugo with the PaperMod theme. Best of all, I leveraged free tools for deployment, analytics, and discussions. Here’s how I built my portfolio website, hosted on GitHub Pages with a Porkbun domain.\nHugo and PaperMod for a Minimalistic Look Hugo is a blazing-fast static site generator, perfect for a developer portfolio. I picked the PaperMod theme because of its:\nClean and professional design. Lightweight and fast performance. Easy configuration for dark mode, search, and social links. Free Hosting with GitHub Pages I wanted a zero-cost, hassle-free hosting solution, and GitHub Pages was the perfect choice. Here’s what I did:\nCreated a public GitHub repository. Configured a GitHub Actions workflow for automatic deployment. Set the gh-pages branch as the deployment source in GitHub Pages settings. Now, every push to my main branch automatically updates my live website.\nFree Analytics with Umami Google Analytics is powerful, but I preferred something privacy-friendly and lightweight. Umami provides:\nSelf-hosted analytics with no third-party tracking. Simple dashboard with essential metrics. No cookies or GDPR concerns. I set up Umami on a free hosting platform and integrated it with my site by adding the tracking script to head.html in Hugo’s layout.\nFree Comments with Giscus I wanted a distraction-free, GitHub-powered commenting system. Giscus was the perfect choice because:\nIt uses GitHub Discussions, so no extra accounts are needed. It’s ad-free and requires no backend setup. Users can comment with their GitHub accounts, keeping discussions relevant. Setting it up was simple:\nInstalled the Giscus GitHub app on my repository. Added the \u0026lt;script\u0026gt; to my site’s config.toml file. Enabled discussions in my GitHub repository settings. Now, visitors can leave comments using GitHub, and I manage them just like regular issues.\nCustom Domain with Porkbun A great domain adds credibility to a portfolio. I registered mhassan.dev on Porkbun, and here’s why it was a great choice:\nAffordable pricing compared to other registrars. Free WHOIS privacy protection. Easy DNS setup for GitHub Pages. Automatic SSL without extra configuration. After purchasing, I simply updated the CNAME record to point to GitHub Pages, and Porkbun handled the HTTPS setup seamlessly.\nFinal Thoughts Building a portfolio website doesn’t have to be expensive. By using Hugo, GitHub Pages, Giscus, Umami, and Porkbun, I got a fast, professional, and cost-effective site with minimal effort. If you’re planning to build your own, these tools are a great starting point!\nCheck out my portfolio at right here dummy, you\u0026rsquo;re on it already :wink:\n","permalink":"https://mhassan.dev/blog/free-website-hosting/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a developer, having a portfolio website is crucial for showcasing skills, projects, and expertise. I wanted a fast, minimal, and easily maintainable site, so I chose \u003cstrong\u003eHugo\u003c/strong\u003e with the \u003cstrong\u003ePaperMod\u003c/strong\u003e theme. Best of all, I leveraged free tools for deployment, analytics, and discussions. Here’s how I built my portfolio website, hosted on \u003cstrong\u003eGitHub Pages\u003c/strong\u003e with a \u003cstrong\u003ePorkbun\u003c/strong\u003e domain.\u003c/p\u003e\n\u003ch2 id=\"hugo-and-papermod-for-a-minimalistic-look\"\u003eHugo and PaperMod for a Minimalistic Look\u003c/h2\u003e\n\u003cp\u003eHugo is a \u003cstrong\u003eblazing-fast static site generator\u003c/strong\u003e, perfect for a developer portfolio. I picked the \u003cstrong\u003ePaperMod\u003c/strong\u003e theme because of its:\u003c/p\u003e","title":"How I Built My Portfolio Website with Hugo, GitHub Pages, and Free Tools"},{"content":"Introduction When dealing with financial transactions in Django applications, maintaining an accurate ledger is critical. However, inefficient signal handling can lead to performance bottlenecks. In this article, we\u0026rsquo;ll explore an optimized approach to recalculating ledger balances while ensuring minimal database impact.\nThe Problem A typical ledger system requires recalculating balances when transactions are inserted, updated, or deleted. Using Django signals, many implementations trigger redundant recalculations, causing excessive database queries and slowing down the application.\nCommon Issues with Signals: Unnecessary Queries: Each save or delete operation triggers a recalculation, even if no meaningful change occurs. Cascade Effects: Bulk operations lead to multiple redundant recalculations. Performance Overhead: Large transaction volumes cause significant slowdowns. Optimized Approach To address these inefficiencies, we introduce a flag-based recalculation strategy that minimizes unnecessary database interactions.\nStep 1: Adding a Recalculation Flag Instead of recalculating every time a transaction is saved, we introduce a _needs_recalc flag:\nfrom django.db import models from django.db.models.signals import post_save, post_delete from django.dispatch import receiver class Transaction(models.Model): amount = models.DecimalField(max_digits=10, decimal_places=2) ledger = models.ForeignKey(\u0026#39;Ledger\u0026#39;, on_delete=models.CASCADE) def save(self, *args, **kwargs): self.ledger._needs_recalc = True super().save(*args, **kwargs) @receiver(post_delete, sender=Transaction) def mark_ledger_for_recalc(sender, instance, **kwargs): instance.ledger._needs_recalc = True Step 2: Efficient Ledger Recalculation Recalculating balances should only occur when necessary, ideally after all related operations are complete:\nfrom django.db import transaction def recalculate_ledger_balances(): ledgers_to_update = Ledger.objects.filter(_needs_recalc=True) for ledger in ledgers_to_update: ledger.recalculate_balance() ledger._needs_recalc = False ledger.save(update_fields=[\u0026#39;balance\u0026#39;]) Step 3: Using Post-Transaction Hooks Using Django’s on_commit() ensures recalculations only happen after successful transactions:\nfrom django.db.transaction import on_commit @receiver(post_save, sender=Transaction) def trigger_ledger_recalc(sender, instance, **kwargs): on_commit(lambda: recalculate_ledger_balances()) Performance Gains With this approach, we achieve:\nReduced Queries: Recalculation happens once per affected ledger, not per transaction. Better Scalability: Batch updates improve efficiency. Consistent Data: Using on_commit() ensures recalculations only happen after successful writes. Conclusion By intelligently managing recalculations, we significantly improve the performance of our ledger system in Django applications. This approach ensures efficiency without sacrificing data accuracy, making it ideal for high-volume financial applications.\n","permalink":"https://mhassan.dev/blog/using-signals-optimally/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWhen dealing with financial transactions in Django applications, maintaining an accurate ledger is critical. However, inefficient signal handling can lead to performance bottlenecks. In this article, we\u0026rsquo;ll explore an optimized approach to recalculating ledger balances while ensuring minimal database impact.\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eA typical ledger system requires recalculating balances when transactions are inserted, updated, or deleted. Using Django signals, many implementations trigger redundant recalculations, causing excessive database queries and slowing down the application.\u003c/p\u003e","title":"Optimizing Django Signals for Efficient Ledger Recalculations"},{"content":"DMCA Notice We respect the intellectual property rights of others and expect our users to do the same. If you believe that content on our website infringes your copyright, please follow the procedure outlined below.\nFiling a DMCA Notice To file a notice of claimed infringement, please include the following information:\nIdentification of the copyrighted work you claim has been infringed. Identification of the material on our site that is claimed to be infringing and information reasonably sufficient to locate the material. Contact Information: Your name, mailing address, telephone number, and email address. A statement that you have a good faith belief that the disputed use is not authorized by the copyright owner, its agent, or the law. A statement, under penalty of perjury, that the information in your notice is accurate and that you are authorized to act on behalf of the copyright owner. Your electronic signature. How to Submit Your Notice Send your DMCA notice to the following email address: raihassanraza10@gmail.com. We will review your notice and take appropriate action in accordance with the Digital Millennium Copyright Act.\nCounter-Notification If you believe that your content was removed in error, you may submit a counter-notification. This must include your contact information and a statement under penalty of perjury that you have a good faith belief the content was removed as a result of mistake or misidentification.\nDisclaimer Please note that this DMCA Notice is provided for informational purposes only and does not constitute legal advice. We recommend consulting with a qualified attorney for any legal concerns.\nContact If you have any questions regarding our DMCA policy, please contact us at raihassanraza10@gmail.com.\nPSA: Again, I\u0026rsquo;m just adding this page for the sake of completeness. You should definitely consult with a legal professional to ensure your DMCA policy is compliant with the law. Also, don\u0026rsquo;t steal content. It\u0026rsquo;s not based at all.\n","permalink":"https://mhassan.dev/dmca/","summary":"\u003ch2 id=\"dmca-notice\"\u003eDMCA Notice\u003c/h2\u003e\n\u003cp\u003eWe respect the intellectual property rights of others and expect our users to do the same. If you believe that content on our website infringes your copyright, please follow the procedure outlined below.\u003c/p\u003e\n\u003ch2 id=\"filing-a-dmca-notice\"\u003eFiling a DMCA Notice\u003c/h2\u003e\n\u003cp\u003eTo file a notice of claimed infringement, please include the following information:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eIdentification of the copyrighted work\u003c/strong\u003e you claim has been infringed.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIdentification of the material\u003c/strong\u003e on our site that is claimed to be infringing and information reasonably sufficient to locate the material.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContact Information:\u003c/strong\u003e Your name, mailing address, telephone number, and email address.\u003c/li\u003e\n\u003cli\u003eA statement that you have a good faith belief that the disputed use is not authorized by the copyright owner, its agent, or the law.\u003c/li\u003e\n\u003cli\u003eA statement, under penalty of perjury, that the information in your notice is accurate and that you are authorized to act on behalf of the copyright owner.\u003c/li\u003e\n\u003cli\u003eYour electronic signature.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"how-to-submit-your-notice\"\u003eHow to Submit Your Notice\u003c/h2\u003e\n\u003cp\u003eSend your DMCA notice to the following email address: \u003ca href=\"mailto:raihassanraza10@gmail.com\"\u003eraihassanraza10@gmail.com\u003c/a\u003e. We will review your notice and take appropriate action in accordance with the Digital Millennium Copyright Act.\u003c/p\u003e","title":"DMCA Notice"},{"content":"Introduction Your privacy is important to us. This Privacy Policy explains how we collect, use, and safeguard your personal information when you visit our website. By using our site, you consent to the practices described in this policy.\nInformation We Collect Personal Information: We may collect your name, email address, and other contact details when you subscribe to our newsletter or contact us. Usage Data: We automatically collect information about how you interact with our website, including pages visited, time spent on pages, and browser details. How We Use Your Information We use your data to:\nImprove website functionality and user experience. Respond to inquiries and provide customer support. Send periodic updates or newsletters (if you have opted in). Data Protection We implement reasonable security measures to protect your personal information from unauthorized access, disclosure, or alteration. However, no data transmission over the internet is entirely secure, so please exercise caution when sharing sensitive information online.\nThird-Party Disclosure We do not sell or share your personal information with third parties, except as necessary to comply with legal obligations or protect our rights.\nCookies Our website uses cookies to enhance your experience. Cookies help us understand how visitors interact with our site so we can improve its functionality. You can control cookie settings through your browser.\nChanges to This Policy We may update this Privacy Policy periodically. Any changes will be posted on this page with an updated effective date. Please review this policy regularly.\nContact Us If you have any questions or concerns about this Privacy Policy, please contact us at raihassanraza10@gmail.com.\nPSA: I created this page quickly and didn\u0026rsquo;t care much about the content. You should definitely spend more time on your privacy policy tho :D\n","permalink":"https://mhassan.dev/privacy-policy/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYour privacy is important to us. This Privacy Policy explains how we collect, use, and safeguard your personal information when you visit our website. By using our site, you consent to the practices described in this policy.\u003c/p\u003e\n\u003ch2 id=\"information-we-collect\"\u003eInformation We Collect\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePersonal Information:\u003c/strong\u003e We may collect your name, email address, and other contact details when you subscribe to our newsletter or contact us.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUsage Data:\u003c/strong\u003e We automatically collect information about how you interact with our website, including pages visited, time spent on pages, and browser details.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"how-we-use-your-information\"\u003eHow We Use Your Information\u003c/h2\u003e\n\u003cp\u003eWe use your data to:\u003c/p\u003e","title":"Privacy Policy"},{"content":"Introduction In modern web development, especially with frameworks like VueJS, building reusable components is a best practice that significantly improves both development efficiency and user experience. When dealing with critical actions—such as password changes, data deletions, or any irreversible actions—maintaining consistency is key. Reusable components, like confirmation modals or standardized buttons, ensure that users have a predictable and secure interaction every time.\nBenefits of Reusable Components Reusable components offer several significant advantages:\nConsistency:\nStandardized UI elements, such as confirmation modals or password prompts, ensure that users are always presented with familiar interfaces when performing critical actions. This consistency reduces confusion and builds trust in your application.\nMaintainability:\nWhen a component is reused across multiple parts of an application, any necessary updates or bug fixes need to be implemented only once. This centralization simplifies maintenance and helps avoid discrepancies in the user experience.\nImproved Development Speed:\nOnce a reusable component is created, it can be easily integrated wherever needed. This reduces code duplication and speeds up the development process, allowing you to focus on new features rather than rewriting similar functionality.\nEnhanced Testing:\nWith a single, well-tested component in use throughout your application, you can be confident that critical user interactions work reliably every time, reducing the potential for errors.\nReusable Components in VueJS VueJS\u0026rsquo;s component-based architecture makes it ideal for creating modular, reusable UI elements. Consider the following example of a confirmation modal component:\nConfirmation Modal Component Example This component can be used anywhere you need to confirm a critical action. It accepts customizable properties for the title and message and emits events based on the user\u0026rsquo;s choice.\n\u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;modal\u0026#34; v-if=\u0026#34;visible\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;{{ title }}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{{ message }}\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;modal-actions\u0026#34;\u0026gt; \u0026lt;button @click=\u0026#34;confirmAction\u0026#34;\u0026gt;Confirm\u0026lt;/button\u0026gt; \u0026lt;button @click=\u0026#34;cancelAction\u0026#34;\u0026gt;Cancel\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default { props: { visible: { type: Boolean, default: false }, title: { type: String, default: \u0026#39;Confirm Action\u0026#39; }, message: { type: String, default: \u0026#39;Are you sure you want to proceed?\u0026#39; } }, methods: { confirmAction() { this.$emit(\u0026#39;confirmed\u0026#39;); }, cancelAction() { this.$emit(\u0026#39;cancelled\u0026#39;); } } }; \u0026lt;/script\u0026gt; \u0026lt;style scoped\u0026gt; .modal { position: fixed; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0, 0, 0, 0.5); display: flex; align-items: center; justify-content: center; } .modal-content { background: #fff; padding: 1.5rem; border-radius: 4px; width: 100%; max-width: 400px; } .modal-actions { display: flex; justify-content: flex-end; gap: 1rem; } \u0026lt;/style\u0026gt; Additional Reusable Components Password Confirmation Modals:\nSimilar to the confirmation modal above, a password confirmation modal can prompt users to re-enter their password before making critical changes, ensuring security and consistency.\nStandardized Button Components:\nCreating a reusable button component that encapsulates various states (default, hover, active, disabled) helps maintain a consistent look and feel across all critical actions.\nConclusion Building reusable components in VueJS not only streamlines the development process but also enhances the overall user experience. For critical actions—like confirming deletions or verifying passwords—a consistent UI is essential to reduce confusion and build user trust. By creating and maintaining reusable components for modals, buttons, and other interface elements, you ensure that your application remains both robust and user-friendly.\nAdopting a component-based approach in VueJS leads to faster development, easier maintenance, and a more consistent interface, all of which are critical for modern web applications. Consider integrating these practices into your next project to see the tangible benefits in both code quality and user satisfaction.\n","permalink":"https://mhassan.dev/blog/reusing-components-benefits/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn modern web development, especially with frameworks like VueJS, building reusable components is a best practice that significantly improves both development efficiency and user experience. When dealing with critical actions—such as password changes, data deletions, or any irreversible actions—maintaining consistency is key. Reusable components, like confirmation modals or standardized buttons, ensure that users have a predictable and secure interaction every time.\u003c/p\u003e\n\u003ch2 id=\"benefits-of-reusable-components\"\u003eBenefits of Reusable Components\u003c/h2\u003e\n\u003cp\u003eReusable components offer several significant advantages:\u003c/p\u003e","title":"The Importance of Reusable Components for Critical Actions in VueJS"},{"content":"Introduction In modern web applications, speed and responsiveness are essential for a smooth user experience. One effective strategy to achieve this is preloading data when a page loads. For example, on a generate invoice page, preloading product data upon mounting ensures that users can search through products without any noticeable delay. This article explains the benefits of preloading data and includes a practical code example.\nBenefits of Preloading Data Preloading data on page load offers several key advantages:\nFaster User Interactions:\nWith the data already available in memory, functions like search can filter through products immediately, without waiting for additional network requests.\nReduced Latency:\nEliminating extra API calls during user interactions minimizes delays, leading to a more responsive interface.\nImproved Application Performance:\nBy consolidating data retrieval to the initial page load, network usage is optimized and subsequent operations become faster.\nEnhanced User Experience:\nImmediate feedback during searches and other interactions creates a smoother workflow, increasing overall user satisfaction.\nCode Example Below is a sample code snippet demonstrating how to preload product data on page load and perform a local search on an invoice page. This example uses a setup function (common in Vue.js with the Composition API) to initialize the product store, load products on mount, and filter the products based on a search query.\nsetup() { const authStore = useAuthStore(); const productStore = useProductStore(); // Initialize the product store // Define reactive references for the search query and search results const searchQuery = ref(\u0026#39;\u0026#39;); const searchResults = ref([]); // Function to preload products when the page loads const loadProducts = async () =\u0026gt; { await productStore.fetchProducts(); // Fetch the products and store them }; // Function to search products locally based on the query const searchProducts = () =\u0026gt; { if (searchQuery.value.trim() === \u0026#39;\u0026#39;) { searchResults.value = []; return; } // Filter products using local data based on the search query searchResults.value = productStore.products.filter(product =\u0026gt; { const productName = product.name || \u0026#39;\u0026#39;; // Default to empty string if undefined const productDescription = product.description || \u0026#39;\u0026#39;; // Default to empty string if undefined return productName.toLowerCase().includes(searchQuery.value.toLowerCase()) || productDescription.toLowerCase().includes(searchQuery.value.toLowerCase()); }); }; // Preload products when the component is mounted onMounted(() =\u0026gt; { loadProducts(); }); return { authStore, productStore, searchQuery, searchResults, searchProducts }; }, Explanation of the Code Initialization:\nThe code initializes the authentication and product stores. Reactive references searchQuery and searchResults are declared to manage the user\u0026rsquo;s search input and the results, respectively.\nPreloading Data:\nThe loadProducts function asynchronously fetches products from the product store when the component mounts. This preloading ensures that all product data is available immediately after the page loads.\nLocal Search:\nThe searchProducts function performs local filtering on the preloaded product data. If the search query is empty, it resets the results. Otherwise, it filters the product list based on whether the product\u0026rsquo;s name or description contains the search query.\nonMounted Hook:\nThe onMounted lifecycle hook calls loadProducts, ensuring that the data is fetched as soon as the component is rendered.\nConclusion Preloading data on page load is a powerful technique to enhance performance in web applications. By loading the necessary data upfront, applications can deliver instant feedback for operations such as searches, resulting in a more responsive and efficient user experience. This strategy not only reduces network latency but also improves overall performance by allowing local data operations.\nImplementing such techniques in your projects, especially in critical areas like the generate invoice page, can lead to faster interactions and a more satisfying user experience. Consider adopting preloading methods in your future projects to optimize performance and streamline data access.\n","permalink":"https://mhassan.dev/blog/preloading-data-for-performance/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn modern web applications, speed and responsiveness are essential for a smooth user experience. One effective strategy to achieve this is preloading data when a page loads. For example, on a generate invoice page, preloading product data upon mounting ensures that users can search through products without any noticeable delay. This article explains the benefits of preloading data and includes a practical code example.\u003c/p\u003e\n\u003ch2 id=\"benefits-of-preloading-data\"\u003eBenefits of Preloading Data\u003c/h2\u003e\n\u003cp\u003ePreloading data on page load offers several key advantages:\u003c/p\u003e","title":"Advantages of Preloading Data on Page Load"},{"content":" Transform Your Business with Custom Software Solutions From point-of-sale systems to full-scale web applications, I bring your ideas to life.\nSchedule a Free Consultation 4.5+ Years Experience 13+ Projects 100% Client Satisfaction What I Bring to Your Project Point of Sale Expertise Creator of Polaris, delivering modern cloud-based solutions for retail, restaurants, and service industries. Seamless operations, powerful analytics, and scalable architecture.\nFull-Stack Development Building robust web applications using Vue.js, Django, and PostgreSQL. Specializing in high-performance, secure, and scalable software solutions tailored to business needs.\nBusiness Automation Streamlining operations through custom software solutions, API integrations, and intelligent workflow automation. Boost efficiency and reduce operational costs.\nE-Commerce Solutions Developing and optimizing e-commerce platforms for hardware, sanitary, and industrial equipment stores. Expertise in UI/UX design, payment integration, and inventory management.\nScalable API Development Designing and building efficient, RESTful APIs with Django and PostgreSQL, ensuring seamless communication between applications and third-party services.\nTechnical Consultation \u0026 Mentorship Providing guidance to businesses and aspiring developers, helping them make informed technical decisions, optimize architecture, and improve software development workflows.\nSchedule Your Free Consultation Choose a time that works for you to discuss:\nInitial project requirements and goals Technical architecture and solution design Polaris demos and customization options Timeline and implementation strategy Book Your Call Now\nWhat People Are Saying \"I highly recommend Hassan, whom I directly supervised during both the internship and our collaborative work on the Final Year Project (FYP). He consistently demonstrated exceptional work ethic, completing assignments ahead of schedule and showcasing a strong understanding of complex tasks. Beyond technical skills, he possesses excellent interpersonal skills, making him an invaluable team player. He consistently brought a positive attitude to the workplace, fostering a collaborative and productive environment. \u0026ldquo;\n- Muhammad Hamid Raza, Principle Architect @Arrivy\n\"Polaris transformed how we manage our retail stores. With automated inventory tracking and seamless invoicing, we have reduced errors and increased sales efficiency.\"\n- Muhammad Inam, Wholesale Business Manager \"As a wholesale business, managing suppliers and sales used to be a nightmare. Polaris streamlined everything, saving us hours every week.\"\n- Usman Ghany, Retail Store Owner Let\u0026rsquo;s Connect I\u0026rsquo;m always excited to hear about new projects and challenges. Reach out through any of these channels:\n📧 Email: raihassanraza10@gmail.com 🌐 Portfolio: mhassan.dev 💼 LinkedIn: linkedin.com/in/hassanraza22 📅 Calendar: Book a call Whether you need a custom software solution, want to implement Polaris, or just want to discuss potential collaborations, I\u0026rsquo;m here to help turn your vision into reality.\n","permalink":"https://mhassan.dev/book-a-call/","summary":"Schedule a consultation to discuss your next project, Polaris implementation, or custom software needs.","title":"Let's Build Something Amazing Together"},{"content":"Background Building a POS system that handles inventory, sales, and real-time reporting presented multiple performance bottlenecks. With a PostgreSQL database and a Django backend, early performance issues arose due to inefficient query patterns, redundant updates, and excessive database hits.\nInitial Problems 1. Slow Bulk Updates Updating multiple records individually in Django was inefficient. Consider this naïve approach:\nfor product in products: product.stock -= 1 product.save() This triggered a separate UPDATE query for each product, significantly slowing down batch operations.\n2. Signal Overhead Django signals were initially used to track stock changes, but they fired on every save, leading to unnecessary computations.\n@receiver(post_save, sender=Sale) def update_inventory(sender, instance, **kwargs): product = instance.product product.stock -= instance.quantity product.save() With high transaction volumes, this became a major bottleneck.\n3. Redundant Queries Certain parts of the system, such as product conversion rates (e.g., unit-based conversions), recalculated values every time instead of caching them.\ndef get_conversion_rate(product): return ConversionRate.objects.get(product=product).rate Optimizations Implemented 1. Using bulk_update for Efficiency Instead of saving each instance separately, Django’s bulk_update was used to update multiple rows efficiently.\nProduct.objects.bulk_update(products, [\u0026#39;stock\u0026#39;]) This reduced the number of queries from N to 1.\n2. Replacing Signals with Direct Updates Instead of relying on Django signals, updates were performed explicitly in views or services:\nSale.objects.create(product=product, quantity=5) Product.objects.filter(id=product.id).update(stock=F(\u0026#39;stock\u0026#39;) - 5) By using F expressions, updates were performed in a single SQL statement, improving efficiency.\n3. Caching Frequently Accessed Data For product conversion rates, a caching mechanism using Django’s built-in cache framework was introduced:\nfrom django.core.cache import cache def get_conversion_rate(product): cache_key = f\u0026#39;conversion_rate_{product.id}\u0026#39; rate = cache.get(cache_key) if rate is None: rate = ConversionRate.objects.get(product=product).rate cache.set(cache_key, rate, timeout=3600) # Cache for 1 hour return rate This reduced redundant database hits and significantly improved response times.\nResults Bulk updates reduced update time from seconds to milliseconds. Eliminating unnecessary signals lowered database write load. Caching reduced redundant queries, leading to faster response times in product-related calculations. Final Thoughts Performance tuning in Django requires a mix of bulk operations, query optimization, and caching. Understanding when to use Django’s ORM features efficiently can prevent unnecessary database load and keep the application responsive. This approach significantly improved the speed of a POS system handling thousands of transactions daily.\n","permalink":"https://mhassan.dev/blog/optimizing-django-performance/","summary":"Optimizing Django for a high-traffic POS system required bulk updates, caching, and efficient query strategies. This post documents the challenges and solutions.","title":"Optimizing Django Performance: Lessons from a POS System"},{"content":"","permalink":"https://mhassan.dev/clients/client1/","summary":"","title":"Client1"},{"content":"Key Features Minimalist and Fast: Developed with Hugo to ensure quick load times and an efficient design. Intuitive Navigation: Utilizes the clean layout of PaperMod for an optimal user experience. Integrated Blog and Projects Section: Effectively presents my work, including detailed project pages and blog posts. Reliable Deployment: Hosted on GitHub Pages for stable and cost-effective performance. Comments Integration with Gisqus: Provides a cost-effective commenting system that limits participation to GitHub users. Gisqus also offers improved moderation controls, reduced spam, and seamless GitHub integration for a more secure and streamlined user experience. Detailed Overview The portfolio website is designed with both form and function in mind. Its key components include:\nResponsive Design: The site adapts seamlessly across devices, ensuring a great viewing experience on desktops, tablets, and mobile devices. SEO Optimization: Built according to SEO best practices to improve search engine visibility. Customizable Layout: Easily modified to add new sections or features as my portfolio evolves. Performance Optimization: Static site generation with Hugo ensures high performance and minimal load times. Preview Below is a screenshot preview of the portfolio website:\nLive Demo To view the live version of the portfolio website, please visit\u0026hellip;.. just kidding. You\u0026rsquo;re already on the website :): Visit My Portfolio\n","permalink":"https://mhassan.dev/projects/portfolio-site/","summary":"\u003ch3 id=\"key-features\"\u003eKey Features\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMinimalist and Fast\u003c/strong\u003e: Developed with Hugo to ensure quick load times and an efficient design.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntuitive Navigation\u003c/strong\u003e: Utilizes the clean layout of PaperMod for an optimal user experience.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegrated Blog and Projects Section\u003c/strong\u003e: Effectively presents my work, including detailed project pages and blog posts.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReliable Deployment\u003c/strong\u003e: Hosted on GitHub Pages for stable and cost-effective performance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComments Integration with Gisqus\u003c/strong\u003e: Provides a cost-effective commenting system that limits participation to GitHub users. Gisqus also offers improved moderation controls, reduced spam, and seamless GitHub integration for a more secure and streamlined user experience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"detailed-overview\"\u003eDetailed Overview\u003c/h3\u003e\n\u003cp\u003eThe portfolio website is designed with both form and function in mind. Its key components include:\u003c/p\u003e","title":"My Portfolio Website"},{"content":" Introduction: Moving Beyond Passive Learning If you're looking to learn web development, you've likely encountered the overwhelming sea of online resources. It's easy to get stuck in a cycle of watching tutorials, passively following along without truly internalizing the concepts or developing the ability to build independently. This phenomenon, often called \"tutorial hell,\" can stall progress and hinder real skill development. The demand for skilled web developers remains high, and fortunately, quality education is increasingly accessible without the hefty price tag of traditional routes. Numerous free platforms offer comprehensive learning paths. However, simply having access to resources isn\u0026rsquo;t enough. Effective learning, especially in a practical field like web development, requires structure, hands-on application, and measurable progress.\nThis article focuses on identifying free web development platforms similar in spirit to freeCodeCamp, The Odin Project, and Full Stack Open. We\u0026rsquo;ll highlight resources that provide structured curricula covering both front-end and back-end development, emphasize project-based learning, and crucially, offer genuinely free certificates upon completion. While a certificate isn\u0026rsquo;t a substitute for a strong portfolio, it serves as a valuable acknowledgment of effort, a motivator, and a tangible credential to list as you build your practical experience.\nThe Power of Active Learning vs. Passive Watching Before exploring specific platforms, it\u0026rsquo;s essential to understand why active learning methods are far more effective than passively consuming video content.\nTrue skill development in coding comes from actively solving problems, writing code, debugging errors, and building projects. Platforms designed around these principles offer significant advantages:\nYou gain a deeper understanding of concepts by applying them directly. You develop essential debugging skills by working through errors – a core competency for any developer. You build a portfolio of unique work, demonstrating practical abilities. You cultivate critical thinking and problem-solving, attributes highly valued by employers. Let\u0026rsquo;s look at platforms that prioritize this active, hands-on approach.\nFree Web Development Platforms with Certificates Our search criteria include: free core learning materials, genuinely free certificates, structured learning paths, comprehensive front-end and back-end coverage, emphasis on projects/exercises, and community support.\nHere are several platforms that align well with these goals:\n1. freeCodeCamp Free Core Curriculum? Yes, entirely free and operated by a non-profit. Free Certificate? Yes, provides multiple free certificates upon completing major curriculum sections. Overview: A highly regarded platform known for helping individuals transition into tech careers since 2014. It offers extensive certifications covering Responsive Web Design, JavaScript, Front End Libraries, Back End Development \u0026amp; APIs, Python for Scientific Computing, Data Analysis, and more. Learning involves interactive coding challenges and building five required projects for each certification. It boasts a large and very active community forum. Bottom Line: An excellent choice for comprehensive, structured, project-based learning with verifiable free certificates and strong community support. 2. The Odin Project Free Core Curriculum? Yes, a completely free and open-source curriculum. Free Certificate? No explicit certificate. The platform\u0026rsquo;s philosophy centers on building a robust portfolio through numerous projects, which serves as the primary proof of skill. Overview: Focuses on teaching full-stack web development by curating high-quality online resources into structured learning paths (Full Stack JavaScript and Full Stack Ruby on Rails). It heavily emphasizes learning by doing, guiding users to build a substantial number of portfolio projects from scratch. Has an active and supportive community, particularly on Discord. Bottom Line: Highly effective for developing deep practical skills and learning how to learn independently. Ideal for self-motivated individuals focused on building a strong portfolio rather than collecting certificates. 3. Full Stack Open (University of Helsinki) Free Core Curriculum? Yes, offered free by the University of Helsinki. Free Certificate? Yes, a free certificate is provided upon sufficient exercise completion. Additionally, offers the possibility of earning official ECTS university credits for free. Overview: A rigorous, university-level course focused on modern JavaScript-based web development (React, Node.js, Express, MongoDB, GraphQL, TypeScript, CI/CD). The course material is primarily text-based, detailed, and interspersed with practical exercises that build complex applications step-by-step. Support is available via a dedicated Discord group. Bottom Line: An outstanding option for those seeking an academically thorough, up-to-date curriculum with the credibility of a university affiliation, a free certificate, and potential university credits. 4. Scrimba Free Core Curriculum? Offers a good selection of free courses; the main structured \u0026ldquo;Career Path\u0026rdquo; may mix free and paid content. Free Certificate? Yes, confirms a free Certificate of Completion for its Front-End Developer Career Path. Availability for individual free courses may vary. Overview: Known for its unique interactive learning format where users can pause video lessons and directly edit the instructor\u0026rsquo;s code within the player. This fosters highly engaging, hands-on learning, particularly strong for front-end technologies (HTML, CSS, JS, React). Features a well-regarded Front-End Developer Career Path and an active Discord community. Bottom Line: Provides a very effective and interactive learning experience, especially for front-end development, offering a free certificate for its main career path. 5. Codecademy Free Core Curriculum? Yes, provides numerous free introductory courses. Free Certificate? Unlikely for free courses. Certificates are typically associated with the paid \u0026ldquo;Pro\u0026rdquo; subscription. Overview: Features a popular interactive, browser-based coding environment ideal for beginners. Offers structured paths covering various web development and programming topics. Provides community forums for peer support. Bottom Line: Great for hands-on experimentation and learning basics interactively, but less suitable if a free certificate is a primary goal. 6. Coursera \u0026amp; edX Free Core Curriculum? Yes, many courses can be audited for free (access to lecture videos and readings). Free Certificate? Generally No. Certificates and access to graded assignments usually require payment. Overview: Host courses, Specializations, and Professional Certificates from top universities and companies. Content quality is typically high. Learning is often through video lectures, readings, and quizzes/assignments. Bottom Line: Excellent resources for accessing high-quality educational content via auditing. Paid enrollment is generally necessary for certificates. 7. Khan Academy Free Core Curriculum? Yes, fully free non-profit platform. Free Certificate? No. Focus is on providing free educational content without formal credentialing for these subjects. Overview: Offers beginner-friendly introductions to computer programming concepts, including HTML, CSS, JavaScript, and SQL, through video tutorials and interactive exercises. Less comprehensive for full-stack development compared to others but excellent for foundational knowledge. Bottom Line: A valuable free resource for learning the fundamentals, particularly for beginners, but does not offer certificates for web development courses. Platform Feature Comparison Platform Free Core? Free Cert? Focus Projects/Exercises? Community? Best For freeCodeCamp Yes Yes Full Stack, Various Tech Yes (Heavy) Yes (Large) All-around free learning, certs, project focus, community The Odin Project Yes No (Portfolio Focus) Full Stack (JS/Rails) Yes (Heavy) Yes (Strong) Deep practical learning, strong portfolio building, self-study Full Stack Open Yes Yes (+ Credits!) Modern Full Stack (JS) Yes (Exercises) Yes Rigorous academic approach, modern tech, university credibility Scrimba Yes (Partial) Yes (Career Path) Front-End Heavy Yes Yes (Active) Interactive learning format, front-end path with certificate Codecademy Yes (Partial) Unlikely/Paid Broad Tech Yes Yes Interactive introductions and basic practice Coursera/edX Yes (Audit) No (Paid) Broad Tech (Uni Level) Some Yes Accessing university-level content via audit Khan Academy Yes No Foundational CS/Web Yes Yes Gentle introduction to programming and web basics Choosing the Right Platform for Your Goals Selecting the best platform depends on your learning style, goals, and commitment level.\nFor a comprehensive free curriculum with guaranteed free certificates and a large support network: freeCodeCamp is a top recommendation. For a rigorous, university-backed deep dive into modern web development with a free certificate (and potential credits): Full Stack Open offers exceptional value. If building a strong portfolio through intensive project work is your priority (certificate secondary): The Odin Project provides a highly effective, practical path. For an engaging, interactive approach focused on front-end development, including a free path certificate: Scrimba is an excellent choice. To explore basics interactively or access university content without needing a free certificate: Codecademy (free tier), Khan Academy, Coursera (audit), and edX (audit) are useful resources. Important Considerations:\nYour Portfolio is Paramount: While certificates have their place, a portfolio showcasing functional, well-coded projects you\u0026rsquo;ve built is the most compelling evidence of your skills for potential employers. Prioritize building! Leverage the Community: Actively participate in forums or Discord channels. Asking questions, helping others, and sharing your progress enhances learning and provides crucial support. Match Your Learning Style: Consider whether you learn best from text, video, interactive exercises, or project challenges. Sample introductory content from different platforms to find the best fit. Conclusion: Focus on Building Skills Navigating the world of free online web development education can be challenging, but excellent resources exist that emphasize active learning and offer tangible recognition through free certificates.\nPlatforms like freeCodeCamp, Full Stack Open, The Odin Project, and Scrimba stand out for providing structured, hands-on learning experiences that move beyond passive tutorial consumption. They empower learners to build practical skills effectively.\nUltimately, the most valuable outcome is the acquisition of demonstrable skills showcased through a strong portfolio. Choose a platform that aligns with your goals and learning preferences, commit to the process, engage with the community, and focus on consistently building and refining your abilities. The journey requires dedication, but these resources provide solid foundations for a successful career in web development.\nDo you use other free platforms for learning web development? Share your recommendations or experiences in the comments below!\n","permalink":"https://mhassan.dev/blog/web-dev-resources/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003ch2 id=\"introduction-moving-beyond-passive-learning\"\u003e\u003cspan style=\"color:#FFB4A2\"\u003eIntroduction: Moving Beyond Passive Learning\u003c/span\u003e\u003c/h2\u003e\n\u003cdiv style=\"text-align: justify;\"\u003e\nIf you're looking to learn web development, you've likely encountered the overwhelming sea of online resources. It's easy to get stuck in a cycle of watching tutorials, passively following along without truly internalizing the concepts or developing the ability to build independently. This phenomenon, often called \"tutorial hell,\" can stall progress and hinder real skill development.\n\u003cp\u003eThe demand for skilled web developers remains high, and fortunately, quality education is increasingly accessible without the hefty price tag of traditional routes. Numerous free platforms offer comprehensive learning paths. However, simply having access to resources isn\u0026rsquo;t enough. Effective learning, especially in a practical field like web development, requires structure, hands-on application, and measurable progress.\u003c/p\u003e","title":"Learn Web Development for Free: Platforms with Certificates (That Aren't Just Tutorials)"},{"content":" Introduction If you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs! The Problem: Dumb Bots and Information Overload Traditional search methods or basic chatbots often fall short when dealing with specific document sets:\nInformation Overload: Manually searching large documents is time-consuming and inefficient. Generic LLM Limitations: Large Language Models (LLMs) are powerful, but they lack specific, up-to-date knowledge about your documents unless explicitly trained on them (which is often impractical). Hallucination Risk: When asked about information outside their training data, LLMs might confidently invent answers that sound plausible but are incorrect. This is unacceptable for reliable FAQ systems. Inconsistent Outputs: Getting answers in a usable, predictable format can be challenging with free-form text generation. We need a system that answers questions accurately based only on a given set of documents and provides answers in a consistent, structured way.\nThe Solution: RAG + Gemini API Our approach combines Retrieval Augmented Generation (RAG) with the capabilities of the Gemini API:\nRAG Pipeline: This involves three main steps:\nIndexing: Convert the source documents (Google Car manuals) into numerical representations (embeddings) using the Gemini text-embedding-004 model and store them in a vector database (ChromaDB). This allows for efficient similarity searches. Retrieval: When a user asks a question, embed the question using the same model and search the vector database to find the most relevant document chunks. Generation: Pass the original question and the retrieved document chunks as context to a powerful LLM (like gemini-2.0-flash). Instruct the model to answer the question based only on the provided context. Gemini API Features:\nHigh-Quality Embeddings: text-embedding-004 provides embeddings suitable for finding semantically similar text. Powerful Generation: gemini-2.0-flash can synthesize answers based on the retrieved context. Structured Output (JSON Mode): We instruct Gemini to return the answer and a confidence score in a predictable JSON format, making it easy for applications to use the output. Optional Grounding: We can even add Google Search as a tool if the local documents don\u0026rsquo;t suffice (though our primary goal here is document-based Q\u0026amp;A). Implementation Highlights Here are some key code snippets demonstrating the core components:\n1. Custom Embedding Function for ChromaDB: We need to tell ChromaDB how to generate embeddings using the Gemini API.\n# --- 4. Define Gemini Embedding Function for ChromaDB --- from chromadb import Documents, EmbeddingFunction, Embeddings from google.api_core import retry from google import genai from google.genai import types is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503}) class GeminiEmbeddingFunction(EmbeddingFunction): document_mode = True # Toggle between indexing docs and embedding queries @retry.Retry(predicate=is_retriable) def __call__(self, input_texts: Documents) -\u0026gt; Embeddings: task = \u0026#34;retrieval_document\u0026#34; if self.document_mode else \u0026#34;retrieval_query\u0026#34; print(f\u0026#34;Embedding {\u0026#39;documents\u0026#39; if self.document_mode else \u0026#39;query\u0026#39;} ({len(input_texts)})...\u0026#34;) try: response = client.models.embed_content( model=\u0026#34;models/text-embedding-004\u0026#34;, contents=input_texts, config=types.EmbedContentConfig(task_type=task), # Specify task type ) return [e.values for e in response.embeddings] except Exception as e: print(f\u0026#34;Error during embedding: {e}\u0026#34;) return [[] for _ in input_texts] 2. Setting up ChromaDB and Indexing: We create a ChromaDB collection and add our documents. get_or_create_collection makes this idempotent.\n# --- 5. Setup ChromaDB Vector Store --- import chromadb import time print(\u0026#34;Setting up ChromaDB...\u0026#34;) DB_NAME = \u0026#34;googlecar_faq_db\u0026#34; embed_fn = GeminiEmbeddingFunction() chroma_client = chromadb.Client() # In-memory client try: db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn) print(f\u0026#34;Collection \u0026#39;{DB_NAME}\u0026#39; ready. Current count: {db.count()}\u0026#34;) if db.count() \u0026lt; len(documents): print(f\u0026#34;Adding/Updating documents in \u0026#39;{DB_NAME}\u0026#39;...\u0026#34;) embed_fn.document_mode = True # Set mode for indexing db.upsert(documents=documents, ids=doc_ids) # Use upsert for safety time.sleep(2) # Allow indexing to settle print(f\u0026#34;Documents added/updated. New count: {db.count()}\u0026#34;) else: print(\u0026#34;Documents already seem to be indexed.\u0026#34;) except Exception as e: print(f\u0026#34;Error setting up ChromaDB collection: {e}\u0026#34;) raise SystemExit(\u0026#34;ChromaDB setup failed. Exiting.\u0026#34;) 3. Retrieving Relevant Documents: This function takes the user query, embeds it (using document_mode=False), and searches ChromaDB.\n# --- 6. Define Retrieval Function --- def retrieve_documents(query: str, n_results: int = 1) -\u0026gt; list[str]: print(f\u0026#34;\\nRetrieving documents for query: \u0026#39;{query}\u0026#39;\u0026#34;) embed_fn.document_mode = False # Switch to query mode try: results = db.query(query_texts=[query], n_results=n_results) if results and results.get(\u0026#39;documents\u0026#39;): retrieved_docs = results[\u0026#39;documents\u0026#39;][0] print(f\u0026#34;Retrieved {len(retrieved_docs)} documents.\u0026#34;) return retrieved_docs else: print(\u0026#34;No documents retrieved.\u0026#34;) return [] except Exception as e: print(f\u0026#34;Error querying ChromaDB: {e}\u0026#34;) return [] 4. Generating the Structured Answer: Here\u0026rsquo;s the core logic combining the query, retrieved context, and instructions for the LLM, specifying JSON output with a confidence score.\n# --- 7. Define Structured Output Schema --- from typing_extensions import Literal from pydantic import BaseModel class AnswerWithConfidence(BaseModel): answer: str confidence: Literal[\u0026#34;High\u0026#34;, \u0026#34;Medium\u0026#34;, \u0026#34;Low\u0026#34;] # --- 8. Define Augmented Generation Function --- def generate_structured_answer(query: str, context_docs: list[str]) -\u0026gt; dict | None: # ... (prompt construction as shown previously) ... prompt = f\u0026#34;\u0026#34;\u0026#34;You are an AI assistant answering questions about a Google car based ONLY on the provided documents. Context Documents: --- {context} --- Question: {query} Based *only* on the information in the context documents above, answer the question. Also, assess your confidence in the answer based *only* on the provided text: - \u0026#34;High\u0026#34; if the answer is directly and clearly stated in the documents. - \u0026#34;Medium\u0026#34; if the answer can be inferred but isn\u0026#39;t explicitly stated. - \u0026#34;Low\u0026#34; if the documents don\u0026#39;t seem to contain the answer or are ambiguous. Return your response ONLY as a JSON object with the keys \u0026#34;answer\u0026#34; and \u0026#34;confidence\u0026#34;. Example format: {{ \u0026#34;answer\u0026#34;: \u0026#34;Your answer here.\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;High/Medium/Low\u0026#34; }} \u0026#34;\u0026#34;\u0026#34; try: generation_config = types.GenerateContentConfig( temperature=0.2, response_mime_type=\u0026#34;application/json\u0026#34;, # Request JSON response_schema=AnswerWithConfidence # Provide the schema ) response = client.models.generate_content( model=\u0026#34;gemini-2.0-flash\u0026#34;, contents=prompt, config=generation_config # Pass the config object ) # ... (response handling as shown previously) ... # Safe access to parsed output if response.candidates and response.candidates[0].content and response.candidates[0].content.parts: parsed_output = response.candidates[0].content.parts[0].parsed if isinstance(parsed_output, dict) and \u0026#34;answer\u0026#34; in parsed_output and \u0026#34;confidence\u0026#34; in parsed_output: return parsed_output # ... (Error handling/logging) ... return {\u0026#34;answer\u0026#34;: \u0026#34;Error: Could not generate/parse structured response.\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;Low\u0026#34;} except Exception as e: print(f\u0026#34;Error during content generation call: {e}\u0026#34;) return {\u0026#34;answer\u0026#34;: f\u0026#34;Error during generation API call: {e}\u0026#34;, \u0026#34;confidence\u0026#34;: \u0026#34;Low\u0026#34;} Tip: Ensure your API key is correctly set up in Kaggle Secrets (GOOGLE_API_KEY). Also, ChromaDB setup might require specific permissions or setup depending on the environment (here we use an in-memory one for simplicity).\nWhy Structured Output and Confidence Scores? Forcing the LLM to output JSON with a specific schema (using response_mime_type and response_schema) brings several advantages:\nReliability: The output format is predictable, making it easy to integrate into downstream applications without complex text parsing. Consistency: Ensures the bot always provides both the answer and its confidence level. Trustworthiness: The confidence score gives the user (or the calling application) an indication of how much to trust the answer, based on the grounding provided by the retrieved documents. A \u0026ldquo;Low\u0026rdquo; confidence answer might trigger a fallback to human support or a broader search. Limitations and Future Work This implementation is a great starting point, but it has limitations:\nDocument Quality: The RAG system\u0026rsquo;s effectiveness heavily depends on the quality, relevance, and comprehensiveness of the indexed documents. Garbage in, garbage out. Retrieval Accuracy: Simple similarity search might not always retrieve the perfect chunk of text, especially for complex queries. More advanced retrieval strategies (like hybrid search or re-ranking) could improve this. Structured Output Failures: While JSON mode is robust, the LLM might occasionally fail to generate perfectly valid JSON matching the schema. More robust error handling and potentially retries could be added. Limited Context Handling (within LLM): While RAG provides context, the LLM itself still has limits on how much context it can process effectively in a single generation step. Very long retrieved passages might need summarization or chunking before being sent to the LLM. Static Knowledge: The bot only knows what\u0026rsquo;s in the ChromaDB index. It doesn\u0026rsquo;t learn automatically. Updates require re-indexing. Future Enhancements:\nImplement Google Search grounding as a fallback when confidence is low or documents are missing. Add conversation memory for multi-turn interactions. Explore more sophisticated retrieval techniques. Build a simple UI (e.g., using Gradio or Streamlit). Fine-tune an embedding model specifically for the car manual domain (though text-embedding-004 is quite capable). Conclusion Building this FAQ bot demonstrates how combining RAG with Gemini\u0026rsquo;s embedding and generation capabilities, especially its structured output mode, can create powerful and reliable AI-driven Q\u0026amp;A systems. By grounding the LLM\u0026rsquo;s responses in specific source documents and requesting a confidence score, we significantly mitigate hallucination and provide a more trustworthy user experience.\nKey Takeaways:\nRAG grounds LLM answers in your specific data. Gemini Embeddings + ChromaDB enable efficient document retrieval. Structured Output (JSON Mode) enhances reliability and integrability. Confidence Scores add a layer of trustworthiness. This approach is versatile and can be adapted for various knowledge bases, from customer support FAQs to internal documentation search.\nI hope this walkthrough provides a clear picture of how this smarter FAQ bot works! Feel free to ask questions or leave a comment with your thoughts or own implementations!\n","permalink":"https://mhassan.dev/blog/smart-faq-bot-google-genai-intensive-course/","summary":"\u003cdiv style=\"text-align: justify;\"\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cspan style=\"color:#FFB4A2\"\u003eIntroduction\u003c/span\u003e\u003c/h2\u003e\n\u003cdiv style=\"text-align: justify;\"\u003e\nIf you've ever found yourself digging through product manuals, company wikis, or lengthy documents just to find a simple answer, you know the pain. The fact you're reading this suggests you're interested in how **Generative AI** can make that process less painful. Stick around for a few minutes, and I'll walk you through how we built a smarter FAQ bot using Google's Gemini API, Retrieval Augmented Generation (RAG), and structured output. This isn't just another chatbot; it's designed to give **reliable, context-aware answers** based *only* on provided information, minimizing the risk of making things up (hallucination). This example uses Google Car manuals, but the principles apply anywhere you have a set of documents you need to query effectively. I'm sharing my journey building this; it's a practical demonstration, not a definitive guide, so adapt the ideas to your needs!\n\u003c/div\u003e\n\u003chr\u003e\n\u003ch2 id=\"the-problem-dumb-bots-and-information-overload\"\u003e\u003cspan style=\"color:#FFB4A2\"\u003eThe Problem: Dumb Bots and Information Overload\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eTraditional search methods or basic chatbots often fall short when dealing with specific document sets:\u003c/p\u003e","title":"Building a Smarter FAQ Bot (with Gemini, RAG, and Structured Output)"},{"content":"You\u0026rsquo;ve reached a place that doesn\u0026rsquo;t exist. Maybe the page was moved, deleted, or never existed in the first place (peak gaslighting).\nHere\u0026rsquo;s what you can do: Go back to the homepage and start over. Explore my projects to see my work. Contact me if you need assistance. Or you can stay here and ponder the orb. The choice is yours.\n","permalink":"https://mhassan.dev/404.html","summary":"\u003cp\u003eYou\u0026rsquo;ve reached a place that doesn\u0026rsquo;t exist. Maybe the page was moved, deleted, or never existed in the first place (peak gaslighting).\u003c/p\u003e\n\u003ch3 id=\"heres-what-you-can-do\"\u003eHere\u0026rsquo;s what you can do:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/\"\u003eGo back to the homepage\u003c/a\u003e and start over.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/projects/\"\u003eExplore my projects\u003c/a\u003e to see my work.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/contact/\"\u003eContact me\u003c/a\u003e if you need assistance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOr you can stay here and ponder the orb. The choice is yours.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Pondering my Orb\" loading=\"lazy\" src=\"/assets/ponder.png\"\u003e\u003c/p\u003e","title":"404 - Page Not Found"}]