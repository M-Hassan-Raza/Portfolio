<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI on Muhammad Hassan Raza</title><link>https://mhassan.dev/tags/ai/</link><description>Recent content in AI on Muhammad Hassan Raza</description><generator>Hugo -- 0.154.2</generator><language>en-us</language><lastBuildDate>Sat, 15 Nov 2025 10:00:00 +0500</lastBuildDate><atom:link href="https://mhassan.dev/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Model Context Protocol: Why This Matters More Than You Think</title><link>https://mhassan.dev/blog/model-context-protocol/</link><pubDate>Sat, 15 Nov 2025 10:00:00 +0500</pubDate><guid>https://mhassan.dev/blog/model-context-protocol/</guid><description>&lt;div style="text-align: justify;"&gt;
&lt;p&gt;Every few months, something gets released that looks like infrastructure plumbing but turns out to matter more than the flashy launches. Model Context Protocol (MCP) is one of those things.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re a developer working with LLMs, MCP will change how you integrate AI into your workflows. Here&amp;rsquo;s an early-adopter perspective on what it is, why it matters, and how to actually use it.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="what-problem-does-mcp-solve"&gt;&lt;span style="color:#8ac7db"&gt;What Problem Does MCP Solve?&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Today&amp;rsquo;s AI tools are context-starved. You paste code into ChatGPT, upload files to Claude, manually copy database schemas into prompts. Every session starts from scratch. Every context window is a blank slate.&lt;/p&gt;</description></item><item><title>Extended Thinking in LLMs: A Mental Model for Developers</title><link>https://mhassan.dev/blog/extended-thinking-llms/</link><pubDate>Thu, 25 Sep 2025 10:00:00 +0500</pubDate><guid>https://mhassan.dev/blog/extended-thinking-llms/</guid><description>&lt;div style="text-align: justify;"&gt;
&lt;p&gt;Extended thinking isn&amp;rsquo;t just &amp;ldquo;model thinks longer&amp;rdquo;—it&amp;rsquo;s a fundamentally different interaction model. If you&amp;rsquo;re prompting extended thinking models (Claude Opus, o1) the same way you prompt standard models, you&amp;rsquo;re leaving most of the value on the table.&lt;/p&gt;
&lt;p&gt;This post is a developer&amp;rsquo;s mental model for working with these systems: when to use them, how to prompt them, and what trade-offs to expect.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="how-extended-thinking-actually-works"&gt;&lt;span style="color:#8ac7db"&gt;How Extended Thinking Actually Works&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Standard LLMs generate tokens one at a time, each token conditioned on everything before it. The model &amp;ldquo;thinks&amp;rdquo; only as fast as it speaks. Ask it to solve a complex problem, and it often commits to an approach in the first few tokens, then rationalizes that approach even if it&amp;rsquo;s wrong.&lt;/p&gt;</description></item><item><title>AI Features Your Users Actually Want (Hint: Not Another Chatbot)</title><link>https://mhassan.dev/blog/ai-features-users-want/</link><pubDate>Sun, 10 Aug 2025 10:00:00 +0500</pubDate><guid>https://mhassan.dev/blog/ai-features-users-want/</guid><description>&lt;div style="text-align: justify;"&gt;
&lt;p&gt;The graveyard of failed AI features is full of chatbots nobody asked for.&lt;/p&gt;
&lt;p&gt;Every product team I talk to has the same story: leadership watched a GPT demo, got excited, and mandated &amp;ldquo;we need AI in the product.&amp;rdquo; Three months later, there&amp;rsquo;s a chatbot in the corner of the app that 3% of users have tried and 0.5% use regularly.&lt;/p&gt;
&lt;p&gt;As CPO at Entropy Labs, I&amp;rsquo;ve been on both sides of this. I&amp;rsquo;ve built AI features that users loved and killed features that seemed brilliant in demos but died in production. Here&amp;rsquo;s what I&amp;rsquo;ve learned about the difference.&lt;/p&gt;</description></item><item><title>LangChain in Production: What the Tutorials Don't Tell You</title><link>https://mhassan.dev/blog/langchain-production/</link><pubDate>Fri, 20 Jun 2025 10:00:00 +0500</pubDate><guid>https://mhassan.dev/blog/langchain-production/</guid><description>&lt;div style="text-align: justify;"&gt;
&lt;p&gt;Every LangChain tutorial ends right where the real work begins. You see a neat 50-line script that queries a PDF, and you think, &amp;ldquo;Cool, I&amp;rsquo;ll ship this by Friday.&amp;rdquo; Three weeks later, you&amp;rsquo;re debugging memory leaks, wondering why your chain silently returns empty strings, and questioning every decision that led you here.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve shipped LangChain-based features to production at multiple companies. Here&amp;rsquo;s what I wish someone had told me before I started.&lt;/p&gt;</description></item><item><title>Claude Opus 4.5: When an AI Finally Gets It</title><link>https://mhassan.dev/blog/claude-opus-45/</link><pubDate>Thu, 15 May 2025 10:00:00 +0500</pubDate><guid>https://mhassan.dev/blog/claude-opus-45/</guid><description>&lt;div style="text-align: justify;"&gt;
&lt;p&gt;I&amp;rsquo;ve been skeptical of every &amp;ldquo;game-changing AI release&amp;rdquo; for the past two years. Every few months, a new model drops and Twitter explodes with claims that AGI is here. Spoiler: it never is. But when Anthropic released Opus 4.5, something actually shifted in how I work. Not because it&amp;rsquo;s AGI—it&amp;rsquo;s decidedly not—but because it&amp;rsquo;s the first model that consistently delivers on complex, multi-step reasoning without falling apart halfway through.&lt;/p&gt;</description></item></channel></rss>